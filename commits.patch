commit a1a40f56704bf733d4f886d08f98dab5a7ced8bd
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:22:49 2025 -0400

    [REFACTOR] Minor changes to get cumulative time

diff --git a/tests/benchmarks/importing_time_calculator.py b/tests/benchmarks/importing_time_calculator.py
index cd9fd86..934170d 100644
--- a/tests/benchmarks/importing_time_calculator.py
+++ b/tests/benchmarks/importing_time_calculator.py
@@ -40,16 +40,21 @@ summary = defaultdict(int)
 log_path = 'tests/benchmarks/import_time.log'
 
 with open(log_path, 'r') as f:
+
     for line in f:
         if not line.startswith("import time:"):
             continue
+
         try:
+            # Split at the seperator
             parts = line.split('|')
-            time_us = int(parts[0].replace("import time:", "").strip())
+
+
+            time_us = int(parts[1].replace("import time:", "").strip())
             module = parts[2].strip().split('.')[0]  # top-level module
             summary[module] += time_us
         except Exception:
-            continue  # Skip malformed lines
+            continue
 
 # Sort by cumulative import time
 sorted_summary = sorted(summary.items(), key=lambda x: x[1], reverse=True)

commit 16a1d9078dce3cf6df87164c9199c470d7e0c7a9
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:19:18 2025 -0400

    [STYLE] Add some whitespace to test experiments.py

diff --git a/tests/test_experiments.py b/tests/test_experiments.py
index 39ffe0d..a0098c9 100644
--- a/tests/test_experiments.py
+++ b/tests/test_experiments.py
@@ -10,6 +10,7 @@ from pylot.util.config import ImmutableConfig, FHDict
 from pylot.util.metrics import MetricsDict
 from pylot.util.thunder import ThunderDict
 
+
 @pytest.fixture
 def exp_dir(tmp_path: Path):
     """
@@ -75,9 +76,9 @@ def test_metrics_property(exp_dir):
     # Log another metric
     second_metric_real = {'mse': 1.0232}
     exp.metricsd['metrics'].log(second_metric_real)
-    
+
     first_metric = exp.metrics.data[0]
     second_metric = exp.metrics.data[1]
 
     assert first_metric_real == first_metric
-    assert second_metric_real == second_metric
\ No newline at end of file
+    assert second_metric_real == second_metric

commit d1797c694fc349d41cbfe90a79294c56a91b1b03
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:19:01 2025 -0400

    [FIX] Disable sorting keys in yaml saving

diff --git a/pylot/util/ioutil.py b/pylot/util/ioutil.py
index 84d186c..b022431 100644
--- a/pylot/util/ioutil.py
+++ b/pylot/util/ioutil.py
@@ -71,7 +71,6 @@ class FileExtensionError(Exception):
 
 
 class FileFormat(ABC):
-
     """
     Base class that other formats inherit from
     children formats must overload one of (save|encode) and
@@ -166,7 +165,7 @@ class YamlFormat(FileFormat):
 
     @classmethod
     def encode(cls, obj) -> str:
-        return yaml.safe_dump(obj, indent=2).encode("utf-8")
+        return yaml.safe_dump(obj, indent=2, sort_keys=False).encode("utf-8")
 
     @classmethod
     def decode(cls, data: bytes) -> object:
@@ -176,7 +175,7 @@ class YamlFormat(FileFormat):
     def save(cls, obj, fp):
         fp = cls.check_fp(fp)
         with fp.open("w") as f:
-            yaml.safe_dump(obj, f)
+            yaml.safe_dump(obj, f, sort_keys=False)
 
     @classmethod
     def load(cls, fp) -> object:
@@ -601,7 +600,7 @@ def autosave(obj, path: Union[str, pathlib.Path], parents=True) -> object:
     # Check if extension is supported
     ext = path.suffix.strip(".")
     if ext not in _DEFAULTFORMAT:
-        
+
         msg = (
             f'Extension {ext} is not supported for autosaving. Unable to save'
             f'{path}'

commit e518c740314d690c847044f19a82f8ea5909dfcd
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:18:36 2025 -0400

    [REFACTOR] Make get_nested() more robust and add documentation

diff --git a/pylot/util/config.py b/pylot/util/config.py
index 82cdbaf..40bf412 100644
--- a/pylot/util/config.py
+++ b/pylot/util/config.py
@@ -80,20 +80,49 @@ def _valid_list_key(key, list_) -> bool:
     return -len(list_) <= key < len(list_)
 
 
-def get_nested(nested_dict, key, sep=None):
-    key = parse_key(key, sep=sep)
-    key_so_far = tuple()
-    d = nested_dict
-    for subkey in key:
-        key_so_far += (subkey,)
-        if isinstance(d, list):
-            if not _valid_list_key(subkey, d):
-                raise KeyError(key_so_far)
-            subkey = int(subkey)
-        if isinstance(d, dict) and subkey not in d:
-            raise KeyError(key_so_far)
-        d = d[subkey]
-    return d
+def get_nested(
+    nested_dict,
+    key: Any,
+    sep: Any = None
+) -> Any:
+    """
+    Retrieve a value from a nested dict/list by `key`.
+
+    Parameters
+    ----------
+    nested_dict : dict or list
+        The container to search.
+    key : str, int, tuple, or list
+        Key or sequence of subkeys.
+    sep : str, optional
+        Separator for string keys.
+
+    Returns
+    -------
+    Any
+        The found value.
+
+    Raises
+    ------
+    KeyError
+        If any subkey is missing or index is invalid.
+    """
+    path = parse_key(key, sep=sep)
+    current = nested_dict
+    so_far: Tuple[Any, ...] = ()
+    for sub in path:
+        so_far += (sub,)
+        if isinstance(current, list):
+            if not _valid_list_key(sub, current):
+                raise KeyError(
+                    f"Invalid index {sub!r} at path {so_far}"
+                )
+            sub = int(sub)
+        if isinstance(current, dict):
+            if sub not in current:
+                raise KeyError(f"Missing key {sub!r} at path {so_far}")
+        current = current[sub]
+    return current
 
 
 def set_nested(nested_dict, key, value, sep=None):
@@ -524,13 +553,13 @@ class ImmutableConfig(HDict):
     -------
     __hash__():
         Returns the hash of the configuration based on its digest.
-    
+
     digest():
         Computes a hexadecimal digest of the internal data.
-    
+
     __setitem__(key, value):
         Disabled. Raises an ImmutableConfigError.
-    
+
     __delitem__(key):
         Disabled. Raises an ImmutableConfigError.
     

commit a1a323049e2c3a426188ae9e6aa2168f93857471
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:18:12 2025 -0400

    [FEAT] Add some losses from multi/universeg

diff --git a/pylot/loss/segmentation.py b/pylot/loss/segmentation.py
index 5de68fe..c0afa57 100644
--- a/pylot/loss/segmentation.py
+++ b/pylot/loss/segmentation.py
@@ -2,11 +2,19 @@ from typing import Optional, Union
 
 from pydantic import validate_call
 
+from typing import Optional, Union, Literal
+
+import torch
+from torch import Tensor
+import torch.nn as nn
+
+from pylot.metrics.util import _metric_reduction, _inputs_as_onehot
+
 from .util import _loss_module_from_func
 from ..util.more_functools import partial
 from ..metrics.segmentation import soft_dice_score, soft_jaccard_score, pixel_mse
 from ..metrics.util import InputMode, Reduction
-from pylot.torch.torchlib import torch, Tensor
+
 
 @validate_call(config=dict(arbitrary_types_allowed=True))
 def soft_dice_loss(
@@ -83,6 +91,106 @@ def soft_jaccard_loss(
     return loss
 
 
+class FocalDiceLoss(nn.Module):
+    """
+    Focal Loss + Soft Dice Loss
+    """
+    def __init__(self, from_logits: bool = False, gamma: float = 20.0, batch_reduction: Optional[Literal["mean"]] = None, **kwargs):
+        super().__init__()
+        self.batch_reduction = batch_reduction
+        self.from_logits = from_logits
+        self.gamma = gamma
+        self.kwargs = kwargs
+
+    def __call__(self, y_pred, y_true, ):
+        # y_pred shape: B x 1 x H x W
+        # We are doing binary segmentation so channel = 1
+
+        focal_loss_term = focal_loss(y_pred, y_true, 
+                                     gamma=self.gamma, 
+                                     reduction='mean', 
+                                     batch_reduction=None,
+                                     from_logits=self.from_logits,
+                                     )
+
+        dice_loss_term = soft_dice_loss(y_pred, y_true,
+                              mode='binary',
+                              weights=None,
+                              reduction='mean', # there's only 1 channel so this is fine
+                              batch_reduction=None,
+                              from_logits=self.from_logits,
+                              **self.kwargs
+                              )
+        
+        loss = focal_loss_term + dice_loss_term
+
+        if self.batch_reduction == 'mean':
+            return loss.mean()
+        else:
+            return loss
+
+
+def focal_loss(
+    y_pred: Tensor,
+    y_true: Tensor,
+    gamma: float = 20.0,
+    weights: Optional[Tensor] = None,
+    channel_weights: Optional[Tensor] = None,
+    mode: str = "auto",
+    reduction: str = "mean",  # Reduction over channels
+    batch_reduction: str = "mean", 
+    ignore_index: Optional[int] = None,
+    from_logits: bool = False,
+    eps: float = 1e-7
+) -> Tensor:
+    """
+    Binary focall loss that allows per-pixel weights
+    https://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf
+    """
+    if weights is not None:
+        batch_size, num_classes = y_pred.shape[:2]
+        weights = weights.view(batch_size, num_classes, -1)
+
+    y_pred, y_true = _inputs_as_onehot(
+        y_pred, y_true, mode=mode, from_logits=from_logits
+    )
+
+    loss = - binary_focal_cross_entropy(
+        y_pred, y_true, weights=weights, eps=eps, dim=-1, gamma=gamma
+    )
+
+    batch_loss = _metric_reduction(
+        loss, reduction=reduction, batch_reduction=batch_reduction, weights=channel_weights, ignore_index=ignore_index
+    )
+
+    return batch_loss
+
+
+def binary_focal_cross_entropy(
+    y_pred: Tensor,
+    y_true: Tensor,
+    weights: Optional[Tensor] = None,
+    gamma: float = 20.0,
+    eps: float = 1e-7,
+    dim = None,
+    ):
+    """
+    Returns -binary focal loss
+    https://focal-loss.readthedocs.io/en/latest/generated/focal_loss.binary_focal_loss.html#focal_loss.binary_focal_loss
+    """
+    assert y_pred.shape == y_true.shape, f"y_pred.shape {y_pred.shape} != y_true.shape {y_true.shape}"
+    if weights is not None:
+        assert y_pred.shape == weights.shape, f"y_pred.shape={y_pred.shape}, weights.shape={weights.shape} do not match"
+
+    left_term = y_true * torch.log(y_pred + eps) * (1 - y_pred)**gamma
+    right_term = (1 - y_true) * torch.log(1-y_pred + eps) * y_pred**gamma
+
+    if weights is not None:
+        return torch.mean((left_term + right_term)*weights, dim=dim)
+    else:
+        return torch.mean(left_term + right_term, dim=dim)
+
+
 pixel_mse_loss = pixel_mse
 binary_soft_dice_loss = partial(soft_dice_loss, mode="binary")
 binary_soft_jaccard_loss = partial(soft_jaccard_loss, mode="binary")
@@ -90,3 +198,4 @@ binary_soft_jaccard_loss = partial(soft_jaccard_loss, mode="binary")
 SoftDiceLoss = _loss_module_from_func("SoftDiceLoss", soft_dice_loss)
 SoftJaccardLoss = _loss_module_from_func("SoftJaccardLoss", soft_jaccard_loss)
 PixelMSELoss = _loss_module_from_func("PixelMSELoss", pixel_mse_loss)
+

commit fc788ae71a1acc05e838fa369dd8b47599d7df40
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:17:47 2025 -0400

    [REFACTOR] Make absolute import more robust

diff --git a/pylot/experiment/util.py b/pylot/experiment/util.py
index c779b30..2ae424d 100644
--- a/pylot/experiment/util.py
+++ b/pylot/experiment/util.py
@@ -6,6 +6,9 @@ import random
 from typing import Tuple, Dict, List
 import numpy as np
 
+import importlib
+import functools
+
 from ..util.config import HDict, Config
 from ..util.ioutil import autoload
 from ..util.more_functools import partial
@@ -59,7 +62,8 @@ def generate_fun_name() -> str:
     return f"{adj}-{noun}"
 
 
-def generate_tuid(nonce_length: int = 4) -> Tuple[str, int]:
+def generate_tuid(
+    nonce_length: int = 4) -> Tuple[str, int]:
     """
     Generate time-based unique ID for experiment run directories.
     """
@@ -70,14 +74,38 @@ def generate_tuid(nonce_length: int = 4) -> Tuple[str, int]:
     return now, nonce.upper()
 
 
-def absolute_import(reference):
-    module, _, attr = reference.rpartition(".")
-    if importlib.util.find_spec(module) is not None:
-        module = importlib.import_module(module)
-        if hasattr(module, attr):
-            return getattr(module, attr)
+def absolute_import(reference: str):
+    """
+    Resolve a dotted path to a Python object, including nested attributes.
+
+    Parameters
+    ----------
+    reference : str
+        Dot-separated reference path, e.g. 'thing.DataClass.attrname'.
+
+    Returns
+    -------
+    Any
+        The resolved Python object.
 
-    raise ImportError(f"Could not import {reference}")
+    Raises
+    ------
+    ImportError
+        If any part of the import fails.
+    """
+    parts = reference.split('.')
+
+    # Try importing the longest valid module prefix
+    for i in reversed(range(1, len(parts))):
+        module_path = '.'.join(parts[:i])
+        try:
+            module = importlib.import_module(module_path)
+            # Resolve remaining attributes
+            return functools.reduce(getattr, parts[i:], module)
+        except (ModuleNotFoundError, AttributeError):
+            continue
+
+    raise ImportError(f"Could not import or resolve: {reference}")
 
 
 def eval_config(config):

commit 3aa6ffd55d066232ae96da183fd689d40f31ea26
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:17:26 2025 -0400

    [CHORE] Minor cleaning to train.py

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index 40d1ec3..1e268fb 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -8,6 +8,10 @@ Example
 >>> exp.run()
 """
 
+__all__ = [
+    'TrainExperiment'
+]
+
 import copy
 import pathlib
 from typing import List
@@ -366,7 +370,7 @@ class TrainExperiment(BaseExperiment):
             following keys: {'model', 'optim', '_epoch'}.
         strict : bool, optional
             Whether to strictly enforce that the keys in the state dictionary
-            match the keys returned by the moduleâ€™s `state_dict` function.
+            match the keys returned by the module's `state_dict` function.
             Default is True.
 
         Examples
@@ -390,8 +394,10 @@ class TrainExperiment(BaseExperiment):
                 # Restore model or optimizer state
                 if isinstance(x, nn.Module):
                     x.load_state_dict(state_dict, strict=strict)
+
                 elif isinstance(x, torch.optim.Optimizer):
                     x.load_state_dict(state_dict)
+
                 else:
                     raise TypeError(f"Unsupported type {type(x)}")
 
@@ -403,7 +409,7 @@ class TrainExperiment(BaseExperiment):
         """
         Save the current state of the model to a checkpoint `.pt` file.
 
-        This method serializes the model's state to 3 main keys: {'model', 
+        This method serializes the model's state to 3 main keys: {'model',
         'optim' and '_epoch'} representing the model state dict, the optimizer
         state dict, and the current epoch number, respectively. The checkpoint
         is written to the `checkpoints/` subdirectory of the experiment dir.
@@ -602,12 +608,12 @@ class TrainExperiment(BaseExperiment):
             'epoch': epoch,
             f'{phase}_loss': meters_collect['loss'],
             f'{phase}_dice_score': meters_collect['dice_score'],
-            }
+        }
 
         if self.config.get('wandb.track_it', False):
             logger.info(f"\n{wandb_metrics}")
             wandb.log(wandb_metrics)
-    
+
         self.metrics.log(metrics)
 
         return metrics

commit 0bfd02cc0dbe924f8a63199fcf8a438fa4442188
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:16:41 2025 -0400

    [FIX] Absolute imports of TrainExperiment

diff --git a/pylot/experiment/distributed.py b/pylot/experiment/distributed.py
index 2ba6510..6956b8d 100644
--- a/pylot/experiment/distributed.py
+++ b/pylot/experiment/distributed.py
@@ -1,7 +1,7 @@
 from torch.nn.parallel import DistributedDataParallel as DDP
 import torch.distributed as dist
 import os
-from pylot.experiment import TrainExperiment
+from pylot.experiment.train import TrainExperiment
 from pylot.util import ThunderDict, MetricsDict
 from torch.utils.data.distributed import DistributedSampler
 from torch.utils.data import DataLoader
diff --git a/pylot/experiment/finetune.py b/pylot/experiment/finetune.py
index d96adb2..7f683d2 100644
--- a/pylot/experiment/finetune.py
+++ b/pylot/experiment/finetune.py
@@ -2,7 +2,7 @@ import pathlib
 from typing import Optional
 
 from pylot.torch.torchlib import torch
-from .train import TrainExperiment
+from pylot.experiment.train import TrainExperiment
 from ..util import Config, autoload
 
 
diff --git a/pylot/experiment/half.py b/pylot/experiment/half.py
index daaa93b..65ed5cd 100644
--- a/pylot/experiment/half.py
+++ b/pylot/experiment/half.py
@@ -1,6 +1,6 @@
 from pylot.util import to_device
 from pylot.torch.torchlib import torch
-from pylot.experiment import TrainExperiment
+from pylot.experiment.train import TrainExperiment
 
 
 class HalfTE(TrainExperiment):

commit bcebb814426dc21b5a6912da5d7fdc5497f0909c
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:16:03 2025 -0400

    [FIX] Remove broken import of undefined expand_keys()

diff --git a/pylot/experiment/config.py b/pylot/experiment/config.py
index b9bed21..b8ce7c5 100644
--- a/pylot/experiment/config.py
+++ b/pylot/experiment/config.py
@@ -3,7 +3,7 @@ import hashlib
 from collections.abc import MutableMapping
 
 # from .util import dict_recursive_update, expand_dots, expand_keys
-from ..util import allbut, expand_keys
+from ..util import allbut  # , expand_keys
 
 
 class Config(MutableMapping):
@@ -40,8 +40,8 @@ class Config(MutableMapping):
             cfg = yaml.safe_load(f)
         return Config(cfg)
 
-    def flatten(self):
-        return expand_keys(self.cfg)
+    #def flatten(self):
+    #    return expand_keys(self.cfg)
 
     def dump(self, file):
         with open(file, "w") as f:

commit c26289c3ac6cb69e10bab2515aa7b05b69985535
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:15:34 2025 -0400

    [FIX] Add config hash back into file names

diff --git a/pylot/experiment/base.py b/pylot/experiment/base.py
index 6b95927..c0e67cb 100644
--- a/pylot/experiment/base.py
+++ b/pylot/experiment/base.py
@@ -33,7 +33,7 @@ import functools
 from abc import abstractmethod
 from typing import Union
 
-# Third party imports 
+# Third party imports
 import yaml
 from loguru import logger
 
@@ -41,7 +41,7 @@ from loguru import logger
 from .util import fix_seed, absolute_import, generate_tuid
 from ..util.metrics import MetricsDict
 from ..util.config import HDict, FHDict, ImmutableConfig, config_digest
-from ..util.ioutil import autosave, ensure_logging
+from ..util.ioutil import autosave
 from ..util.libcheck import check_environment
 from ..util.thunder import ThunderDict
 
@@ -93,7 +93,7 @@ class BaseExperiment:
     def __init__(
         self,
         path: str,
-        logging_level: str = 'debug',
+        logging_level: str = 'CRITICAL',
     ):
         """
         Initialize an experiment from an existing experiment directory.
@@ -113,10 +113,10 @@ class BaseExperiment:
         self.path = path
 
         # Make sure logging is set up and make first log confirming experiment
-        ensure_logging(
-            log_file_abspath=self.path / 'output.log',
-            level=logging_level,
-        )
+        # ensure_logging(
+        #    log_file_abspath=self.path / 'output.log',
+        #    level=logging_level,
+        # )
 
         logger.info(f'Absolute path to experiment run: "{self.path}"')
 
@@ -131,7 +131,7 @@ class BaseExperiment:
         # Load the configuration file (defaults to 'config.yml')
         self.config = ImmutableConfig.from_file(path / "config.yml")
 
-        config_str = yaml.safe_dump(self.config._data, sort_keys=False)
+        # self.config = yaml.safe_dump(self.config._data, sort_keys=False)
 
         # Initialize stores
         self.properties = FHDict(self.path / "properties.json")
@@ -211,7 +211,6 @@ class BaseExperiment:
 
         # Make a default root for the experiment
         default_experiment_root = 'pylot_experiments'
-        print('config: ', config)
 
         # Make sure there's a valid root directory for the experiments
         if "log" not in config:
@@ -221,7 +220,7 @@ class BaseExperiment:
         if "root" not in config["log"]:
             config['log']['root'] = default_experiment_root
 
-        # Determine base folder for experiments        
+        # Determine base folder for experiments
         experiments_root = pathlib.Path(config['log']['root'])
 
         # Log the root path of the experiments
@@ -235,7 +234,7 @@ class BaseExperiment:
         digest = config_digest(config)
 
         # Generate the (unique) name for the experiment run.
-        experiment_unique_id = f"{created_timestamp}-{random_suffix}"#-{digest}"
+        experiment_unique_id = f"{created_timestamp}-{digest}-{random_suffix}"
         logger.info(
             f'Made unique identifier for experiment: "{experiment_unique_id}"'
         )
@@ -246,7 +245,7 @@ class BaseExperiment:
         # TODO: Determine wherelse `nonce` and `create_time` are used. Change.
         metadata = {
             "create_time": created_timestamp,
-            "nonce": random_suffix, 
+            "nonce": random_suffix,
             "digest": digest
         }
 

commit 0a50e6fcde8b1f2d6732b4783d61ebe94e000f43
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:15:03 2025 -0400

    [ARCH] Add imports to __init__'s

diff --git a/pylot/experiment/__init__.py b/pylot/experiment/__init__.py
index b77528a..0b21b59 100644
--- a/pylot/experiment/__init__.py
+++ b/pylot/experiment/__init__.py
@@ -1,3 +1,11 @@
+from . import base
+from . import config
+from . import distributed
+from . import finetune
+from . import half
+from . import train
+from . import util
+
 from .base import BaseExperiment
 from .train import TrainExperiment
 from .util import autoload_experiment
diff --git a/pylot/torch/__init__.py b/pylot/torch/__init__.py
index e69de29..6122839 100644
--- a/pylot/torch/__init__.py
+++ b/pylot/torch/__init__.py
@@ -0,0 +1,2 @@
+from . import api
+from . import torchlib

commit 3ffa71c3f986867d9190df0695c1f135ba5480b2
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:14:31 2025 -0400

    [REFACTOR] Do not log metrics df

diff --git a/pylot/callbacks/epoch.py b/pylot/callbacks/epoch.py
index 7d3321a..a8cfeed 100644
--- a/pylot/callbacks/epoch.py
+++ b/pylot/callbacks/epoch.py
@@ -13,6 +13,7 @@ from loguru import logger
 from pylot.torch.torchlib import torch
 import pandas as pd
 
+
 def PrintLogged(experiment):
 
     def PrintLoggedCallback(epoch):
@@ -23,8 +24,6 @@ def PrintLogged(experiment):
 
         df = df[df.epoch == epoch].drop(columns=["epoch"])
 
-        logger.critical(df)
-
         dfp = pd.pivot_table(
             pd.melt(
                 df,
@@ -36,8 +35,6 @@ def PrintLogged(experiment):
             aggfunc="mean",
         )
 
-        logger.critical(dfp)
-
         dfp.columns = [x[1] for x in dfp.columns]
 
         # Log the metrics table.

commit 082eef43d2d934beaf8c1d1a8f99009061d98efc
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:13:50 2025 -0400

    [BUILD] Update readme adn requirements

diff --git a/README.md b/README.md
index 2b33aa5..51c80a8 100644
--- a/README.md
+++ b/README.md
@@ -60,7 +60,7 @@ log:
 ## 2 Define an Experiment
 
 ```python
-from pylot.experiment.base import TrainExperiment
+from pylot.experiment.train import TrainExperiment
 
 class MyExperiment(TrainExperiment):
     """
diff --git a/requirements.txt b/requirements.txt
index ebd6f31..9f721f1 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -12,7 +12,6 @@ ipython==8.12.3
 ipywidgets==8.1.6
 jc==1.25.4
 Jinja2==3.1.6
-kornia==0.8.0
 lmdbm==0.0.6
 lz4==4.4.4
 matplotlib==3.10.1
@@ -29,7 +28,7 @@ pydantic==2.11.3
 pympler==1.1
 PyYAML==6.0.2
 rich==14.0.0
-s3fs==2025.3.2
+# s3fs==2025.3.2
 scikit_learn==1.6.1
 scipy==1.15.2
 seaborn==0.13.2

commit 8861b026021af1748ea4155608c11286e69e038f
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 20:13:26 2025 -0400

    [BUILD] Update pyproject.toml

diff --git a/pyproject.toml b/pyproject.toml
index 1f3ce19..bd6eb2b 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -14,27 +14,19 @@ classifiers = [
     "Programming Language :: Python",
     "Programming Language :: Python :: 3",
 ]
-keywords = ["deep learning"]
+keywords = ["Deep Learning", "Experiment"]
 dependencies = [
-#    "albumentations",
-#    "box",
-#    "decoder",
-#    "device",
     "diskcache",
     "einops",
-#    "fastcore",
     "graphviz",
-#    "humanize",
-#    "imohash",
     "IPython",
     "ipywidgets",
     "jc",
     "Jinja2",
-    "kornia",
     "lmdbm",
      "lz4",
     "matplotlib",
-#    "more-itertools",
+    "more-itertools",
     "msgpack-numpy",
 #    "msgpack-python",
     "numpy",
@@ -50,7 +42,7 @@ dependencies = [
 #    "Pympler",
     "PyYAML",
     "rich",
-    "s3fs",
+#     "s3fs",
     "scikit-learn",
     "scipy",
     "seaborn",
@@ -65,6 +57,7 @@ dependencies = [
     "typer",
     "xxhash",
     "zstd",
+    "fastcore",
 ]
 
 requires-python = ">=3.9"

commit 24a7757cc9e314da8c889af20807cc683a61c270
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 12:01:27 2025 -0400

    [FIX] Broken import Tensor

diff --git a/pylot/loss/total_variation.py b/pylot/loss/total_variation.py
index aa57040..b08a7a5 100644
--- a/pylot/loss/total_variation.py
+++ b/pylot/loss/total_variation.py
@@ -1,6 +1,6 @@
 from typing import Optional
 
-from pylot.torch.torchlib import torch
+from pylot.torch.torchlib import torch, Tensor
 from .util import _loss_module_from_func
 
 

commit d2a461d0f6b7a9dd0e3ac036191e99c92b82a227
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Jun 3 11:59:07 2025 -0400

    [FIX] Broken import nn and F

diff --git a/pylot/loss/flat.py b/pylot/loss/flat.py
index ee77585..0c6389e 100644
--- a/pylot/loss/flat.py
+++ b/pylot/loss/flat.py
@@ -1,5 +1,5 @@
 from functools import wraps
-from pylot.torch.torchlib import nn
+from pylot.torch.torchlib import nn, F
 
 
 def flatten_loss(loss_func, keep_channels=False):

commit f7e3977b2a011e6511c6d3472dcbc12d156d65c2
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Mon Jun 2 19:46:45 2025 -0400

    [FIX] Reflect removing datasets package in __init__

diff --git a/pylot/__init__.py b/pylot/__init__.py
index 5448757..20ba46b 100644
--- a/pylot/__init__.py
+++ b/pylot/__init__.py
@@ -47,7 +47,6 @@ mind.
 
 from . import analysis
 from . import callbacks
-from . import datasets
 from . import experiment
 from . import loss
 from . import metrics
@@ -61,7 +60,6 @@ from . import util
 
 from .analysis import *
 from .callbacks import *
-from .datasets import *
 from .experiment import *
 from .loss import *
 from .metrics import *

commit af6232c3549b54970f7ef77b315eac4258bf6d3e
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Mon Jun 2 19:46:00 2025 -0400

    [ARCH] Remove datasets package

diff --git a/pylot/datasets/__init__.py b/pylot/datasets/__init__.py
deleted file mode 100644
index 4eee280..0000000
--- a/pylot/datasets/__init__.py
+++ /dev/null
@@ -1,13 +0,0 @@
-"""Dataset module inclduing preprocessing and custom datasets
-
-The wrappers here include proper
-"""
-
-from .util import train_val_split, stratified_train_val_split
-from .path import dataset_path, DatapathMixin
-from .indexed import IndexedImageFolder, IndexedDatasetFolder, IndexedImageDataset
-from .subset import subset_dataset
-from .selfsuper import SelfsuperDataset, self_supervised
-from .vision import *
-from .image_folder_tar import ImageFolderTar
-from .cuda import CUDACachedDataset
diff --git a/pylot/datasets/cuda.py b/pylot/datasets/cuda.py
deleted file mode 100644
index 39fde40..0000000
--- a/pylot/datasets/cuda.py
+++ /dev/null
@@ -1,20 +0,0 @@
-from pylot.torch.torchlib import torch, Dataset
-from ..util import to_device
-
-
-class CUDACachedDataset(Dataset):
-    def __init__(self, dataset: Dataset):
-        assert torch.cuda.is_available()
-        self._dataset = dataset
-        self._cache = [to_device(i, "cuda") for i in self._dataset]
-
-    def __getitem__(self, idx):
-        return self._cache[idx]
-
-    def __getattr__(self, key):
-        # This works because __getattr__ is only called as last resort
-        # https://stackoverflow.com/questions/2405590/how-do-i-override-getattr-without-breaking-the-default-behavior
-        return getattr(self._dataset, key)
-
-    def __len__(self):
-        return len(self._cache)
diff --git a/pylot/datasets/download/miniplaces.py b/pylot/datasets/download/miniplaces.py
deleted file mode 100755
index ea0d271..0000000
--- a/pylot/datasets/download/miniplaces.py
+++ /dev/null
@@ -1,75 +0,0 @@
-#!/usr/bin/env python
-import shutil
-import pathlib
-import subprocess
-
-
-def download_miniplaces():
-    root = pathlib.Path()
-    download_cmds = [
-        'curl -O http://miniplaces.csail.mit.edu/data/data.tar.gz',
-        'curl -O https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/categories.txt',
-        'curl -O https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt',
-        'curl -O https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt',
-        'tar -xvzf data.tar.gz images/',
-        'rm data.tar.gz',
-    ]
-
-    for cmd in download_cmds:
-        subprocess.run(cmd.split(' '))
-
-    # mv images/x x
-    for f in ('train', 'val', 'test'):
-        fromf = (root / 'images' / f)
-        if fromf.exists():
-            tof = (root / f).as_posix()
-            shutil.move(fromf.as_posix(), tof)
-
-    (root / 'images').rmdir()
-
-    # Read category files
-    categories = {}  # name -> int
-    with open(root / 'categories.txt', 'r') as f:
-        for line in f.read().strip().split('\n'):
-            p, i = line.split(' ')
-            categories[p[1:]] = i
-
-    from_to = {c: c.split('/')[1] for c in categories}
-
-    # Move train files
-
-    trash = set()
-    for fromf, tof in from_to.items():
-
-        (root / 'val' / tof).mkdir(parents=True, exist_ok=True)
-
-        fromf = (root / 'train' / fromf).as_posix()
-        tof = (root / 'train' / tof).as_posix()
-
-        shutil.move(fromf, tof)
-        trash.add("/".join(fromf.split("/")[:-1]))
-
-    # remove train empty folders
-    for folder in sorted(trash, key=lambda x: -len(x)):
-        f = pathlib.Path(folder)
-        if f.exists():
-            f.rmdir()
-
-    # inverse categories
-    icategories = {i: c.split('/')[1] for c, i in categories.items()}
-    # map files to their folders
-    val = {}
-    with open(root / 'val.txt', 'r') as f:
-        for line in f.read().strip().split('\n'):
-            fromf, c = line.split(' ')
-            val[fromf] = icategories[c]
-
-    # Relocate val files
-    for fromf, tof in val.items():
-        fromf = (root / fromf).as_posix()
-        tof = (root / 'val' / tof).as_posix()
-        shutil.move(fromf, tof)
-
-
-if __name__ == "__main__":
-    download_miniplaces()
diff --git a/pylot/datasets/fetch.py b/pylot/datasets/fetch.py
deleted file mode 100644
index 5e589d7..0000000
--- a/pylot/datasets/fetch.py
+++ /dev/null
@@ -1,15 +0,0 @@
-import pathlib
-from typing import Union
-from ..util import S3Path
-
-
-def fetch_dataset(name: str, dest: Union[str, pathlib.Path]):
-
-    path = S3Path(f'data/{name}')
-    target = pathlib.Path(dest)
-
-    assert path.exists(), f'Folder s3://data/{name} does not exist'
-
-    path._s3driver.get(str(path), target / name)
-
-
diff --git a/pylot/datasets/image_folder_tar.py b/pylot/datasets/image_folder_tar.py
deleted file mode 100644
index 05212e2..0000000
--- a/pylot/datasets/image_folder_tar.py
+++ /dev/null
@@ -1,71 +0,0 @@
-# from https://gist.github.com/rwightman/5a7c9232eb57da20afa80cedb8ac2d38
-import os
-import re
-
-import tarfile
-from PIL import Image
-
-from ..util import autoload
-from pylot.torch.torchlib import torch, Dataset
-
-IMG_EXTENSIONS = [".png", ".jpg", ".jpeg"]
-
-
-def natural_key(string_):
-    """See http://www.codinghorror.com/blog/archives/001018.html"""
-    return [int(s) if s.isdigit() else s for s in re.split(r"(\d+)", string_.lower())]
-
-
-def _extract_tar_info(tarfile, prefix=None):
-    class_to_idx = {}
-    files = []
-    targets = []
-    for ti in tarfile.getmembers():
-        if not ti.isfile():
-            continue
-        dirname, basename = os.path.split(ti.path)
-        if prefix is not None and not dirname.startswith("./" + prefix):
-            continue
-        target = os.path.basename(dirname)
-        class_to_idx[target] = None
-        ext = os.path.splitext(basename)[1]
-        if ext.lower() in IMG_EXTENSIONS:
-            files.append(ti)
-            targets.append(target)
-    for idx, c in enumerate(sorted(class_to_idx.keys(), key=natural_key)):
-        class_to_idx[c] = idx
-    tarinfo_and_targets = zip(files, [class_to_idx[t] for t in targets])
-    tarinfo_and_targets = sorted(
-        tarinfo_and_targets, key=lambda k: natural_key(k[0].path)
-    )
-    return tarinfo_and_targets
-
-
-class ImageFolderTar(data.Dataset):
-    def __init__(self, root, transform=None, prefix=None, index=None):
-
-        assert os.path.isfile(root)
-        self.root = root
-        if index is not None:
-            self.imgs = autoload(index)
-        else:
-            with tarfile.open(
-                root
-            ) as tf:  # cannot keep this open across processes, reopen later
-                self.imgs = _extract_tar_info(tf, prefix=prefix)
-        self.tarfile = None  # lazy init in __getitem__
-        self.transform = transform
-        self.targets = torch.Tensor([i for _, i in self.imgs])
-
-    def __getitem__(self, index):
-        if self.tarfile is None:
-            self.tarfile = tarfile.open(self.root)
-        tarinfo, target = self.imgs[index]
-        iob = self.tarfile.extractfile(tarinfo)
-        img = Image.open(iob).convert("RGB")
-        if self.transform is not None:
-            img = self.transform(img)
-        return img, target
-
-    def __len__(self):
-        return len(self.imgs)
diff --git a/pylot/datasets/indexed.py b/pylot/datasets/indexed.py
deleted file mode 100644
index e7ee839..0000000
--- a/pylot/datasets/indexed.py
+++ /dev/null
@@ -1,137 +0,0 @@
-import json
-import os
-import pathlib
-
-from torchvision.datasets import VisionDataset
-from torchvision.datasets.folder import make_dataset, IMG_EXTENSIONS, default_loader
-
-
-def removeprefix(s, prefix):
-    if s.startswith(prefix):
-        s = s[len(prefix) :]
-    return s
-
-
-class IndexedDatasetFolder(VisionDataset):
-    def __init__(
-        self,
-        root,
-        loader,
-        extensions=None,
-        transform=None,
-        target_transform=None,
-        is_valid_file=None,
-    ):
-        super().__init__(root, transform=transform, target_transform=target_transform)
-
-        index = pathlib.Path(root).with_suffix(".json")
-        root_prefix = str(root) + "/"
-        if not index.exists():
-            classes, class_to_idx = self._find_classes(self.root)
-            samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file)
-            if not index.exists():
-                # Navigating the FS can take long, some processs might have
-                # Finished before we do
-                labels = [label for _, label in samples]
-                relative_paths = [
-                    removeprefix(path, root_prefix) for path, _ in samples
-                ]
-                cache = {
-                    "classes": classes,
-                    "class_to_idx": class_to_idx,
-                    "labels": labels,
-                    "relative_paths": relative_paths,
-                }
-                with open(index, "w") as f:
-                    json.dump(cache, f)
-        else:
-            with open(index, "r") as f:
-                cache = json.load(f)
-                classes, class_to_idx = cache["classes"], cache["class_to_idx"]
-                paths = [root_prefix + rel_path for rel_path in cache["relative_paths"]]
-                samples = [(path, label) for path, label in zip(paths, cache["labels"])]
-
-        if len(samples) == 0:
-            msg = "Found 0 files in subfolders of: {}\n".format(self.root)
-            if extensions is not None:
-                msg += "Supported extensions are: {}".format(",".join(extensions))
-            raise RuntimeError(msg)
-
-        self.loader = loader
-        self.extensions = extensions
-
-        self.classes = classes
-        self.class_to_idx = class_to_idx
-        self.samples = samples
-        self.targets = [s[1] for s in samples]
-
-    def _find_classes(self, dir):
-        """
-        Finds the class folders in a dataset.
-
-        Args:
-            dir (string): Root directory path.
-
-        Returns:
-            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.
-
-        Ensures:
-            No class is a subdirectory of another.
-        """
-        classes = [d.name for d in os.scandir(dir) if d.is_dir()]
-        classes.sort()
-        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
-        return classes, class_to_idx
-
-    def __getitem__(self, index):
-        """
-        Args:
-            index (int): Index
-
-        Returns:
-            tuple: (sample, target) where target is class_index of the target class.
-        """
-        path, target = self.samples[index]
-        sample = self.loader(path)
-        if self.transform is not None:
-            sample = self.transform(sample)
-        if self.target_transform is not None:
-            target = self.target_transform(target)
-
-        return sample, target
-
-    def __len__(self):
-        return len(self.samples)
-
-    @property
-    def filepaths(self):
-        index = pathlib.Path(self.root).with_suffix(".json")
-        with index.open("r") as f:
-            cache = json.load(f)
-        return cache["relative_paths"]
-
-
-class IndexedImageFolder(IndexedDatasetFolder):
-    def __init__(
-        self,
-        root,
-        transform=None,
-        target_transform=None,
-        loader=default_loader,
-        is_valid_file=None,
-    ):
-        super().__init__(
-            root,
-            loader,
-            IMG_EXTENSIONS if is_valid_file is None else None,
-            transform=transform,
-            target_transform=target_transform,
-            is_valid_file=is_valid_file,
-        )
-        self.imgs = self.samples
-
-
-def IndexedImageDataset(root, train=True, **kwargs):
-    root = pathlib.Path(root)
-    root /= "train" if train else "val"
-    return IndexedImageFolder(root, **kwargs)
diff --git a/pylot/datasets/lmdb.py b/pylot/datasets/lmdb.py
deleted file mode 100644
index a75f6a4..0000000
--- a/pylot/datasets/lmdb.py
+++ /dev/null
@@ -1,201 +0,0 @@
-# Some inspiration from:
-# - https://github.com/rmccorm4/PyTorch-LMDB/blob/master/folder2lmdb.py
-# - https://junyonglee.me/research/pytorch/How-to-use-LMDB-with-PyTorch-DataLoader/
-# - https://github.com/Lyken17/Efficient-PyTorch/blob/master/tools/folder2lmdb.py
-# Some background on LMDB: https://blogs.kolabnow.com/2018/06/07/a-short-guide-to-lmdb
-
-
-import io
-import math
-from concurrent.futures import ProcessPoolExecutor
-from typing import Callable, Literal, Optional
-
-import lmdb
-import numpy as np
-from PIL import Image
-from torchvision.datasets import VisionDataset
-from tqdm.auto import tqdm
-
-from pylot.datasets import IndexedImageFolder
-from pylot.util.ioutil import autodecode, autoencode
-
-
-class ImageFolderLMDB(VisionDataset):
-    def __init__(
-        self,
-        root: str,
-        transform: Optional[Callable] = None,
-        target_transform: Optional[Callable] = None,
-    ):
-        super().__init__(root, transform=transform, target_transform=target_transform)
-        env = lmdb.open(str(self.root), readonly=True, lock=False)
-        with env.begin(write=False) as txn:
-            index = autodecode(txn.get(b"_index.msgpack"), ".msgpack")
-            self.__dict__.update(index)
-
-        self.samples = list(zip(self.paths, self.targets))
-
-    @property
-    def class_to_idx(self):
-        return {k: i for i, k in enumerate(self.classes)}
-
-    def __len__(self):
-        return len(self.samples)
-
-    def __getitem__(self, i):
-        if not hasattr(self, "_env"):
-            self._env = lmdb.open(
-                str(self.root),
-                readonly=True,
-                lock=False,
-                readahead=False,
-                meminit=False,
-            )
-            self._txn = self._env.begin(write=False)
-
-        path = self.paths[i]
-        target = self.targets[i]
-
-        # with self._env.begin(write=False) as txn:
-        #     _bytes = txn.get(path.encode())
-        _bytes = self._txn.get(path.encode())
-
-        if self.write_mode == "compressed":
-            sample = Image.open(io.BytesIO(_bytes))
-            if sample.mode == "RGBA" or sample.mode == "L":
-                sample = sample.convert("RGB")
-        else:
-            sample = Image.fromarray(autodecode(_bytes, ".msgpack"))
-        if self.transform is not None:
-            sample = self.transform(sample)
-        if self.target_transform is not None:
-            target = self.target_transform(target)
-
-        return sample, target
-
-    def extra_repr(self) -> str:
-        compression = {"compressed": "JPG", "raw": "RAW"}[self.write_mode]
-        return f"Compression: {compression}"
-
-    @classmethod
-    def fromfolder(
-        cls,
-        folder: str,
-        outfile: str,
-        max_workers: int = 16,
-        write_mode: Literal["compressed", "raw"] = "compressed",
-    ) -> "ImageFolderLMDB":
-
-        dataset = IndexedImageFolder(folder)
-        relpaths = dataset.filepaths
-        paths, labels = zip(*dataset.samples)
-
-        index = {
-            "classes": dataset.classes,
-            "paths": relpaths,
-            "targets": labels,
-            "write_mode": write_mode,
-        }
-
-        global _load  # needed to trick concurrent executor
-
-        def _load(path):
-            if write_mode == "compressed":
-                with open(path, "rb") as f:
-                    return f.read()
-            else:
-                x = np.array(Image.open(path).convert("RGB"))
-                return autoencode(x, ".msgpack")
-
-        # We use lmdbm for writing only as it makes autogrowing the DB easier
-        from lmdbm import Lmdb
-
-        with Lmdb.open(outfile, "c", map_size=2 ** 32) as db:
-            db["_index.msgpack"] = autoencode(index, "_index.msgpack")
-
-            with ProcessPoolExecutor(max_workers=max_workers) as executor:
-                for i, image_bytes in enumerate(
-                    tqdm(executor.map(_load, paths), total=len(dataset))
-                ):
-                    db[relpaths[i]] = image_bytes
-        return cls(outfile)
-
-    @classmethod
-    def fromdataset(
-        cls, dataset: VisionDataset, outfile: str, max_workers: int = 0,
-    ) -> "ImageFolderLMDB":
-
-        classes = (
-            dataset.classes
-            if hasattr(dataset, "classes")
-            else sorted(np.unique(dataset.targets).tolist())
-        )
-
-        global _load
-
-        def _load(idx):
-            image, _ = dataset[idx]
-            array = np.array(image.convert("RGB"))
-            return autoencode(array, ".msgpack")
-
-        N = math.ceil(math.log10(len(dataset)))
-
-        # We use lmdbm for writing only as it makes autogrowing the DB easier
-        from lmdbm import Lmdb
-
-        with Lmdb.open(outfile, "c", map_size=2 ** 32) as db:
-
-            paths = []
-
-            if max_workers > 0:
-                with ProcessPoolExecutor(max_workers=max_workers) as executor:
-                    for i, image_bytes in enumerate(
-                        tqdm(
-                            executor.map(_load, range(len(dataset))), total=len(dataset)
-                        )
-                    ):
-                        path = f"{i:0{N}d}"
-                        db[path] = image_bytes
-                        paths.append(path)
-            else:
-                for i in tqdm(range(len(dataset))):
-                    path = f"{i:0{N}d}"
-                    db[path] = _load(i)
-                    paths.append(path)
-
-            index = {
-                "classes": classes,
-                "paths": paths,
-                "targets": dataset.targets,
-                "write_mode": "raw",
-            }
-
-            db["_index.msgpack"] = autoencode(index, "_index.msgpack")
-
-        return cls(outfile)
-
-
-if __name__ == "__main__":
-    import sys
-    from pathlib import Path
-
-    import typer
-
-    def convert(
-        source_folder: Path,
-        outfile: Path,
-        max_workers: int = typer.Option(16, "-P", "--max-workers"),
-        write_mode: str = "compressed",
-    ):
-        if not source_folder.exists():
-            print(f"Source path {source_folder} does not exist")
-            sys.exit(1)
-        outfile = outfile.with_suffix(".lmdb")
-        ImageFolderLMDB.fromfolder(
-            str(source_folder),
-            str(outfile),
-            max_workers=max_workers,
-            write_mode=write_mode,
-        )
-
-    typer.run(convert)
diff --git a/pylot/datasets/organize/convert_to_imagefolder.py b/pylot/datasets/organize/convert_to_imagefolder.py
deleted file mode 100644
index ab7bbb6..0000000
--- a/pylot/datasets/organize/convert_to_imagefolder.py
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/usr/bin/env python
-
-# Script for converting from original TinyImagenet format to
-# Pytorch ImageFolder format.
-
-import pathlib
-import shutil
-
-
-def main():
-    val = pathlib.Path('val')
-    valorig = pathlib.Path('val_orig')
-    valimages = valorig / 'images'
-    annotations = valorig / 'val_annotations.txt'
-
-    with open(annotations) as infile:
-        filename_category_map = {}
-        for line in infile:
-            filename, category = line.split()[:2]
-            filename_category_map[filename] = category
-
-    for category in set(filename_category_map.values()):
-        (val / category).mkdir(parents=True, exist_ok=True)
-
-    for p in valimages.iterdir():
-        filename = p.name
-        category = filename_category_map[filename]
-        shutil.copy(p, val / category / filename)
-
-    print('Done')
-
-
-if __name__ == '__main__':
-    main()
diff --git a/pylot/datasets/path.py b/pylot/datasets/path.py
deleted file mode 100644
index 05564d6..0000000
--- a/pylot/datasets/path.py
+++ /dev/null
@@ -1,50 +0,0 @@
-import os
-import pathlib
-
-
-def dataset_path(dataset, path=None):
-    """Get the path to a specified dataset
-
-    Arguments:
-        dataset {str} -- Name of the dataset
-
-    Keyword Arguments:
-        path {str} -- Semicolon separated list of paths to look for dataset folders (default: {None})
-
-    Returns:
-        dataset_path -- pathlib.Path for the first match
-
-    Raises:
-        ValueError -- If no path is provided and DATAPATH is not set
-        LookupError -- If the given dataset cannot be found
-    """
-    if path is None:
-        # Look for the dataset in known paths
-        if "DATAPATH" in os.environ:
-            path = os.environ["DATAPATH"]
-            paths = [pathlib.Path(p) for p in path.split(":")]
-        else:
-            raise ValueError(
-                f"No path specified. A path must be provided, or the folder must be listed in your DATAPATH"
-            )
-
-    paths = [pathlib.Path(p) for p in path.split(":")]
-
-    for p in paths:
-        p = (p / dataset).resolve()
-        if p.exists():
-            return p
-    raise LookupError(f"Could not find {dataset} in {paths}")
-
-
-class DatapathMixin:
-    @property
-    def path(self):
-        root = getattr(self, "root", None)
-        if root:
-            return root
-        return dataset_path(self._folder_name, root)
-
-    @property
-    def _folder_name(self):
-        return self.__class__.__name__
diff --git a/pylot/datasets/prepare.py b/pylot/datasets/prepare.py
deleted file mode 100644
index 1255458..0000000
--- a/pylot/datasets/prepare.py
+++ /dev/null
@@ -1,47 +0,0 @@
-# From albumentations example
-# https://albumentations.ai/docs/examples/pytorch_semantic_segmentation/
-from urllib.request import urlretrieve
-import os
-import shutil
-from tqdm.auto import tqdm
-
-# torchvision datasets provides similar (and more sophisticated) tools under torchvision.datasets.utils
-
-
-class TqdmUpTo(tqdm):
-    def update_to(self, b=1, bsize=1, tsize=None):
-        if tsize is not None:
-            self.total = tsize
-        self.update(b * bsize - self.n)
-
-
-def download_url(url, filepath):
-    directory = os.path.dirname(os.path.abspath(filepath))
-    os.makedirs(directory, exist_ok=True)
-    if os.path.exists(filepath):
-        print("Dataset already exists on the disk. Skipping download.")
-        return
-
-    with TqdmUpTo(
-        unit="B",
-        unit_scale=True,
-        unit_divisor=1024,
-        miniters=1,
-        desc=os.path.basename(filepath),
-    ) as t:
-        urlretrieve(url, filename=filepath, reporthook=t.update_to, data=None)
-        t.total = t.n
-
-
-def extract_archive(filepath):
-    extract_dir = os.path.dirname(os.path.abspath(filepath))
-    shutil.unpack_archive(filepath, extract_dir)
-
-
-def download_and_extract_archive(url, root):
-    filename = os.path.basename(url)
-    filename = os.path.join(root, filename)
-    print(f"Downloading {url} to {filename}")
-    download_url(url, filename)
-    print(f"Extracting {filename} to {root}")
-    extract_archive(filename)
diff --git a/pylot/datasets/segmentation/__init__.py b/pylot/datasets/segmentation/__init__.py
deleted file mode 100644
index c2da98a..0000000
--- a/pylot/datasets/segmentation/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from .ellipse_segmentation import EllipseSegmentation
\ No newline at end of file
diff --git a/pylot/datasets/segmentation/ellipse_segmentation.py b/pylot/datasets/segmentation/ellipse_segmentation.py
deleted file mode 100644
index 1b28e92..0000000
--- a/pylot/datasets/segmentation/ellipse_segmentation.py
+++ /dev/null
@@ -1,89 +0,0 @@
-import random
-import numpy as np
-import cv2
-
-# Ellipse toy data generators
-def gen_random_image(shape):
-    H, W, C = shape
-    img = np.zeros((H, W, C), dtype=np.uint8)
-    mask = np.zeros((H, W), dtype=np.uint8)
-
-    # Background
-    dark_color0 = random.randint(0, 100)
-    dark_color1 = random.randint(0, 100)
-    dark_color2 = random.randint(0, 100)
-    img[:, :, 0] = dark_color0
-    img[:, :, 1] = dark_color1
-    img[:, :, 2] = dark_color2
-
-    # Object
-    light_color0 = random.randint(dark_color0+1, 255)
-    light_color1 = random.randint(dark_color1+1, 255)
-    light_color2 = random.randint(dark_color2+1, 255)
-    center_0 = random.randint(0, H)
-    center_1 = random.randint(0, W)
-    r1 = random.randint(10, 56)
-    r2 = random.randint(10, 56)
-    cv2.ellipse(img, (center_0, center_1), (r1, r2), 0, 0, 360, (light_color0, light_color1, light_color2), -1)
-    cv2.ellipse(mask, (center_0, center_1), (r1, r2), 0, 0, 360, 255, -1)
-
-    # White noise
-    density = random.uniform(0, 0.1)
-    for i in range(H):
-        for j in range(W):
-            if random.random() < density:
-                img[i, j, 0] = random.randint(0, 255)
-                img[i, j, 1] = random.randint(0, 255)
-                img[i, j, 2] = random.randint(0, 255)
-
-    return img, mask
-
-def EllipseGenerator(batch_size, shape):
-    while True:
-        image_list = []
-        mask_list = []
-        for i in range(batch_size):
-            img, mask = gen_random_image(shape)
-            image_list.append(img)
-            mask_list.append(mask)
-
-        image_list = np.array(image_list, dtype=np.float64)
-        image_list /= 255.0
-        image_list -= 0.5
-        mask_list = np.array(mask_list, dtype=np.float64)
-        mask_list /= 255.0
-        mask_list = mask_list[..., np.newaxis]
-        yield image_list, mask_list
-
-
-from torch.utils.data import Dataset
-
-class EllipseSegmentation(Dataset):
-
-
-    def __init__(self, shape, epoch=1, normalize=True):
-        if isinstance(shape, int):
-            self.shape = (shape, shape, 3)
-        elif isinstance(shape, tuple):
-            self.shape = (*shape[:2], 3)
-
-        self.epoch = epoch
-        self.normalize = normalize
-
-    def __len__(self):
-        return self.epoch
-
-    def __getitem__(self, idx):
-        img, mask = gen_random_image(self.shape)
-        img = np.array(img, dtype=np.float32)
-        mask = np.array(mask, dtype=np.float32)[..., np.newaxis]
-
-        if self.normalize:
-            img = (img / 255.0) - 0.5
-            mask /= 255.0
-
-        # Torch wants channels first
-        img = img.transpose((2,0,1))
-        mask = mask.transpose((2,0,1))
-
-        return img, mask
\ No newline at end of file
diff --git a/pylot/datasets/selfsuper.py b/pylot/datasets/selfsuper.py
deleted file mode 100644
index 07b93f2..0000000
--- a/pylot/datasets/selfsuper.py
+++ /dev/null
@@ -1,34 +0,0 @@
-from functools import wraps
-from typing import Tuple
-
-from torch import Tensor
-from torch.utils.data import Dataset
-
-
-class SelfsuperDataset(Dataset):
-    def __init__(self, inner_dataset: Dataset):
-        self.inner_dataset = inner_dataset
-
-    def __getitem__(self, index) -> Tuple[Tensor, Tensor]:
-        x = self.inner_dataset[index]
-        if isinstance(x, tuple):
-            x = x[0]
-        return x, x
-
-    def __len__(self) -> int:
-        return len(self.inner_dataset)  # type: ignore
-
-
-def self_supervised(dataset: Dataset) -> Dataset:
-    def duplicate(f):
-        @wraps(f)
-        def duplicated(index):
-            x = f(index)
-            if isinstance(x, tuple):
-                x = x[0]
-            return x, x
-
-        return duplicated
-
-    dataset.__getitem__ = duplicate(dataset.__getitem__)  # type: ignore
-    return dataset
diff --git a/pylot/datasets/split.py b/pylot/datasets/split.py
deleted file mode 100644
index c03b130..0000000
--- a/pylot/datasets/split.py
+++ /dev/null
@@ -1,35 +0,0 @@
-from typing import List, Tuple
-
-from pydantic import validate_call
-
-
-
-@validate_call
-def data_splits(
-    values: List[str], splits: Tuple[float, float, float], seed: int
-) -> Tuple[List[str], List[str], List[str]]:
-    from sklearn.model_selection import train_test_split
-
-    if len(set(values)) != len(values):
-        raise ValueError(f"Duplicate entries found in values")
-
-    if (s := sum(splits)) != 1.0:
-        raise ValueError(f"Splits must add up to 1.0, got {splits}->{s}")
-
-    train_size, val_size, test_size = splits
-    values = sorted(values)
-    if test_size == 0.0:
-        trainval, test = values, []
-    else:
-        trainval, test = train_test_split(
-            values, test_size=test_size, random_state=seed
-        )
-    val_ratio = val_size / (train_size + val_size)
-    if val_size == 0.0:
-        train, val = trainval, []
-    else:
-        train, val = train_test_split(trainval, test_size=val_ratio, random_state=seed)
-
-    assert sorted(train + val + test) == values, "Missing Values"
-
-    return (train, val, test)
diff --git a/pylot/datasets/subset.py b/pylot/datasets/subset.py
deleted file mode 100644
index a598d1b..0000000
--- a/pylot/datasets/subset.py
+++ /dev/null
@@ -1,40 +0,0 @@
-from random import shuffle
-from collections import defaultdict
-from copy import deepcopy
-
-# Subset a DatasetFolder or a ImageFolder
-def subset_dataset(
-    dataset, n_classes=None, samples_per_class=None, random=False, inplace=False
-):
-    if not inplace:
-        dataset = deepcopy(dataset)
-    # Subset classes
-    if n_classes is not None:
-        if random:
-            shuffle(dataset.classes)
-        dataset.classes = dataset.classes[:n_classes]
-        dataset.class_to_idx = {
-            k: v for k, v in dataset.class_to_idx.items() if k in set(dataset.classes)
-        }
-        idxs = set(dataset.class_to_idx.values())
-        dataset.targets = [i for i in dataset.targets if i in idxs]
-        dataset.samples = [i for i in dataset.samples if i[1] in idxs]
-
-    # Subset samples per class
-    if random:
-        shuffle(dataset.samples)
-
-    count = defaultdict(int)
-    new_targets = []
-    new_samples = []
-    for sample in dataset.samples:
-        target = sample[1]
-        if count[target] < samples_per_class:
-            count[target] += 1
-            new_targets.append(target)
-            new_samples.append(sample)
-
-    dataset.samples = new_samples
-    dataset.targets = new_targets
-    return dataset
-
diff --git a/pylot/datasets/thunder.py b/pylot/datasets/thunder.py
deleted file mode 100644
index 9cffa23..0000000
--- a/pylot/datasets/thunder.py
+++ /dev/null
@@ -1,56 +0,0 @@
-import pathlib
-from typing import List
-
-from pydantic import validate_call
-
-from torch.utils.data import Dataset
-
-from ..util import ThunderLoader, ThunderReader, UniqueThunderReader
-
-
-class ThunderDataset(Dataset):
-    @validate_call
-    def __init__(
-        self, path: pathlib.Path, preload: bool = False, reuse_fp: bool = True
-    ):
-        self._path = path
-        self.preload = preload
-        if preload:
-            self._db = ThunderLoader(path)
-        elif reuse_fp:
-            self._db = UniqueThunderReader(path)
-        else:
-            self._db = ThunderReader(path)
-
-        if "samples" in self._db.keys():
-            self.samples: List[str] = self._db["_samples"]
-        self.attrs = self._db.get("_attrs", {})
-
-    def _load(self, key):
-        true_key = self.samples[key]
-        data = self._db[true_key]
-        return data
-
-    def __getitem__(self, key):
-        return self._load(key)
-
-    def __len__(self):
-        return len(self.samples)
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}({repr(str(self._path))})"
-
-    @staticmethod
-    def supress_readonly_warning():
-        """
-        Ignores warnings about non-writable numpy arrays
-        when constructing torch objects. This is ok, 99%
-        of the time since data is not backprop'ed or 
-        modified in place (e.g. changing dtype forces copy).
-        Still, not called by default, subclass should decide
-        whether to call it upon __init__
-        """
-        import warnings
-
-        msg = """The given NumPy array is not writable.*"""
-        warnings.filterwarnings("ignore", msg, UserWarning)
diff --git a/pylot/datasets/util.py b/pylot/datasets/util.py
deleted file mode 100644
index f97bfd5..0000000
--- a/pylot/datasets/util.py
+++ /dev/null
@@ -1,26 +0,0 @@
-import math
-import numpy as np
-from pylot.torch.torchlib import torch
-from torch.utils.data import random_split, Subset
-
-
-def train_val_split(dataset, val_split, seed=None):
-    dataset_size = len(dataset)
-    val_size = math.ceil(val_split * dataset_size)
-    train_size = dataset_size - val_size
-    kwargs = {}
-    if seed is not None:
-        kwargs["generator"] = torch.Generator().manual_seed(seed)
-    return random_split(dataset, [train_size, val_size], **kwargs)
-
-
-def stratified_train_val_split(dataset, val_split, seed=None):
-    from sklearn.model_selection import train_test_split
-
-    indices = np.arange(len(dataset))
-    stratify = getattr(dataset, 'targets', None)
-    train_indices, val_indices = train_test_split(
-        indices, test_size=val_split, random_state=seed, stratify=stratify
-    )
-    return Subset(dataset, train_indices), Subset(dataset, val_indices)
-
diff --git a/pylot/datasets/vision/__init__.py b/pylot/datasets/vision/__init__.py
deleted file mode 100644
index f74fd86..0000000
--- a/pylot/datasets/vision/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-from .main import *
-from .nist import *
-from .pascal import PascalVOC2012
-from .oxfordpets import OxfordPets
-from .cubbirds import CUBBirds
diff --git a/pylot/datasets/vision/cubbirds.py b/pylot/datasets/vision/cubbirds.py
deleted file mode 100644
index 1888bcf..0000000
--- a/pylot/datasets/vision/cubbirds.py
+++ /dev/null
@@ -1,155 +0,0 @@
-from functools import lru_cache
-from typing import Tuple
-import os
-
-import numpy as np
-import pandas as pd
-from PIL import Image
-from torch.utils.data import Dataset
-from torchvision.datasets.utils import download_file_from_google_drive, extract_archive
-
-from ..path import DatapathMixin
-
-
-class CUBBirds(Dataset, DatapathMixin):
-    """
-    http://www.vision.caltech.edu/visipedia/CUB-200-2011.html
-    """
-
-    MODES = ("cls", "seg")
-    MEAN_CH = (0.486, 0.5, 0.433)
-    STD_CH = (0.232, 0.228, 0.267)
-
-    def __init__(
-        self,
-        split,
-        download=False,
-        root=None,
-        cache=False,
-        preproc=False,
-        mode="cls",
-        size: Tuple[int, int] = (192, 256),
-        augmentation=False,
-    ):
-        train = (split == 'train')
-
-        assert mode in self.MODES, f"Mode {mode} not one of {','.join(self.MODES)}"
-        assert not download or (
-            root is not None
-        ), "For download, root must be specified"
-
-        self.mode = mode
-        self.train = train
-        self.root = root
-        self.cache = cache
-        self.preproc = preproc
-        self.size = size
-        self.augmentation = augmentation
-
-        if download:
-            self._download()
-
-        self._load_annotations()
-
-        if self.preproc:
-            self._load_transforms()
-
-        if self.cache:
-            if self.augmentation:
-                self._get_image = lru_cache(maxsize=None)(self._get_image)
-                self._get_mask = lru_cache(maxsize=None)(self._get_mask)
-            else:
-                self.__getitem__ = lru_cache(maxsize=None)(self.__getitem__)
-
-    def _load_annotations(self):
-        def load_txt(file):
-            return pd.read_csv(
-                self.path / "CUB_200_2011" / file, index_col=0, sep=" ", names=["data"]
-            )
-
-        selector = load_txt("train_test_split.txt").data == int(self.train)
-        self._files = load_txt("images.txt")[selector].data.values
-        self._labels = load_txt("image_class_labels.txt")[selector].data.values - 1
-        self._classes = load_txt("classes.txt").data.tolist()
-        self.images_path = self.path / "CUB_200_2011/images"
-
-    def _load_transforms(self):
-        import albumentations as A
-        from albumentations.pytorch import ToTensorV2
-        transforms = []
-        if self.size:
-            height, width = self.size
-
-            if self.train and self.augmentation:
-                transforms = [
-                    A.RandomResizedCrop(height, width),
-                    A.HorizontalFlip(p=0.5),
-                ]
-            else:
-                transforms = [A.Resize(height, width)]
-
-        # For either train/val we need to Normalize channels and convert to tensor
-        transforms += [
-            A.Normalize(mean=self.MEAN_CH, std=self.STD_CH),
-            ToTensorV2(),
-        ]
-        self.transform = A.Compose(transforms)
-
-    @property
-    def classes(self):
-        return self._classes
-
-    def __len__(self):
-        return len(self._files)
-
-    def __getitem__(self, index):
-        image = self._get_image(index)
-        if self.mode == "cls":
-            label = self._labels[index]
-            if self.preproc:
-                image = self.transform(image=image)["image"]
-        else:
-            label = self._get_mask(index)
-            if self.preproc:
-                augment = self.transform(image=image, mask=label)
-                image, label = augment["image"], augment["mask"]
-                label = label[None, ...].float()
-
-        return image, label
-
-    def _get_image(self, i):
-        image = Image.open(self.path / "CUB_200_2011/images" / self._files[i]).convert(
-            "RGB"
-        )
-        if self.preproc:
-            image = np.array(image)
-        return image
-
-    def _get_mask(self, i):
-        # Mask images have 5 grey levels (I believe this corresponds to 5 workers)
-        # We consider 1 when at least 3 workers agree, 0 otherwise
-
-        seg = Image.open(
-            (self.path / "segmentations" / self._files[i]).with_suffix(".png")
-        ).convert("L")
-        # return seg
-        mask = np.array(seg, dtype=np.uint8) // 51
-        mask[mask < 3] = 0
-        mask[mask >= 3] = 1
-        if self.preproc:
-            return mask
-        return Image.fromarray(mask * 255)
-
-    def _download(self):
-        download_file_from_google_drive(
-            "1hbzc_P1FuxMkcabkgn9ZKinBwW683j45",
-            self.root,
-            filename="CUB_200_2011.tgz",
-        )
-        download_file_from_google_drive(
-            "1EamOKGLoTuZdtcVYbHMWNpkn3iAVj8TP",
-            self.root,
-            filename="segmentations.tgz",
-        )
-        extract_archive(os.path.join(self.root, "CUB_200_2011.tgz"))
-        extract_archive(os.path.join(self.root, "segmentations.tgz"))
diff --git a/pylot/datasets/vision/emnist.py b/pylot/datasets/vision/emnist.py
deleted file mode 100644
index 6c962f4..0000000
--- a/pylot/datasets/vision/emnist.py
+++ /dev/null
@@ -1,114 +0,0 @@
-import string
-
-from torchvision.datasets.mnist import *
-
-import einops as E
-
-class EMNIST(MNIST):
-    """`EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist>`_ Dataset.
-
-    Args:
-        root (string): Root directory of dataset where ``EMNIST/raw/train-images-idx3-ubyte``
-            and  ``EMNIST/raw/t10k-images-idx3-ubyte`` exist.
-        split (string): The dataset has 6 different splits: ``byclass``, ``bymerge``,
-            ``balanced``, ``letters``, ``digits`` and ``mnist``. This argument specifies
-            which one to use.
-        train (bool, optional): If True, creates dataset from ``training.pt``,
-            otherwise from ``test.pt``.
-        download (bool, optional): If True, downloads the dataset from the internet and
-            puts it in root directory. If dataset is already downloaded, it is not
-            downloaded again.
-        transform (callable, optional): A function/transform that  takes in an PIL image
-            and returns a transformed version. E.g, ``transforms.RandomCrop``
-        target_transform (callable, optional): A function/transform that takes in the
-            target and transforms it.
-    """
-
-    url = "https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip"
-    md5 = "58c8d27c78d21e728a6bc7b3cc06412e"
-    splits = ("byclass", "bymerge", "balanced", "letters", "digits", "mnist")
-    # Merged Classes assumes Same structure for both uppercase and lowercase version
-    _merged_classes = {
-        "c",
-        "i",
-        "j",
-        "k",
-        "l",
-        "m",
-        "o",
-        "p",
-        "s",
-        "u",
-        "v",
-        "w",
-        "x",
-        "y",
-        "z",
-    }
-    _all_classes = set(string.digits + string.ascii_letters)
-    classes_split_dict = {
-        "byclass": sorted(list(_all_classes)),
-        "bymerge": sorted(list(_all_classes - _merged_classes)),
-        "balanced": sorted(list(_all_classes - _merged_classes)),
-        "letters": list(string.ascii_lowercase),
-        "digits": list(string.digits),
-        "mnist": list(string.digits),
-    }
-
-    def __init__(self, root: str, split: str = "letters", **kwargs: Any) -> None:
-        self._split = verify_str_arg(split, "split", self.splits)
-        self.training_file = self._training_file(split)
-        self.test_file = self._test_file(split)
-        super().__init__(root, **kwargs)
-        self.classes = self.classes_split_dict[self._split]
-        if split == 'letters':
-            self.targets -= 1
-
-    @staticmethod
-    def _training_file(split) -> str:
-        return f"training_{split}.pt"
-
-    @staticmethod
-    def _test_file(split) -> str:
-        return f"test_{split}.pt"
-
-    @property
-    def _file_prefix(self) -> str:
-        return f"emnist-{self._split}-{'train' if self.train else 'test'}"
-
-    @property
-    def images_file(self) -> str:
-        return os.path.join(self.raw_folder, f"{self._file_prefix}-images-idx3-ubyte")
-
-    @property
-    def labels_file(self) -> str:
-        return os.path.join(self.raw_folder, f"{self._file_prefix}-labels-idx1-ubyte")
-
-    def _load_data(self):
-        imgs = read_image_file(self.images_file)
-        labels = read_label_file(self.labels_file)
-        imgs = E.rearrange(imgs, 'N w h -> N h w')
-        return imgs, labels
-
-    def _check_exists(self) -> bool:
-        return all(
-            check_integrity(file) for file in (self.images_file, self.labels_file)
-        )
-
-    def download(self) -> None:
-        """Download the EMNIST data if it doesn't exist already."""
-
-        if self._check_exists():
-            return
-
-        os.makedirs(self.raw_folder, exist_ok=True)
-
-        download_and_extract_archive(
-            self.url, download_root=self.raw_folder, md5=self.md5
-        )
-        gzip_folder = os.path.join(self.raw_folder, "gzip")
-        for gzip_file in os.listdir(gzip_folder):
-            if gzip_file.endswith(".gz"):
-                extract_archive(os.path.join(gzip_folder, gzip_file), self.raw_folder)
-        shutil.rmtree(gzip_folder)
-
diff --git a/pylot/datasets/vision/imagenet.py b/pylot/datasets/vision/imagenet.py
deleted file mode 100644
index fbc79b9..0000000
--- a/pylot/datasets/vision/imagenet.py
+++ /dev/null
@@ -1,76 +0,0 @@
-from dataclasses import dataclass
-from typing import Literal
-
-import torchvision.transforms as TF
-
-from ..indexed import IndexedImageFolder
-from ..path import DatapathMixin
-
-from ...util.validation import validate_arguments_init
-from ...util import autoload
-
-
-IMAGENET_MEAN = (0.485, 0.456, 0.406)
-IMAGENET_STD = (0.229, 0.224, 0.225)
-
-SIMPLE_LABELS_URL = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"
-DETAILED_LABELS_URL = 'https://gist.githubusercontent.com/JJGO/d61c1d1eb161dba58e6354fddca1e99a/raw/3b4962e7fb1213be9627b53c2ce1f4043209efc5/imagenet-detailed-labels.json'
-
-
-@validate_arguments_init
-@dataclass(repr=False)
-class ImageNet(IndexedImageFolder, DatapathMixin):
-
-    split: Literal["train", "val"] = "train"
-    transforms: bool = True
-
-    def __post_init__(self):
-        self._filter_warnings()
-        super().__init__(self.path / self.split, transform=self.get_transform())
-
-    def get_transform(self):
-        if not self.transforms:
-            return None
-        if self.split == "train":
-            return TF.Compose(
-                [
-                    TF.RandomResizedCrop(224),
-                    TF.RandomHorizontalFlip(),
-                    TF.ToTensor(),
-                    TF.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
-                ]
-            )
-        elif self.split == "val":
-            return TF.Compose(
-                [
-                    TF.Resize(256),
-                    TF.CenterCrop(224),
-                    TF.ToTensor(),
-                    TF.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
-                ]
-            )
-
-    @staticmethod
-    def _filter_warnings():
-        # ImageNet loading from files can produce benign EXIF errors
-        import warnings
-
-        warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)
-
-    @property
-    def simple_labels(self):
-        file = self.path / "imagenet-simple-labels.json"
-        if not file.exists():
-            from urllib.request import urlretrieve
-
-            urlretrieve(SIMPLE_LABELS_URL, str(file))
-        return autoload(file)
-
-    @property
-    def detailed_labels(self):
-        file = self.path / "imagenet-detailed-labels.json"
-        if not file.exists():
-            from urllib.request import urlretrieve
-
-            urlretrieve(DETAILED_LABELS_URL, str(file))
-        return autoload(file)
diff --git a/pylot/datasets/vision/imagenette.py b/pylot/datasets/vision/imagenette.py
deleted file mode 100644
index a1f6d62..0000000
--- a/pylot/datasets/vision/imagenette.py
+++ /dev/null
@@ -1,51 +0,0 @@
-from typing import Literal
-
-from ..lmdb import ImageFolderLMDB
-from ..path import DatapathMixin
-
-from ...util.meta import store_attr
-
-__all__ = ["Imagenette", "Imagewoof"]
-
-
-class ImagenetteBase(ImageFolderLMDB, DatapathMixin):
-    def __init__(
-        self,
-        split: Literal["train", "val"],
-        resolution: Literal[320, 160, "full"] = 320,
-    ):
-        store_attr()
-        super().__init__(self.path)  # , transform=self.preproc_transform)
-
-
-class Imagenette(ImagenetteBase):
-    @property
-    def _folder_name(self):
-        res = {
-            "full": "",
-            320: "-320",
-            160: "-160",
-        }[self.resolution]
-        return f"Imagenette-lmdb/imagenette2{res}/{self.split}"
-
-
-class Imagewoof(ImagenetteBase):
-    @property
-    def _folder_name(self):
-        res = {
-            "full": "",
-            320: "-320",
-            160: "-160",
-        }[self.resolution]
-        return f"Imagenette-lmdb/imagewoof2{res}/{self.split}"
-
-
-# https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-320.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-160.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagewang.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagewang-320.tgz
-# https://s3.amazonaws.com/fast-ai-imageclas/imagewang-160.tgz
diff --git a/pylot/datasets/vision/main.py b/pylot/datasets/vision/main.py
deleted file mode 100644
index 87a1c6d..0000000
--- a/pylot/datasets/vision/main.py
+++ /dev/null
@@ -1,218 +0,0 @@
-import pathlib
-import os
-
-from torchvision import transforms, datasets
-import PIL
-
-from ..indexed import IndexedImageDataset
-
-__all__ = [
-    "CIFAR10",
-    "CIFAR100",
-    "ImageNet",
-    "Places365",
-    "Miniplaces",
-    "TinyImageNet",
-]
-
-_constructors = {
-    "MNIST": datasets.MNIST,
-    "CIFAR10": datasets.CIFAR10,
-    "CIFAR100": datasets.CIFAR100,
-    "ImageNet": IndexedImageDataset,
-    "Places365": IndexedImageDataset,
-    "Miniplaces": IndexedImageDataset,
-    "TinyImageNet": IndexedImageDataset,
-}
-
-
-def dataset_path(dataset, path=None):
-    """Get the path to a specified dataset
-
-    Arguments:
-        dataset {str} -- One of MNIST, CIFAR10, CIFAR100, ImageNet, Places365
-
-    Keyword Arguments:
-        path {str} -- Semicolon separated list of paths to look for dataset folders (default: {None})
-
-    Returns:
-        dataset_path -- pathlib.Path for the first match
-
-    Raises:
-        ValueError -- If no path is provided and DATAPATH is not set
-        LookupError -- If the given dataset cannot be found
-    """
-    if path is None:
-        # Look for the dataset in known paths
-        if "DATAPATH" in os.environ:
-            path = os.environ["DATAPATH"]
-            paths = [pathlib.Path(p) for p in path.split(":")]
-        else:
-            raise ValueError(
-                f"No path specified. A path must be provided, \n \
-                           or the folder must be listed in your DATAPATH"
-            )
-
-    paths = [pathlib.Path(p) for p in path.split(":")]
-
-    for p in paths:
-        p = (p / dataset).resolve()
-        if p.exists():
-            # print(f"Found {dataset} under {p}")
-            return p
-    raise LookupError(f"Could not find {dataset} in {paths}")
-
-
-def dataset_builder(dataset, train=True, normalize=None, preproc=None, path=None):
-    """Build a torch.utils.Dataset with proper preprocessing
-
-    Arguments:
-        dataset {str} -- One of MNIST, CIFAR10, CIFAR100, ImageNet, Places365
-
-    Keyword Arguments:
-        train {bool} -- Whether to return train or validation set (default: {True})
-        normalize {torchvision.Transform} -- Transform to normalize data channel wise (default: {None})
-        preproc {list(torchvision.Transform)} -- List of preprocessing operations (default: {None})
-        path {str} -- Semicolon separated list of paths to look for dataset folders (default: {None})
-
-    Returns:
-        torch.utils.data.Dataset -- Dataset object with transforms and normalization
-    """
-    if preproc is not None:
-        preproc += [transforms.ToTensor()]
-        if normalize is not None:
-            preproc += [normalize]
-        preproc = transforms.Compose(preproc)
-
-    kwargs = {"transform": preproc}
-
-    path = dataset_path(dataset, path)
-
-    return _constructors[dataset](path, train=train, **kwargs)
-
-
-def MNIST(train=True, path=None, norm=False, augmentation=False, augment_kw=None):
-    """Thin wrapper around torchvision.datasets.CIFAR10"""
-    augment_kwargs = dict(
-        degrees=10,
-        translate=(0.05, 0.05),
-        scale=(0.9, 1.0),
-        shear=(5, 5),
-        interpolation=PIL.Image.BILINEAR,
-    )
-    if augment_kw:
-        augment_kwargs.update(augment_kw)
-    mean, std = 0.1307, 0.3081
-    normalize = transforms.Normalize(mean=(mean,), std=(std,)) if norm else None
-    preproc = [] if not augmentation else [transforms.RandomAffine(**augment_kwargs)]
-    dataset = dataset_builder("MNIST", train, normalize, preproc, path)
-    dataset.shape = (1, 28, 28)
-    dataset.n_classes = 10
-    return dataset
-
-
-def CIFAR10(train=True, path=None):
-    """Thin wrapper around torchvision.datasets.CIFAR10"""
-    mean, std = [0.491, 0.482, 0.447], [0.247, 0.243, 0.262]
-    normalize = transforms.Normalize(mean=mean, std=std)
-    if train:
-        preproc = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4)]
-    else:
-        preproc = []
-    dataset = dataset_builder("CIFAR10", train, normalize, preproc, path)
-    dataset.shape = (3, 32, 32)
-    dataset.n_classes = 10
-    return dataset
-
-
-def CIFAR100(train=True, path=None):
-    """Thin wrapper around torchvision.datasets.CIFAR100"""
-    mean, std = [0.507, 0.487, 0.441], [0.267, 0.256, 0.276]
-    normalize = transforms.Normalize(mean=mean, std=std)
-    if train:
-        preproc = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4)]
-    else:
-        preproc = []
-    dataset = dataset_builder("CIFAR100", train, normalize, preproc, path)
-    dataset.shape = (3, 32, 32)
-    dataset.n_classes = 100
-    return dataset
-
-
-def ImageNet(train=True, path=None):
-    """Thin wrapper around IndexedImageDataset"""
-    # TODO Better data augmentation?
-    # ImageNet loading from files can produce benign EXIF errors
-    import warnings
-
-    warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)
-
-    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
-    normalize = transforms.Normalize(mean=mean, std=std)
-    if train:
-        preproc = [transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()]
-    else:
-        preproc = [transforms.Resize(256), transforms.CenterCrop(224)]
-    dataset = dataset_builder("ImageNet", train, normalize, preproc, path)
-    dataset.shape = (3, 224, 224)
-    dataset.n_classes = 1000
-    return dataset
-
-
-def Places365(train=True, path=None):
-    """Thin wrapper around IndexedImageDataset"""
-    # Note : Bolei used the normalization for Imagenet, not the one for Places!
-    # # https://github.com/CSAILVision/places365/blob/master/train_placesCNN.py
-    # So these are kept so weights are compatible
-    # TODO Better data augmentation
-    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
-
-    normalize = transforms.Normalize(mean=mean, std=std)
-    if train:
-        preproc = [transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()]
-    else:
-        preproc = [transforms.Resize(256), transforms.CenterCrop(224)]
-    dataset = dataset_builder("Places365", train, normalize, preproc, path)
-    dataset.shape = (3, 224, 224)
-    dataset.n_classes = 365
-    return dataset
-
-
-def TinyImageNet(train=True, path=None):
-    """Thin wrapper around IndexedImageDataset"""
-    # TODO Better data augmentation
-
-    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
-    normalize = transforms.Normalize(mean=mean, std=std)
-    if train:
-        preproc = [transforms.RandomCrop(64, 8), transforms.RandomHorizontalFlip()]
-    else:
-        preproc = []
-    dataset = dataset_builder("TinyImageNet", train, normalize, preproc, path)
-    dataset.shape = (3, 64, 64)
-    dataset.n_classes = 200
-    return dataset
-
-
-def Miniplaces(train=True, path=None):
-    """Thin wrapper around IndexedImageDataset"""
-    # TODO compute normalization constants for Miniplaces
-    # TODO Better data augmentation
-
-    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
-    normalize = transforms.Normalize(mean=mean, std=std)
-    if train:
-        preproc = [transforms.RandomCrop(128, 16), transforms.RandomHorizontalFlip()]
-    else:
-        preproc = []
-    dataset = dataset_builder("Miniplaces", train, normalize, preproc, path)
-    dataset.shape = (3, 128, 128)
-    dataset.n_classes = 100
-    return dataset
-
-
-def nanoImageNet(train=True, path=None):
-    from .subset import subset_dataset
-
-    d = ImageNet(train=train, path=path)
-    return subset_dataset(d, 10, 1000)
diff --git a/pylot/datasets/vision/nist.py b/pylot/datasets/vision/nist.py
deleted file mode 100644
index 5db8112..0000000
--- a/pylot/datasets/vision/nist.py
+++ /dev/null
@@ -1,177 +0,0 @@
-from typing import Literal
-
-import torchvision.transforms as TF
-import torchvision.datasets as TD
-from pydantic import validate_call
-import numpy as np
-
-from ..indexed import IndexedImageDataset
-from ..path import DatapathMixin
-from .emnist import EMNIST
-from .usps28 import USPS28
-from .notmnist import notMNIST, notMNISTLarge
-
-
-__all__ = [
-    "MNIST",
-    "EMNIST",
-    "KMNIST",
-    "USPS28",
-    "FashionMNIST",
-    "notMNIST",
-    "notMNISTLarge",
-]
-
-DATASET_CONSTRUCTOR = {
-    "MNIST": TD.MNIST,
-    "KMNIST": TD.KMNIST,
-    "EMNIST": EMNIST,
-    "FashionMNIST": TD.FashionMNIST,
-    "USPS": TD.USPS,
-    "USPS28": USPS28,
-    "notMNIST": notMNIST,
-    "notMNISTLarge": notMNISTLarge,
-    "CIFAR10": TD.CIFAR10,
-    "CIFAR100": TD.CIFAR100,
-    "ImageNet": IndexedImageDataset,
-    "Places365": IndexedImageDataset,
-    "Miniplaces": IndexedImageDataset,
-    "TinyImageNet": IndexedImageDataset,
-}
-
-DATASET_NCLASSES = {
-    "MNIST": 10,
-    "FashionMNIST": 10,
-    "EMNIST": 26,
-    "KMNIST": 10,
-    "USPS": 10,
-    "USPS28": 10,
-    "CIFAR10": 10,
-    "SVHN": 10,
-    "STL10": 10,
-    "LSUN": 10,
-    "TinyImageNet": 200,
-    "notMNIST": 10,
-    "notMNISTLarge": 10,
-}
-
-DATASET_SIZES = {
-    "MNIST": (28, 28),
-    "FashionMNIST": (28, 28),
-    "EMNIST": (28, 28),
-    "QMNIST": (28, 28),
-    "KMNIST": (28, 28),
-    "notMNIST": (28, 28),
-    "USPS": (16, 16),
-    "USPS28": (28, 28),
-    "SVHN": (32, 32),
-    "CIFAR10": (32, 32),
-    "STL10": (96, 96),
-    "TinyImageNet": (64, 64),
-}
-
-DATASET_NORMALIZATION = {
-    "MNIST": ((0.1307,), (0.3081,)),
-    "USPS": ((0.2459,), (0.2977,)),
-    "USPS28": ((0.2459,), (0.2977,)),
-    "FashionMNIST": ((0.2849,), (0.3516,)),
-    "QMNIST": ((0.1307,), (0.3081,)),
-    "EMNIST": ((0.1716,), (0.3297,)),
-    "KMNIST": ((0.1910,), (0.3470,)),
-    "notMNIST": ((0.419,), (0.455,)),
-    "notMNISTLarge": ((0.413,), (0.451,)),
-    "ImageNet": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
-    "TinyImageNet": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
-    "CIFAR10": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
-    "CIFAR100": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
-    "STL10": ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
-}
-
-
-def _fromVisionDataset(dataset_name):
-
-    class_ = DATASET_CONSTRUCTOR[dataset_name]
-
-    class DatasetWrapper(class_, DatapathMixin):
-        @validate_call
-        def __init__(
-            self,
-            split: Literal["train", "val", "test"],
-            val_split: float = 0.0,
-            seed: int = 42,
-            download: bool = False,
-            augmentation: bool = True,
-            transforms: bool = True,
-        ):
-            self.split = split
-            self.val_split = val_split
-            self.seed = seed
-            self.augmentation = augmentation
-            if split == "val" and val_split == 0.0:
-                raise ValueError("val_split cannot be 0.0 when split is 'val'")
-
-            aug_transforms = [TF.RandomCrop(28, padding=4)] if self.augmentation else []
-            transform = TF.Compose(aug_transforms + [
-                TF.ToTensor(),
-                TF.Normalize(*self.normalization)
-            ]) if transforms else None
-            root = self.path.parent if dataset_name not in ("USPS",) else self.path
-            super().__init__(
-                root, transform=transform, train=(split != "test"), download=download
-            )
-            self._init_split()
-
-        def _init_split(self):
-            from sklearn.model_selection import train_test_split
-
-            if self.split == "test":
-                self.indices = np.arange(len(self.targets))
-            elif self.val_split == 0:
-                if self.split == "train":
-                    self.indices = np.arange(len(self.targets))
-                else:
-                    self.indices = []
-            else:
-                train_idx, val_idx = train_test_split(
-                    np.arange(len(self.targets)),
-                    random_state=self.seed,
-                    test_size=self.val_split,
-                    stratify=self.targets,
-                )
-                if self.split == "train":
-                    self.indices = train_idx
-                else:
-                    self.indices = val_idx
-
-        def __len__(self):
-            return len(self.indices)
-
-        def __getitem__(self, idx):
-            return super().__getitem__(self.indices[idx])
-
-        @property
-        def _folder_name(self):
-            return dataset_name
-
-        @property
-        def normalization(self):
-            return DATASET_NORMALIZATION[dataset_name]
-
-        @property
-        def size(self):
-            return DATASET_SIZES[dataset_name]
-
-        @property
-        def n_classes(self):
-            return DATASET_NCLASSES[dataset_name]
-
-    return type(dataset_name, (DatasetWrapper,), {})
-
-
-MNIST = _fromVisionDataset("MNIST")
-EMNIST = _fromVisionDataset("EMNIST")
-KMNIST = _fromVisionDataset("KMNIST")
-USPS28 = _fromVisionDataset("USPS28")
-FashionMNIST = _fromVisionDataset("FashionMNIST")
-notMNIST = _fromVisionDataset("notMNIST")
-notMNISTLarge = _fromVisionDataset("notMNISTLarge")
diff --git a/pylot/datasets/vision/notmnist.py b/pylot/datasets/vision/notmnist.py
deleted file mode 100644
index 13151d0..0000000
--- a/pylot/datasets/vision/notmnist.py
+++ /dev/null
@@ -1,99 +0,0 @@
-from pathlib import Path
-from concurrent.futures import ProcessPoolExecutor
-
-import numpy as np
-from PIL import Image
-from pydantic import validate_call
-from torchvision.datasets import MNIST
-
-from tqdm.auto import tqdm
-
-from pylot.torch.torchlib import torch
-from pylot.datasets import IndexedImageFolder
-
-# Download and untar
-# http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz
-# http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz
-
-# NOTE:
-# There are invalid images that need to be deleted, and the Index might need to be recreated several times
-
-
-def _load_sample(path):
-    #     try:
-    return Image.open(path).convert("L")
-
-
-#     except PIL.UnidentifiedImageError:
-#         return path
-
-
-class notMNIST(MNIST):
-
-    classes = list("ABCDEFGHIJ")
-
-    # loading code is a tad hacky to make the dataset MNIST-inherited
-
-    @validate_call
-    def __init__(
-        self,
-        root: Path,
-        train: bool = True,
-        transform=None,
-        target_transform=None,
-        download: bool = False,
-        preload: bool = True,
-    ):
-        from sklearn.model_selection import train_test_split
-        if download:
-            raise NotImplementedError
-        self.train = train
-        self.transform = transform
-        self.target_transform = target_transform
-        self.preload = preload
-        self._dset = IndexedImageFolder(root / self._folder)
-        idxs = np.arange(len(self._dset.imgs))
-        train_idx, test_idx = train_test_split(
-            idxs, random_state=42, test_size=0.2, stratify=self._dset.targets
-        )
-        self.split_idx = train_idx if train else test_idx
-
-        # [...,0] is because images are loaded as RGB but are Greyscale
-        # self.data = [torch.from_numpy(np.array(dset[i][0])[..., 0]) for i in split_idx]
-        self.targets = torch.from_numpy(np.array(self._dset.targets))[self.split_idx]
-        self.root = self._dset.root
-
-        if self.preload:
-            self._load_data()
-
-    def _load_data(self):
-        paths = [self._dset.imgs[i][0] for i in self.split_idx]
-        with ProcessPoolExecutor(max_workers=16) as executor:
-            self.data = list(
-                tqdm(executor.map(_load_sample, paths), leave=False, total=len(paths))
-            )
-
-    def __getitem__(self, idx):
-        if self.preload:
-            x = self.data[idx]
-            y = self.targets[idx].item()
-        else:
-            idx = self.split_idx[idx]
-            x, y = self._dset[idx]
-            x = x.convert("L")
-        x = self.transform(x) if self.transform else x
-        y = self.target_transform(y) if self.target_transform else y
-        return x, y
-
-    def __len__(self):
-        return len(self.targets)
-
-    @property
-    def _folder(self):
-        return "notMNIST/notMNIST_small"
-
-
-class notMNISTLarge(notMNIST):
-    @property
-    def _folder(self):
-        return "notMNIST/notMNIST_large"
diff --git a/pylot/datasets/vision/oxfordpets.py b/pylot/datasets/vision/oxfordpets.py
deleted file mode 100644
index b8d2d6d..0000000
--- a/pylot/datasets/vision/oxfordpets.py
+++ /dev/null
@@ -1,202 +0,0 @@
-from functools import lru_cache
-from typing import Tuple
-
-import numpy as np
-import pandas as pd
-from PIL import Image
-from pylot.torch.torchlib import F, Dataset
-from torchvision.datasets.utils import download_and_extract_archive
-
-from ..path import DatapathMixin
-
-
-class OxfordPets(Dataset, DatapathMixin):
-    """
-    https://www.robots.ox.ac.uk/~vgg/data/pets/
-    """
-
-    # TODO add keypoint support for bounding boxes
-    MODES = ("cls2", "cls37", "seg1", "seg2", "seg37")
-    MEAN_CH = (0.478, 0.443, 0.394)
-    STD_CH = (0.268, 0.263, 0.27)
-
-    def __init__(
-        self,
-        split,
-        download=False,
-        root=None,
-        cache=False,
-        preproc=False,
-        mode="cls2",
-        size: Tuple[int, int] = (192, 256),
-        augmentation=False,
-        onehot: bool = False,
-    ):
-        train = (split == 'train')
-        assert mode in self.MODES, f"Mode {mode} not one of {','.join(self.MODES)}"
-        assert not download or (
-            root is not None
-        ), "For download, root must be specified"
-
-        self.mode = mode
-        self.train = train
-        self.breeds = "37" in self.mode
-        self.root = root
-        self.cache = cache
-        self.preproc = preproc
-        self.size = size
-        self.augmentation = augmentation
-        self.onehot = onehot
-        self.n_classes = len(self.classes) + 1  # Extra channel for background
-
-        if download:
-            self._download()
-
-        self._load_annotations()
-
-        if self.preproc:
-            self._load_transforms()
-
-        if self.cache:
-            if self.augmentation:
-                self._get_image = lru_cache(maxsize=None)(self._get_image)
-                self._get_mask = lru_cache(maxsize=None)(self._get_mask)
-            else:
-                self.__getitem__ = lru_cache(maxsize=None)(self.__getitem__)
-
-    def _load_annotations(self):
-        self.images_path = self.path / "images"
-        self.masks_path = self.path / "annotations/trimaps"
-
-        label_file = "trainval.txt" if self.train else "test.txt"
-        label_df = pd.read_csv(
-            self.path / "annotations" / label_file,
-            sep=" ",
-            names=["file", "id", "species", "breed"],
-        )
-
-        self._files = label_df.file.values
-        labels = label_df.id if self.breeds else label_df.species
-        # Labels are one indexed
-        self._labels = labels.values - 1
-
-    def _load_transforms(self):
-        import albumentations as A
-        from albumentations.pytorch import ToTensorV2
-        transforms = []
-        if self.size:
-            height, width = self.size
-
-            if self.train and self.augmentation:
-                transforms = [
-                    A.RandomResizedCrop(height, width),
-                    A.HorizontalFlip(p=0.5),
-                ]
-            else:
-                transforms = [A.Resize(height, width)]
-
-        # For either train/val we need to Normalize channels and convert to tensor
-        transforms += [
-            A.Normalize(mean=self.MEAN_CH, std=self.STD_CH),
-            ToTensorV2(),
-        ]
-        self.transform = A.Compose(transforms)
-
-    def __len__(self):
-        return len(self._files)
-
-    def __getitem__(self, index):
-        image = self._get_image(index)
-        segment = self.mode.startswith("seg")
-        if not segment:
-            label = self._labels[index]
-            if self.preproc:
-                image = self.transform(image=image)["image"]
-        else:
-            label = self._get_mask(index)
-            if self.preproc:
-                augment = self.transform(image=image, mask=label)
-                image, label = augment["image"], augment["mask"].long()
-                if self.mode == "seg1":
-                    # Dummy dimension for BCE(WithLogits)Loss or volumetrics losses
-                    label = label[None, ...].float()
-                if self.onehot:
-                    label = (
-                        F.one_hot(label, num_classes=self.n_classes)
-                        .permute((2, 0, 1))
-                        .float()
-                    )
-
-        return image, label
-
-    def _get_image(self, i):
-        # Convert is required in case RGBA
-        image = Image.open(self.images_path / (self._files[i] + ".jpg")).convert("RGB")
-        if self.preproc:
-            image = np.array(image)
-        return image
-
-    def _get_mask(self, i):
-        trimap = Image.open(self.masks_path / (self._files[i] + ".png"))
-        mask = np.array(trimap)
-        # Pixel Annotations: 1: Foreground 2:Background 3: Not classified
-        mask[mask == 2] = 0.0
-        mask[(mask == 1) | (mask == 3)] = 1.0
-
-        if self.mode == "seg1":
-            return mask
-
-        mask *= 1 + self._labels[i]
-        return mask
-
-    def _download(self):
-        base_url = "https://www.robots.ox.ac.uk/~vgg/data/pets/data/"
-        download_and_extract_archive(base_url + "images.tar.gz", self.root)
-        download_and_extract_archive(base_url + "annotations.tar.gz", self.root)
-
-    @property
-    def classes(self):
-        if self.mode == "seg1":
-            return ["mask"]
-        if not self.breeds:
-            return ["cat", "dog"]
-        else:
-            return [
-                "Abyssinian",
-                "american_bulldog",
-                "american_pit_bull_terrier",
-                "basset_hound",
-                "beagle",
-                "Bengal",
-                "Birman",
-                "Bombay",
-                "boxer",
-                "British_Shorthair",
-                "chihuahua",
-                "Egyptian_Mau",
-                "english_cocker_spaniel",
-                "english_setter",
-                "german_shorthaired",
-                "great_pyrenees",
-                "havanese",
-                "japanese_chin",
-                "keeshond",
-                "leonberger",
-                "Maine_Coon",
-                "miniature_pinscher",
-                "newfoundland",
-                "Persian",
-                "pomeranian",
-                "pug",
-                "Ragdoll",
-                "Russian_Blue",
-                "saint_bernard",
-                "samoyed",
-                "scottish_terrier",
-                "shiba_inu",
-                "Siamese",
-                "Sphynx",
-                "staffordshire_bull_terrier",
-                "wheaten_terrier",
-                "yorkshire_terrier",
-            ]
diff --git a/pylot/datasets/vision/pascal.py b/pylot/datasets/vision/pascal.py
deleted file mode 100644
index c7fcba8..0000000
--- a/pylot/datasets/vision/pascal.py
+++ /dev/null
@@ -1,151 +0,0 @@
-from functools import lru_cache
-from typing import Tuple
-
-import numpy as np
-from torchvision.datasets import VOCSegmentation
-from pylot.torch.torchlib import F, Dataset
-
-from ..path import DatapathMixin
-
-
-class PascalVOC2012(Dataset, DatapathMixin):
-    """
-    http://host.robots.ox.ac.uk/pascal/VOC/voc2012/
-    """
-
-    MODES = ("seg20",)  # , "detect")
-    MEAN_CH = (0.457, 0.443, 0.407)
-    STD_CH = (0.273, 0.269, 0.285)
-
-    def __init__(
-        self,
-        train: bool = True,
-        preproc: bool = False,
-        augmentation: bool = False,
-        cache: bool = False,
-        size: Tuple[int, int] = (192, 256),
-        mode: str = "seg20",
-        onehot: bool = False,
-    ):
-        assert mode in self.MODES, f"Mode {mode} not one of {','.join(self.MODES)}"
-
-        self.cache = cache
-        self.train = train
-        self.preproc = preproc
-        self.augmentation = augmentation
-        self.size = size
-        self.mode = mode
-        self.onehot = onehot
-        self.n_classes = len(self.classes)
-
-        image_set = "train" if self.train else "val"
-        self.D = VOCSegmentation(root=self.path, year="2012", image_set=image_set)
-
-        if self.preproc:
-            self._load_transforms()
-
-        if self.cache:
-            if self.augmentation:
-                self.D.__getitem__ = lru_cache(maxsize=None)(self.D.__getitem__)
-            else:
-                self.__getitem__ = lru_cache(maxsize=None)(self.__getitem__)
-
-    def _load_transforms(self):
-        import albumentations as A
-        from albumentations.pytorch import ToTensorV2
-        transforms = []
-        if self.size:
-            height, width = self.size
-
-            if self.train and self.augmentation:
-                transforms = [
-                    A.RandomResizedCrop(height, width),
-                    A.HorizontalFlip(p=0.5),
-                ]
-            else:
-                transforms = [A.Resize(height, width)]
-
-        # For either train/val we need to Normalize channels and convert to tensor
-        transforms += [
-            A.Normalize(mean=self.MEAN_CH, std=self.STD_CH),
-            ToTensorV2(),
-        ]
-        self.transform = A.Compose(transforms)
-
-    def __len__(self):
-        return len(self.D)
-
-    def __getitem__(self, index):
-        image, label = self.D[index]
-        if self.preproc:
-            image = np.array(image)
-            label = np.array(label)
-            label[label == 255] = 0
-            augment = self.transform(image=image, mask=label)
-            image, label = augment["image"], augment["mask"].long()
-            if self.onehot:
-                label = (
-                    F.one_hot(label, num_classes=self.n_classes)
-                    .permute((2, 0, 1))
-                    .float()
-                )
-        return image, label
-
-    @property
-    def classes(self):
-        return [
-            "__background__",
-            "aeroplane",
-            "bicycle",
-            "bird",
-            "boat",
-            "bottle",
-            "bus",
-            "car",
-            "cat",
-            "chair",
-            "cow",
-            "diningtable",
-            "dog",
-            "horse",
-            "motorbike",
-            "person",
-            "pottedplant",
-            "sheep",
-            "sofa",
-            "train",
-            "tvmonitor",
-        ]
-
-    @property
-    def classes_dict(self):
-        d = {i: l for i, l in enumerate(self.labels)}
-        d[255] = "__void__"
-        return d
-
-    @property
-    def palette(self):
-        VOC_COLORMAP = [
-            [0, 0, 0],
-            [128, 0, 0],
-            [0, 128, 0],
-            [128, 128, 0],
-            [0, 0, 128],
-            [128, 0, 128],
-            [0, 128, 128],
-            [128, 128, 128],
-            [64, 0, 0],
-            [192, 0, 0],
-            [64, 128, 0],
-            [192, 128, 0],
-            [64, 0, 128],
-            [192, 0, 128],
-            [64, 128, 128],
-            [192, 128, 128],
-            [0, 64, 0],
-            [128, 64, 0],
-            [0, 192, 0],
-            [128, 192, 0],
-            [0, 64, 128],
-        ]
-        return np.array(VOC_COLORMAP)
diff --git a/pylot/datasets/vision/usps28.py b/pylot/datasets/vision/usps28.py
deleted file mode 100644
index 2d91ebc..0000000
--- a/pylot/datasets/vision/usps28.py
+++ /dev/null
@@ -1,41 +0,0 @@
-import os
-
-import numpy as np
-from torchvision.datasets import MNIST, USPS
-from tqdm.auto import tqdm
-from pylot.torch.torchlib import torch
-
-class USPS28(MNIST):
-    def _load_data(self):
-        split = "train" if self.train else "test"
-
-        file = os.path.join(self.root, f"USPS28/usps28-{split}.npz")
-        data = np.load(file)
-        imgs = torch.from_numpy(data["imgs"])
-        targets = torch.from_numpy(data["targets"])
-        return imgs, targets
-
-    def download(self):
-        import PIL
-
-        dst_folder = os.path.join(self.root, self.__class__.__name__)
-        os.makedirs(dst_folder, exist_ok=True)
-
-        for split in ["train", "test"]:
-            resized_images = []
-            d = USPS(self.root / "USPS", train=(split == "train"))
-            for i in tqdm(d):
-                r = i[0].resize((28, 28), PIL.Image.NEAREST)
-                resized_images.append(np.array(r))
-            resized_images = np.array(resized_images)
-            file = os.path.join(dst_folder, f"usps28-{split}.npz")
-            targets = np.array(d.targets)
-            np.savez_compressed(file, imgs=resized_images, targets=targets)
-
-    def _check_exists(self):
-        import pathlib
-
-        return all(
-            (pathlib.Path(self.root) / "USPS28" / f"usps28-{split}.npz").exists()
-            for split in ["train", "test"]
-        )
diff --git a/pylot/datasets/vision/utils.py b/pylot/datasets/vision/utils.py
deleted file mode 100644
index 6f90d6c..0000000
--- a/pylot/datasets/vision/utils.py
+++ /dev/null
@@ -1,40 +0,0 @@
-from ...util.meter import StatsMeter
-import numpy as np
-from tqdm.auto import tqdm
-
-
-def channel_stats(dataset, equal_size=True, precision=None):
-
-    if equal_size:
-        meter = StatsMeter()
-        # For each image we add a datum (H,W,C) and reduce at the end
-        for x, _ in tqdm(dataset):
-            meter.add(np.array(x))
-        reduction = StatsMeter()
-        H, W, C = np.array(x).shape
-        N = len(dataset)
-        for i in range(H):
-            for j in range(W):
-                reduction += StatsMeter.from_values(
-                    n=N, mean=meter.mean[i, j], std=meter.std[i, j]
-                )
-        mean, std = reduction.mean, reduction.std
-
-    else:
-        # For each image we add data for all the pixels (H*W)
-        meter = StatsMeter()
-        for x, _ in tqdm(dataset):
-            x = np.array(x)
-            H, W, C = x.shape
-            x = x.reshape(-1, C)
-            mu = x.mean(axis=0)
-            std = x.std(axis=0)
-            meter += StatsMeter.from_values(n=H * W, mean=mu, std=std)
-        mean, std = meter.mean, meter.std
-
-    mean = tuple((mean / 255).tolist())
-    std = tuple((std / 255).tolist())
-    if precision:
-        mean = tuple(round(x, precision) for x in mean)
-        std = tuple(round(x, precision) for x in std)
-    return mean, std

commit 4b301e5cc17ff1c0346f574fd7039d37b6dc8e74
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue May 6 13:53:39 2025 -0400

    [BREAKING] Remove augmentations.

diff --git a/pylot/__init__.py b/pylot/__init__.py
index db96b08..5448757 100644
--- a/pylot/__init__.py
+++ b/pylot/__init__.py
@@ -46,7 +46,6 @@ mind.
 """
 
 from . import analysis
-from . import augmentation
 from . import callbacks
 from . import datasets
 from . import experiment
@@ -61,7 +60,6 @@ from . import torch
 from . import util
 
 from .analysis import *
-from .augmentation import *
 from .callbacks import *
 from .datasets import *
 from .experiment import *
diff --git a/pylot/augmentation/__init__.py b/pylot/augmentation/__init__.py
deleted file mode 100644
index 6ac4fcf..0000000
--- a/pylot/augmentation/__init__.py
+++ /dev/null
@@ -1,3 +0,0 @@
-from kornia.augmentation import *
-from .geometry import *
-from .variable import *
diff --git a/pylot/augmentation/common.py b/pylot/augmentation/common.py
deleted file mode 100644
index 1eda6d4..0000000
--- a/pylot/augmentation/common.py
+++ /dev/null
@@ -1,39 +0,0 @@
-from typing import Union, Tuple
-
-import numpy as np
-import kornia.augmentation as KA
-
-
-# TODO: make typing work for float too
-def _as2tuple(value: Union[int, Tuple[int, int]]) -> Tuple[int, int]:
-    # because kornia.morphology works only with two-tuples
-    if isinstance(value, (int, float)):
-        return (value, value)
-    if isinstance(value, list):
-        value = tuple(value)
-    assert isinstance(value, tuple) and len(value) == 2, f"Invalid 2-tuple {value}"
-    return value
-
-
-def _as_single_val(value):
-    if isinstance(value, (int, float)):
-        return value
-    assert (
-        isinstance(value, (tuple, list)) and len(value) == 2
-    ), f"Invalid 2-tuple {value}"
-    if any(isinstance(i, float) for i in value):
-        value = (float(value[0]), float(value[1]))
-    if isinstance(value[0], int):
-        return np.random.randint(*value)
-    else:
-        return np.random.uniform(*value)
-
-
-class AugmentationBase2D(KA.AugmentationBase2D):
-
-    """ Dummy class because Kornia really wants me to overload
-    the .compute_transformation method
-    """
-
-    def compute_transformation(self, input, params):
-        return self.identity_matrix(input)
diff --git a/pylot/augmentation/geometry.py b/pylot/augmentation/geometry.py
deleted file mode 100644
index 5126a33..0000000
--- a/pylot/augmentation/geometry.py
+++ /dev/null
@@ -1,15 +0,0 @@
-import kornia.augmentation as KA
-
-
-def RandomScale(scale, **kwargs):
-    return KA.RandomAffine(degrees=0.0, translate=0.0, scale=scale, shear=0.0, **kwargs)
-
-
-def RandomTranslate(translate, **kwargs):
-    return KA.RandomAffine(
-        degrees=0.0, translate=translate, scale=0.0, shear=0.0, **kwargs
-    )
-
-
-def RandomShear(shear, **kwargs):
-    return KA.RandomAffine(degrees=0.0, translate=0.0, scale=0.0, shear=shear, **kwargs)
diff --git a/pylot/augmentation/variable.py b/pylot/augmentation/variable.py
deleted file mode 100644
index 808cec4..0000000
--- a/pylot/augmentation/variable.py
+++ /dev/null
@@ -1,241 +0,0 @@
-from typing import Optional, Union, Tuple, Dict
-
-
-import numpy as np
-import kornia as K
-from kornia.constants import BorderType
-
-from pydantic import validate_call
-
-from .common import AugmentationBase2D, _as_single_val, _as2tuple
-from pylot.torch.torchlib import torch
-
-class RandomBrightnessContrast(AugmentationBase2D):
-    def __init__(
-        self,
-        brightness: Union[float, Tuple[float, float]] = 0.0,
-        contrast: Union[float, Tuple[float, float]] = 1.0,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(
-            p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim,
-        )
-        self.brightness = brightness
-        self.contrast = contrast
-
-    def generate_parameters(self, input_shape: torch.Size):
-
-        brightness = _as_single_val(self.brightness)
-        contrast = _as_single_val(self.contrast)
-
-        order = np.random.permutation(2)
-
-        return dict(brightness=brightness, contrast=contrast, order=order)
-
-    def apply_transform(
-        self,
-        input: torch.Tensor,
-        params: Dict[str, torch.Tensor],
-        transform: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-        transforms = [
-            lambda img: K.enhance.adjust_brightness(img, params["brightness"]),
-            lambda img: K.enhance.adjust_contrast(img, params["contrast"]),
-        ]
-
-        jittered = input
-        for idx in params["order"].tolist():
-            t = transforms[idx]
-            jittered = t(jittered)
-
-        return jittered
-
-
-class FilterBase(AugmentationBase2D):
-    @validate_call
-    def __init__(
-        self,
-        kernel_size: Union[int, Tuple[int, int]],
-        sigma: Union[float, Tuple[float, float]],
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(
-            p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim,
-        )
-        self.kernel_size = kernel_size
-        self.sigma = sigma
-
-
-class VariableFilterBase(FilterBase):
-
-    """Helper class for tasks that involve a random filter"""
-
-    def generate_parameters(self, input_shape: torch.Size):
-        kernel_size = _as_single_val(self.kernel_size)
-        sigma = _as_single_val(self.sigma)
-        return dict(kernel_size=kernel_size, sigma=sigma)
-
-
-class RandomVariableGaussianBlur(VariableFilterBase):
-    def __init__(
-        self,
-        kernel_size: Union[int, Tuple[int, int]],
-        sigma: Union[float, Tuple[float, float]],
-        border_type: str = "reflect",
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(
-            kernel_size=kernel_size,
-            sigma=sigma,
-            p=p,
-            same_on_batch=same_on_batch,
-            keepdim=keepdim,
-        )
-        self.flags = dict(border_type=BorderType.get(border_type))
-
-    def apply_transform(
-        self,
-        input: torch.Tensor,
-        params: Dict[str, torch.Tensor],
-        transform: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-
-        kernel_size = _as2tuple(self.kernel_size)
-        sigma = _as2tuple(self.sigma)
-
-        return K.filters.gaussian_blur2d(
-            input, kernel_size, sigma, self.flags["border_type"].name.lower()
-        )
-
-
-class RandomVariableBoxBlur(AugmentationBase2D):
-    def __init__(
-        self,
-        kernel_size: Union[int, Tuple[int, int]] = 3,
-        border_type: str = "reflect",
-        normalized: bool = True,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(
-            p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim,
-        )
-        self.flags = dict(border_type=border_type, normalized=normalized)
-
-    def generate_parameters(self, input_shape: torch.Size):
-        kernel_size = _as_single_val(self.kernel_size)
-        return Dict(kernel_size=kernel_size)
-
-    def apply_transform(
-        self,
-        input: torch.Tensor,
-        params: Dict[str, torch.Tensor],
-        transform: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-        kernel_size = _as2tuple(params["kernel_size"])
-        return K.filters.box_blur(
-            input, kernel_size, self.flags["border_type"], self.flags["normalized"]
-        )
-
-
-class RandomVariableGaussianNoise(AugmentationBase2D):
-    def __init__(
-        self,
-        mean: Union[float, Tuple[float, float]] = 0.0,
-        std: Union[float, Tuple[float, float]] = 1.0,
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(
-            p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim,
-        )
-        self.mean = mean
-        self.std = std
-
-    def generate_parameters(self, input_shape: torch.Size):
-
-        mean = _as_single_val(self.mean)
-        std = _as_single_val(self.std)
- 
-        if torch.cuda.is_available():
-            noise = torch.cuda.FloatTensor(input_shape).normal_(mean, std)
-        else:
-            noise = torch.FloatTensor(input_shape).normal_(mean, std)
-
-        return dict(noise=noise)
-
-    def apply_transform(
-        self,
-        input: torch.Tensor,
-        params: Dict[str, torch.Tensor],
-        transform: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-        return input + params["noise"]
-
-
-class RandomVariableElasticTransform(AugmentationBase2D):
-    def __init__(
-        self,
-        kernel_size: Union[int, Tuple[int, int]] = 63,
-        sigma: Union[float, Tuple[float, float]] = 32,
-        alpha: Union[float, Tuple[float, float]] = 1.0,
-        align_corners: bool = False,
-        mode: str = "bilinear",
-        padding_mode: str = "zeros",
-        same_on_batch: bool = False,
-        p: float = 0.5,
-        keepdim: bool = False,
-    ) -> None:
-        super().__init__(
-            p=p, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim,
-        )
-        self.flags = dict(
-            kernel_size=kernel_size,
-            sigma=sigma,
-            alpha=alpha,
-            align_corners=align_corners,
-            mode=mode,
-            padding_mode=padding_mode,
-        )
-
-    def generate_parameters(self, shape: torch.Size) -> Dict[str, torch.Tensor]:
-        B, _, H, W = shape
-        if self.same_on_batch:
-            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(
-                B, 1, 1, 1
-            )
-        else:
-            noise = torch.rand(B, 2, H, W, device=self.device, dtype=self.dtype)
-
-        kernel_size = _as_single_val(self.flags["kernel_size"])
-        sigma = _as_single_val(self.flags["sigma"])
-        alpha = _as_single_val(self.flags["alpha"])
-
-        return dict(
-            noise=noise * 2 - 1, kernel_size=kernel_size, sigma=sigma, alpha=alpha
-        )
-
-    def apply_transform(
-        self,
-        input: torch.Tensor,
-        params: Dict[str, torch.Tensor],
-        transform: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
-        return K.geometry.transform.elastic_transform2d(
-            input,
-            params["noise"].to(input),
-            _as2tuple(params["kernel_size"]),
-            _as2tuple(params["sigma"]),
-            _as2tuple(params["alpha"]),
-            self.flags["align_corners"],
-            self.flags["mode"],
-            self.flags["padding_mode"],
-        )

commit 44fbf00ce88487617f92b4546f6d5697054c02d2
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue May 6 13:48:43 2025 -0400

    [REFACTOR] Prefer torch barrel import for faster initialization

diff --git a/pylot/augmentation/variable.py b/pylot/augmentation/variable.py
index e4f0ac5..808cec4 100644
--- a/pylot/augmentation/variable.py
+++ b/pylot/augmentation/variable.py
@@ -1,6 +1,6 @@
 from typing import Optional, Union, Tuple, Dict
 
-import torch
+
 import numpy as np
 import kornia as K
 from kornia.constants import BorderType
@@ -8,7 +8,7 @@ from kornia.constants import BorderType
 from pydantic import validate_call
 
 from .common import AugmentationBase2D, _as_single_val, _as2tuple
-
+from pylot.torch.torchlib import torch
 
 class RandomBrightnessContrast(AugmentationBase2D):
     def __init__(
diff --git a/pylot/callbacks/__init__.py b/pylot/callbacks/__init__.py
index 3934240..ea2e01f 100644
--- a/pylot/callbacks/__init__.py
+++ b/pylot/callbacks/__init__.py
@@ -13,7 +13,6 @@ from .epoch import (
 
 from .setup import (
     Summary,
-    Topology,
     ParameterTable,
     ModuleTable,
     CheckHalfCosineSchedule,
diff --git a/pylot/callbacks/epoch.py b/pylot/callbacks/epoch.py
index 09cdfb2..7d3321a 100644
--- a/pylot/callbacks/epoch.py
+++ b/pylot/callbacks/epoch.py
@@ -10,7 +10,7 @@ from pydantic import validate_call
 from tabulate import tabulate
 from loguru import logger
 
-import torch
+from pylot.torch.torchlib import torch
 import pandas as pd
 
 def PrintLogged(experiment):
diff --git a/pylot/callbacks/img.py b/pylot/callbacks/img.py
index 7be8852..60922c3 100644
--- a/pylot/callbacks/img.py
+++ b/pylot/callbacks/img.py
@@ -1,8 +1,7 @@
 from dataclasses import dataclass
 from typing import Optional, List, Any
 
-import torch
-from torch import Tensor
+from pylot.torch.torchlib import torch, Tensor
 
 from ..nn.hooks import HookedModule
 
diff --git a/pylot/callbacks/misc.py b/pylot/callbacks/misc.py
index 4cf17c7..812196a 100644
--- a/pylot/callbacks/misc.py
+++ b/pylot/callbacks/misc.py
@@ -1,4 +1,4 @@
-import torch
+from pylot.torch.torchlib import torch
 
 
 class LockMemory:
diff --git a/pylot/callbacks/setup.py b/pylot/callbacks/setup.py
index bbb39cb..0d3ed72 100644
--- a/pylot/callbacks/setup.py
+++ b/pylot/callbacks/setup.py
@@ -3,7 +3,6 @@ import json
 from tabulate import tabulate
 from loguru import logger
 
-from ..util import S3Path
 from ..util.summary import summary
 
 from ..metrics import module_table, parameter_table
@@ -69,32 +68,6 @@ def Summary(experiment, filename="summary.txt"):
             json.dump(s, f)
 
 
-def Topology(experiment, filename="topology", extension="svg"):
-    assert extension in ("pdf", "svg")
-    from torchviz import make_dot
-
-    x, y = next(iter(experiment.train_dl))
-    x = x.to(experiment.device)
-    y = y.to(experiment.device)
-
-    # Save model topology
-    topology_path = experiment.path / filename
-    topology_pdf_path = topology_path.with_suffix("." + extension)
-
-    if not topology_pdf_path.exists():
-
-        yhat = experiment.model(x)
-        loss = experiment.loss_func(yhat, y)
-        g = make_dot(loss)
-        if extension == "svg":
-            g.format = "svg"
-        with S3Path.as_local(topology_path) as lf:
-            g.render(lf)
-        # Interested in pdf, the graphviz file can be removed
-        if topology_path.exists():
-            topology_path.unlink()
-
-
 def CheckHalfCosineSchedule(experiment):
 
     scheduler = experiment.get_param("train.scheduler.scheduler", None)
diff --git a/pylot/callbacks/wrapup.py b/pylot/callbacks/wrapup.py
index 7edea67..b145e51 100644
--- a/pylot/callbacks/wrapup.py
+++ b/pylot/callbacks/wrapup.py
@@ -1,7 +1,10 @@
 import os
-import s3fs
 import getpass
 
+from pylot.util.imports import optional_import
+
+s3fs = optional_import('s3fs')
+
 class S3Copy:
     def __init__(self, experiment):
         self.experiment = experiment
diff --git a/pylot/datasets/cuda.py b/pylot/datasets/cuda.py
index 1e291d8..39fde40 100644
--- a/pylot/datasets/cuda.py
+++ b/pylot/datasets/cuda.py
@@ -1,6 +1,4 @@
-import torch
-from torch.utils.data import Dataset
-
+from pylot.torch.torchlib import torch, Dataset
 from ..util import to_device
 
 
diff --git a/pylot/datasets/image_folder_tar.py b/pylot/datasets/image_folder_tar.py
index 8c016d4..05212e2 100644
--- a/pylot/datasets/image_folder_tar.py
+++ b/pylot/datasets/image_folder_tar.py
@@ -1,13 +1,12 @@
 # from https://gist.github.com/rwightman/5a7c9232eb57da20afa80cedb8ac2d38
-import torch.utils.data as data
 import os
 import re
-import torch
+
 import tarfile
 from PIL import Image
 
 from ..util import autoload
-
+from pylot.torch.torchlib import torch, Dataset
 
 IMG_EXTENSIONS = [".png", ".jpg", ".jpeg"]
 
diff --git a/pylot/datasets/split.py b/pylot/datasets/split.py
index bd28ab4..c03b130 100644
--- a/pylot/datasets/split.py
+++ b/pylot/datasets/split.py
@@ -1,13 +1,14 @@
 from typing import List, Tuple
 
 from pydantic import validate_call
-from sklearn.model_selection import train_test_split
+
 
 
 @validate_call
 def data_splits(
     values: List[str], splits: Tuple[float, float, float], seed: int
 ) -> Tuple[List[str], List[str], List[str]]:
+    from sklearn.model_selection import train_test_split
 
     if len(set(values)) != len(values):
         raise ValueError(f"Duplicate entries found in values")
diff --git a/pylot/datasets/util.py b/pylot/datasets/util.py
index 5ed9ca2..f97bfd5 100644
--- a/pylot/datasets/util.py
+++ b/pylot/datasets/util.py
@@ -1,7 +1,6 @@
 import math
 import numpy as np
-from sklearn.model_selection import train_test_split
-import torch
+from pylot.torch.torchlib import torch
 from torch.utils.data import random_split, Subset
 
 
@@ -16,6 +15,8 @@ def train_val_split(dataset, val_split, seed=None):
 
 
 def stratified_train_val_split(dataset, val_split, seed=None):
+    from sklearn.model_selection import train_test_split
+
     indices = np.arange(len(dataset))
     stratify = getattr(dataset, 'targets', None)
     train_indices, val_indices = train_test_split(
diff --git a/pylot/datasets/vision/nist.py b/pylot/datasets/vision/nist.py
index 77b02ae..5db8112 100644
--- a/pylot/datasets/vision/nist.py
+++ b/pylot/datasets/vision/nist.py
@@ -3,7 +3,6 @@ from typing import Literal
 import torchvision.transforms as TF
 import torchvision.datasets as TD
 from pydantic import validate_call
-from sklearn.model_selection import train_test_split
 import numpy as np
 
 from ..indexed import IndexedImageDataset
@@ -123,6 +122,8 @@ def _fromVisionDataset(dataset_name):
             self._init_split()
 
         def _init_split(self):
+            from sklearn.model_selection import train_test_split
+
             if self.split == "test":
                 self.indices = np.arange(len(self.targets))
             elif self.val_split == 0:
diff --git a/pylot/datasets/vision/notmnist.py b/pylot/datasets/vision/notmnist.py
index 9642b82..13151d0 100644
--- a/pylot/datasets/vision/notmnist.py
+++ b/pylot/datasets/vision/notmnist.py
@@ -1,15 +1,14 @@
 from pathlib import Path
 from concurrent.futures import ProcessPoolExecutor
 
-import torch
 import numpy as np
 from PIL import Image
 from pydantic import validate_call
-from sklearn.model_selection import train_test_split
 from torchvision.datasets import MNIST
 
 from tqdm.auto import tqdm
 
+from pylot.torch.torchlib import torch
 from pylot.datasets import IndexedImageFolder
 
 # Download and untar
@@ -45,6 +44,7 @@ class notMNIST(MNIST):
         download: bool = False,
         preload: bool = True,
     ):
+        from sklearn.model_selection import train_test_split
         if download:
             raise NotImplementedError
         self.train = train
diff --git a/pylot/datasets/vision/oxfordpets.py b/pylot/datasets/vision/oxfordpets.py
index eb6e195..b8d2d6d 100644
--- a/pylot/datasets/vision/oxfordpets.py
+++ b/pylot/datasets/vision/oxfordpets.py
@@ -4,8 +4,7 @@ from typing import Tuple
 import numpy as np
 import pandas as pd
 from PIL import Image
-import torch.nn.functional as F
-from torch.utils.data import Dataset
+from pylot.torch.torchlib import F, Dataset
 from torchvision.datasets.utils import download_and_extract_archive
 
 from ..path import DatapathMixin
diff --git a/pylot/datasets/vision/pascal.py b/pylot/datasets/vision/pascal.py
index e32de99..c7fcba8 100644
--- a/pylot/datasets/vision/pascal.py
+++ b/pylot/datasets/vision/pascal.py
@@ -3,8 +3,7 @@ from typing import Tuple
 
 import numpy as np
 from torchvision.datasets import VOCSegmentation
-from torch.utils.data import Dataset
-import torch.nn.functional as F
+from pylot.torch.torchlib import F, Dataset
 
 from ..path import DatapathMixin
 
diff --git a/pylot/datasets/vision/usps28.py b/pylot/datasets/vision/usps28.py
index cf8d675..2d91ebc 100644
--- a/pylot/datasets/vision/usps28.py
+++ b/pylot/datasets/vision/usps28.py
@@ -1,10 +1,9 @@
 import os
 
-import torch
 import numpy as np
 from torchvision.datasets import MNIST, USPS
 from tqdm.auto import tqdm
-
+from pylot.torch.torchlib import torch
 
 class USPS28(MNIST):
     def _load_data(self):
diff --git a/pylot/experiment/finetune.py b/pylot/experiment/finetune.py
index 7926192..d96adb2 100644
--- a/pylot/experiment/finetune.py
+++ b/pylot/experiment/finetune.py
@@ -1,7 +1,7 @@
 import pathlib
 from typing import Optional
 
-import torch
+from pylot.torch.torchlib import torch
 from .train import TrainExperiment
 from ..util import Config, autoload
 
diff --git a/pylot/experiment/half.py b/pylot/experiment/half.py
index c0ae52f..daaa93b 100644
--- a/pylot/experiment/half.py
+++ b/pylot/experiment/half.py
@@ -1,5 +1,5 @@
 from pylot.util import to_device
-import torch
+from pylot.torch.torchlib import torch
 from pylot.experiment import TrainExperiment
 
 
diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index 149c798..40d1ec3 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -13,12 +13,10 @@ import pathlib
 from typing import List
 
 from tqdm import tqdm
-import torch
 import wandb
-from torch import nn
-from torch.utils.data import DataLoader
 from loguru import logger
 
+from pylot.torch.torchlib import torch, DataLoader, nn
 from ..nn.util import num_params, split_param_groups_by_weight_decay
 from ..util.ioutil import autosave
 from ..util.meter import MeterDict
diff --git a/pylot/experiment/util.py b/pylot/experiment/util.py
index 110133a..c779b30 100644
--- a/pylot/experiment/util.py
+++ b/pylot/experiment/util.py
@@ -4,13 +4,12 @@ import importlib
 import pathlib
 import random
 from typing import Tuple, Dict, List
+import numpy as np
 
 from ..util.config import HDict, Config
 from ..util.ioutil import autoload
 from ..util.more_functools import partial
-
-import torch
-import numpy as np
+from pylot.torch.torchlib import torch
 
 
 def fix_seed(seed):
diff --git a/pylot/loss/flat.py b/pylot/loss/flat.py
index 22fc118..ee77585 100644
--- a/pylot/loss/flat.py
+++ b/pylot/loss/flat.py
@@ -1,6 +1,5 @@
 from functools import wraps
-from torch import nn
-import torch.nn.functional as F
+from pylot.torch.torchlib import nn
 
 
 def flatten_loss(loss_func, keep_channels=False):
diff --git a/pylot/loss/ncc.py b/pylot/loss/ncc.py
index 2e1dc17..8bc8294 100644
--- a/pylot/loss/ncc.py
+++ b/pylot/loss/ncc.py
@@ -1,8 +1,7 @@
 import math
 
 import numpy as np
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 from .util import _loss_module_from_func
 
diff --git a/pylot/loss/segmentation.py b/pylot/loss/segmentation.py
index db7c04f..5de68fe 100644
--- a/pylot/loss/segmentation.py
+++ b/pylot/loss/segmentation.py
@@ -1,15 +1,12 @@
 from typing import Optional, Union
 
-import torch
-from torch import Tensor
-
 from pydantic import validate_call
 
 from .util import _loss_module_from_func
 from ..util.more_functools import partial
 from ..metrics.segmentation import soft_dice_score, soft_jaccard_score, pixel_mse
 from ..metrics.util import InputMode, Reduction
-
+from pylot.torch.torchlib import torch, Tensor
 
 @validate_call(config=dict(arbitrary_types_allowed=True))
 def soft_dice_loss(
diff --git a/pylot/loss/total_variation.py b/pylot/loss/total_variation.py
index 596a9e6..aa57040 100644
--- a/pylot/loss/total_variation.py
+++ b/pylot/loss/total_variation.py
@@ -1,8 +1,6 @@
 from typing import Optional
 
-import torch
-from torch import Tensor
-
+from pylot.torch.torchlib import torch
 from .util import _loss_module_from_func
 
 
diff --git a/pylot/loss/vae.py b/pylot/loss/vae.py
index 3946006..df44346 100644
--- a/pylot/loss/vae.py
+++ b/pylot/loss/vae.py
@@ -1,5 +1,4 @@
-import torch
-import torch.nn.functional as F
+from pylot.torch.torchlib import torch, F
 
 def kld_loss(mu, logvar):
     # see Appendix B from VAE paper:
diff --git a/pylot/loss/wgan.py b/pylot/loss/wgan.py
index 40d34b9..f36bbce 100644
--- a/pylot/loss/wgan.py
+++ b/pylot/loss/wgan.py
@@ -1,4 +1,4 @@
-import torch
+from pylot.torch.torchlib import torch
 
 
 def gradient_penalty(y, x, device):
diff --git a/pylot/metrics/activation.py b/pylot/metrics/activation.py
index e110653..20a4184 100644
--- a/pylot/metrics/activation.py
+++ b/pylot/metrics/activation.py
@@ -1,6 +1,5 @@
 import numpy as np
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 from .size import dtype2bits
 from ..pruning.utils import get_activations
diff --git a/pylot/metrics/segmentation.py b/pylot/metrics/segmentation.py
index 3177a27..b209106 100644
--- a/pylot/metrics/segmentation.py
+++ b/pylot/metrics/segmentation.py
@@ -1,7 +1,6 @@
 from typing import Optional, Union, List, Literal
 
-import torch
-from torch import Tensor
+from pylot.torch.torchlib import torch, Tensor
 from pydantic import validate_call
 
 
diff --git a/pylot/metrics/size.py b/pylot/metrics/size.py
index 630f9d3..8d70cb4 100644
--- a/pylot/metrics/size.py
+++ b/pylot/metrics/size.py
@@ -1,5 +1,5 @@
 import numpy as np
-import torch
+from pylot.torch.torchlib import torch
 
 # https://pytorch.org/docs/stable/tensor_attributes.html
 dtype2bits = {
diff --git a/pylot/metrics/util.py b/pylot/metrics/util.py
index 5adc352..579842f 100644
--- a/pylot/metrics/util.py
+++ b/pylot/metrics/util.py
@@ -2,8 +2,7 @@ from typing import Literal, Optional, Tuple, Union
 
 import einops as E
 from pydantic import validate_call
-import torch
-import torch.nn.functional as F
+from pylot.torch.torchlib import torch, F
 from torch import Tensor
 
 InputMode = Literal["binary", "multiclass", "onehot", "auto"]
diff --git a/pylot/models/hyper/base.py b/pylot/models/hyper/base.py
index d6079d5..7ca9de9 100644
--- a/pylot/models/hyper/base.py
+++ b/pylot/models/hyper/base.py
@@ -2,9 +2,7 @@ from dataclasses import dataclass
 from typing import List, Dict, Tuple, Optional, Union
 
 import numpy as np
-import torch
-from torch import nn
-from torch import Tensor
+from pylot.torch.torchlib import torch, nn, Tensor
 
 from ...nn.init import initialize_layer
 from ...nn.nonlinearity import get_nonlinearity
diff --git a/pylot/models/hyper/container.py b/pylot/models/hyper/container.py
index 6e592e1..5b07e9d 100644
--- a/pylot/models/hyper/container.py
+++ b/pylot/models/hyper/container.py
@@ -1,8 +1,7 @@
 import copy
 from typing import Dict, Any
 
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 from .base import HyperNet
 from .delta import DeltaHyperNet
diff --git a/pylot/models/hyper/delta.py b/pylot/models/hyper/delta.py
index fc54e09..437c401 100644
--- a/pylot/models/hyper/delta.py
+++ b/pylot/models/hyper/delta.py
@@ -1,8 +1,7 @@
 from dataclasses import dataclass
 from typing import Dict, Optional, Union
 
-import torch
-from torch import Tensor, nn
+from pylot.torch.torchlib import torch, nn, Tensor
 
 from ...nn.init import initialize_bias, initialize_weight
 from .base import HyperNet
diff --git a/pylot/models/hyper/encoder.py b/pylot/models/hyper/encoder.py
index 83d44b4..6b8c4c3 100644
--- a/pylot/models/hyper/encoder.py
+++ b/pylot/models/hyper/encoder.py
@@ -1,5 +1,5 @@
 import math
-import torch
+from pylot.torch.torchlib import torch
 
 
 class Encoder:
diff --git a/pylot/models/hyper/norm.py b/pylot/models/hyper/norm.py
index d88a4ad..637e499 100644
--- a/pylot/models/hyper/norm.py
+++ b/pylot/models/hyper/norm.py
@@ -2,9 +2,7 @@ from dataclasses import dataclass
 from typing import List, Dict, Tuple, Optional, Union
 
 import numpy as np
-import torch
-from torch import nn
-from torch import Tensor
+from pylot.torch.torchlib import torch, nn, Tensor
 
 from ...nn.init import initialize_layer
 from ...nn.nonlinearity import get_nonlinearity
diff --git a/pylot/models/unet.py b/pylot/models/unet.py
index eaea529..9acdf8a 100644
--- a/pylot/models/unet.py
+++ b/pylot/models/unet.py
@@ -1,8 +1,7 @@
 from dataclasses import dataclass
 from typing import Any, Dict, List, Literal, Optional
 
-import torch
-import torch.nn.functional as F
+from pylot.torch.torchlib import torch, F
 from torch import Tensor, nn
 
 from ..nn import ConvBlock, get_nonlinearity
diff --git a/pylot/models/vae.py b/pylot/models/vae.py
index f488740..a355d9f 100644
--- a/pylot/models/vae.py
+++ b/pylot/models/vae.py
@@ -2,8 +2,7 @@ import math
 
 from typing import List, Dict, Optional, Any, Tuple
 
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 from ..loss import vae_loss
 
 
diff --git a/pylot/models/vision/cifar_resnet.py b/pylot/models/vision/cifar_resnet.py
index c23131a..c88adfa 100644
--- a/pylot/models/vision/cifar_resnet.py
+++ b/pylot/models/vision/cifar_resnet.py
@@ -30,10 +30,7 @@ Reference:
 If you use this implementation in you work, please don't forget to mention the
 author, Yerlan Idelbayev.
 """
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.nn.init as init
+from pylot.torch.torchlib import torch, nn, F
 
 
 __all__ = [
@@ -49,7 +46,7 @@ __all__ = [
 
 def _weights_init(m):
     if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
-        init.kaiming_normal_(m.weight)
+        nn.init.kaiming_normal_(m.weight)
 
 
 class LambdaLayer(nn.Module):
diff --git a/pylot/models/vision/cifar_vgg.py b/pylot/models/vision/cifar_vgg.py
index 39db7be..a1cf5ff 100644
--- a/pylot/models/vision/cifar_vgg.py
+++ b/pylot/models/vision/cifar_vgg.py
@@ -3,12 +3,7 @@
 # BLOG POST : http://torch.ch/blog/2015/07/30/cifar.html
 
 import math
-import torch
-import torch.nn as nn
-
-# import torch.nn.functional as F
-import torch.nn.init as init
-
+from pylot.torch.torchlib import torch, nn
 
 
 class ConvBNReLU(nn.Module):
@@ -86,9 +81,9 @@ class VGGBnDrop(nn.Module):
     def reset_weights(self):
         def init_weights(module):
             if isinstance(module, nn.Conv2d):
-                fan_in, _ = init._calculate_fan_in_and_fan_out(module.weight)
-                init.normal_(module.weight, 0, math.sqrt(2) / fan_in)
-                init.zeros_(module.bias)
+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)
+                nn.init.normal_(module.weight, 0, math.sqrt(2) / fan_in)
+                nn.init.zeros_(module.bias)
 
         self.apply(init_weights)
 
diff --git a/pylot/models/vision/fresnet.py b/pylot/models/vision/fresnet.py
index f54efab..8ab175e 100644
--- a/pylot/models/vision/fresnet.py
+++ b/pylot/models/vision/fresnet.py
@@ -1,4 +1,4 @@
-import torch.nn as nn
+from pylot.torch.torchlib import nn
 from torchvision.models.resnet import ResNet, Bottleneck, BasicBlock
 
 __all__ = [
diff --git a/pylot/models/vision/mnistnet.py b/pylot/models/vision/mnistnet.py
index 726ad47..cb1db02 100644
--- a/pylot/models/vision/mnistnet.py
+++ b/pylot/models/vision/mnistnet.py
@@ -3,9 +3,7 @@
 [description]
 """
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+from pylot.torch.torchlib import torch, nn, F
 
 # from . import weights_path
 
diff --git a/pylot/models/void/unet.py b/pylot/models/void/unet.py
index 0706ef8..54255bd 100644
--- a/pylot/models/void/unet.py
+++ b/pylot/models/void/unet.py
@@ -1,10 +1,7 @@
 from dataclasses import dataclass
 from typing import List, Optional, Dict, Any
 
-import torch
-from torch import Tensor
-from torch import nn
-import torch.nn.functional as F
+from pylot.torch.torchlib import torch, Tensor, nn, F
 
 from ...nn import get_nonlinearity
 from ...nn.hyper import VoidModule, VoidConvBlock
diff --git a/pylot/models/voxelmorph.py b/pylot/models/voxelmorph.py
index ba5a539..9c9b577 100644
--- a/pylot/models/voxelmorph.py
+++ b/pylot/models/voxelmorph.py
@@ -1,7 +1,6 @@
 from typing import Dict, Tuple, Any
 
-import torch
-from torch import nn, Tensor
+from pylot.torch.torchlib import torch, nn, Tensor
 
 from ..nn.spatial_transformer import SpatialTransformer
 from .unet import UNet
diff --git a/pylot/models/wgan.py b/pylot/models/wgan.py
index 41bf823..0092382 100644
--- a/pylot/models/wgan.py
+++ b/pylot/models/wgan.py
@@ -1,5 +1,4 @@
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 # from ..loss.wgan import gradient_penalty, G_wgan, D_wgan_gp
 
diff --git a/pylot/nn/batch_renorm.py b/pylot/nn/batch_renorm.py
index f4aed85..a6338ff 100644
--- a/pylot/nn/batch_renorm.py
+++ b/pylot/nn/batch_renorm.py
@@ -1,7 +1,7 @@
 # modified from  
 # https://github.com/ludvb/batchrenorm/blob/master/batchrenorm/batchrenorm.py
 # https://github.com/mf1024/Batch-Renormalization-PyTorch/blob/master/batch_renormalization.py
-import torch
+from pylot.torch.torchlib import torch
 from pydantic import validate_call
 
 
diff --git a/pylot/nn/bmap.py b/pylot/nn/bmap.py
index e42b514..ffb873c 100644
--- a/pylot/nn/bmap.py
+++ b/pylot/nn/bmap.py
@@ -2,8 +2,7 @@ from typing import Callable, Tuple, Union
 
 import numpy as np
 
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 Axes = Union[int, Tuple[int, ...]]
 
diff --git a/pylot/nn/bufferdict.py b/pylot/nn/bufferdict.py
index e83bb12..253b736 100644
--- a/pylot/nn/bufferdict.py
+++ b/pylot/nn/bufferdict.py
@@ -7,11 +7,10 @@ from __future__ import annotations
 import collections
 from collections import OrderedDict
 
-import torch
-from torch.nn import Module
+from pylot.torch.torchlib import torch, nn
 
 
-class BufferDict(Module):
+class BufferDict(nn.Module):
     r"""Holds buffers in a dictionary.
 
     BufferDict can be indexed like a regular Python dictionary, but buffers it
diff --git a/pylot/nn/conv_block.py b/pylot/nn/conv_block.py
index 16a9ea5..66ccba0 100644
--- a/pylot/nn/conv_block.py
+++ b/pylot/nn/conv_block.py
@@ -1,13 +1,14 @@
 from dataclasses import dataclass
 from typing import Optional, Union, Literal, List, Any, Dict
 
-import torch.nn as nn
+
 
 from .nonlinearity import get_nonlinearity
 from .init import initialize_layer
 from .norm import get_normlayer, NormType
 from .drop import DropPath
 from ..util.validation import validate_arguments_init
+from pylot.torch.torchlib import nn
 
 
 @validate_arguments_init
diff --git a/pylot/nn/ema.py b/pylot/nn/ema.py
index 4980c89..3279c87 100644
--- a/pylot/nn/ema.py
+++ b/pylot/nn/ema.py
@@ -11,8 +11,7 @@ from typing import Optional
 
 from pydantic import validate_call
 
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 
 class ModelEMA(nn.Module):
diff --git a/pylot/nn/flatten.py b/pylot/nn/flatten.py
index 5dd61fd..5f4e96e 100644
--- a/pylot/nn/flatten.py
+++ b/pylot/nn/flatten.py
@@ -1,8 +1,6 @@
 from typing import Sequence, Iterable, Tuple
 
-import torch
-from torch import nn
-from torch import Tensor
+from pylot.torch.torchlib import torch, nn, Tensor
 
 
 class Flatten(nn.Module):
diff --git a/pylot/nn/frozen_batchnorm.py b/pylot/nn/frozen_batchnorm.py
index 4d75e09..e0c0bae 100644
--- a/pylot/nn/frozen_batchnorm.py
+++ b/pylot/nn/frozen_batchnorm.py
@@ -1,5 +1,4 @@
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 
 class FrozenBatchNorm2d(nn.Module):
diff --git a/pylot/nn/hyper/linear.py b/pylot/nn/hyper/linear.py
index 1353456..b36c2fb 100644
--- a/pylot/nn/hyper/linear.py
+++ b/pylot/nn/hyper/linear.py
@@ -1,5 +1,4 @@
-from torch import Tensor
-import torch.nn.functional as F
+from pylot.torch.torchlib import Tensor, F
 
 from .void import VoidModule
 from .xparam import ExternalParameter
diff --git a/pylot/nn/interpolation.py b/pylot/nn/interpolation.py
index 63c4ccb..58072aa 100644
--- a/pylot/nn/interpolation.py
+++ b/pylot/nn/interpolation.py
@@ -1,4 +1,4 @@
-import torch.nn.functional as F
+from pylot.torch.torchlib import F
 
 
 def resize(x, scale_factor=None, size=None, interpolation_mode="linear"):
diff --git a/pylot/nn/nonlinearity.py b/pylot/nn/nonlinearity.py
index 34606e2..b4ca959 100644
--- a/pylot/nn/nonlinearity.py
+++ b/pylot/nn/nonlinearity.py
@@ -1,5 +1,4 @@
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 
 class Sine(nn.Module):
diff --git a/pylot/nn/norm.py b/pylot/nn/norm.py
index 001bc04..7a72a2c 100644
--- a/pylot/nn/norm.py
+++ b/pylot/nn/norm.py
@@ -1,9 +1,8 @@
 from typing import Literal, Tuple, Union
 
 import einops as E
-import torch
 from pylot.nn import batch_renorm
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 NormType = Union[
     Literal["batch", "batchre", "layer", "instance", "channel"],
diff --git a/pylot/nn/record.py b/pylot/nn/record.py
index 83387f3..245cc1d 100644
--- a/pylot/nn/record.py
+++ b/pylot/nn/record.py
@@ -1,7 +1,6 @@
 import numpy as np
 import pandas as pd
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 from contextlib import contextmanager
 
diff --git a/pylot/nn/regularization.py b/pylot/nn/regularization.py
index 1b38a21..5c4388b 100644
--- a/pylot/nn/regularization.py
+++ b/pylot/nn/regularization.py
@@ -1,6 +1,4 @@
-import torch
-from torch import nn
-import torch.autograd
+from pylot.torch.torchlib import torch, nn
 
 
 # Functions to perform L1 and L2 regularization on the output of a layer
diff --git a/pylot/nn/samplewise.py b/pylot/nn/samplewise.py
index 74a6915..94b0260 100644
--- a/pylot/nn/samplewise.py
+++ b/pylot/nn/samplewise.py
@@ -1,9 +1,7 @@
 from typing import List
 
 import numpy as np
-import torch
-from torch import Tensor
-from torch import nn
+from pylot.torch.torchlib import torch, nn, Tensor
 
 
 class SamplewiseLayerNorm(nn.Module):
diff --git a/pylot/nn/spatial_transformer.py b/pylot/nn/spatial_transformer.py
index afb9ba0..cf8064f 100644
--- a/pylot/nn/spatial_transformer.py
+++ b/pylot/nn/spatial_transformer.py
@@ -1,6 +1,4 @@
-import torch
-from torch import nn
-import torch.nn.functional as F
+from pylot.torch.torchlib import torch, nn, F
 
 
 class SpatialTransformer(nn.Module):
diff --git a/pylot/nn/squeeze_excite.py b/pylot/nn/squeeze_excite.py
index 5652860..2843ec2 100644
--- a/pylot/nn/squeeze_excite.py
+++ b/pylot/nn/squeeze_excite.py
@@ -1,5 +1,4 @@
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 
 # See:
 #
diff --git a/pylot/nn/vmap.py b/pylot/nn/vmap.py
index 496ac93..5d68ec9 100644
--- a/pylot/nn/vmap.py
+++ b/pylot/nn/vmap.py
@@ -1,7 +1,6 @@
 from typing import Callable
 
-import torch
-from torch import nn
+from pylot.torch.torchlib import torch, nn
 import einops as E
 
 # Jaxlike vmap exploiting reshaping to the batch dimension.
diff --git a/pylot/nn/weight_norm.py b/pylot/nn/weight_norm.py
index 068b97d..9e2f590 100644
--- a/pylot/nn/weight_norm.py
+++ b/pylot/nn/weight_norm.py
@@ -1,6 +1,4 @@
-import torch
-import torch.nn as nn
-from torch.nn import Parameter
+from pylot.torch.torchlib import torch, nn
 
 # Hookless implementation of weight norm
 
@@ -22,8 +20,8 @@ class WeightNorm(nn.Module):
             # construct g,v such that w = g/||v|| * v
             g = torch.norm(w)
             v = w/g.expand_as(w)
-            g = Parameter(g.data)
-            v = Parameter(v.data)
+            g = nn.Parameter(g.data)
+            v = nn.Parameter(v.data)
 
             # remove w from parameter list
             del self.module._parameters[name_w]
diff --git a/pylot/optim/larc.py b/pylot/optim/larc.py
index adff647..a28e49f 100644
--- a/pylot/optim/larc.py
+++ b/pylot/optim/larc.py
@@ -1,4 +1,5 @@
-import torch
+from pylot.torch.torchlib import torch
+
 from torch.optim import SGD
 
 from ..util import delegates, separate_kwargs
@@ -112,8 +113,8 @@ class LARC(object):
 
 class SGDLARC(LARC):
     @delegates(to=LARC.__init__)
-    @delegates(to=SGD.__init__, keep=True, but=["eps", "trust_coef"])
+    @delegates(to=torch.optim.SGD.__init__, keep=True, but=["eps", "trust_coef"])
     def __init__(self, params, **kwargs):
-        sgd_kwargs, lars_kwargs = separate_kwargs(kwargs, SGD.__init__)
-        optim = SGD(params, **sgd_kwargs)
+        sgd_kwargs, lars_kwargs = separate_kwargs(kwargs, torch.optim.SGD.__init__)
+        optim = torch.optim.SGD(params, **sgd_kwargs)
         super().__init__(optim, **lars_kwargs)
diff --git a/pylot/optim/lars.py b/pylot/optim/lars.py
index fb4ceef..75f7315 100644
--- a/pylot/optim/lars.py
+++ b/pylot/optim/lars.py
@@ -6,8 +6,7 @@ References:
     - https://github.com/noahgolmant/pytorch-lars/blob/master/lars.py
 """
 
-import torch
-from torch.optim import SGD
+from pylot.torch.torchlib import torch
 
 from .wrapper import OptimWrapper
 from ..util import delegates, separate_kwargs
@@ -120,8 +119,8 @@ class LARS(OptimWrapper):
 
 class SGDLARS(LARS):
     @delegates(to=LARS.__init__)
-    @delegates(to=SGD.__init__, keep=True, but=["eps", "trust_coef"])
+    @delegates(to=torch.optim.SGD.__init__, keep=True, but=["eps", "trust_coef"])
     def __init__(self, params, **kwargs):
-        sgd_kwargs, lars_kwargs = separate_kwargs(kwargs, SGD.__init__)
-        optim = SGD(params, **sgd_kwargs)
+        sgd_kwargs, lars_kwargs = separate_kwargs(kwargs, torch.optim.SGD.__init__)
+        optim = torch.optim.SGD(params, **sgd_kwargs)
         super().__init__(optim, **lars_kwargs)
diff --git a/pylot/optim/sam.py b/pylot/optim/sam.py
index 4128b58..28de185 100644
--- a/pylot/optim/sam.py
+++ b/pylot/optim/sam.py
@@ -1,5 +1,4 @@
-import torch
-
+from pylot.torch.torchlib import torch
 from .wrapper import OptimWrapper
 
 
diff --git a/pylot/optim/wrapper.py b/pylot/optim/wrapper.py
index 08d1481..24e1518 100644
--- a/pylot/optim/wrapper.py
+++ b/pylot/optim/wrapper.py
@@ -1,8 +1,7 @@
-import torch
-from torch.optim import Optimizer
+from pylot.torch.torchlib import torch
 
 
-class OptimWrapper(Optimizer):
+class OptimWrapper(torch.optim.Optimizer):
 
     # Mixin class that defines convenient functions for writing Optimizer Wrappers
 
diff --git a/pylot/torch/__init__.py b/pylot/torch/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/pylot/torch/api.py b/pylot/torch/api.py
index c6f0299..28a1b1e 100644
--- a/pylot/torch/api.py
+++ b/pylot/torch/api.py
@@ -1,7 +1,6 @@
 from fastcore.utils import patch
 import numpy
-import torch
-
+from pylot.torch.torchlib import torch
 
 @patch
 def np(tensor: torch.Tensor) -> numpy.ndarray:
diff --git a/pylot/torch/torchlib.py b/pylot/torch/torchlib.py
new file mode 100644
index 0000000..19bbae4
--- /dev/null
+++ b/pylot/torch/torchlib.py
@@ -0,0 +1,10 @@
+"""
+'barrel imports' for pytorch
+"""
+
+__all__ = ["torch", "nn", "Tensor", "F", "Dataset", "DataLoader"]
+
+import torch
+from torch import nn, Tensor
+import torch.nn.functional as F
+from torch.utils.data import Dataset, DataLoader
diff --git a/pylot/util/__init__.py b/pylot/util/__init__.py
index 76551c5..13184ca 100644
--- a/pylot/util/__init__.py
+++ b/pylot/util/__init__.py
@@ -17,7 +17,6 @@ from .notify import beep
 from .shapecheck import ShapeChecker
 from .pipes import redirect_std, Tee, quiet_std, Unbuffered, Capturing, temporary_save_path, quiet_stdout
 from .print import printy, hsizeof
-from .s3 import S3Path, make_path
 from .store import AutoStore
 from .timer import Timer, StatsTimer, CUDATimer, StatsCUDATimer, HistoryTimer, HistoryCUDATimer
 from .torchutils import to_device, trace_model_viz, torch_traceback
diff --git a/pylot/util/dupes.py b/pylot/util/dupes.py
index 4d3f609..d942ae5 100644
--- a/pylot/util/dupes.py
+++ b/pylot/util/dupes.py
@@ -3,7 +3,6 @@ from typing import List, Dict
 from collections import defaultdict
 
 import numpy as np
-from scipy.spatial.distance import pdist
 
 
 def undirected_connected_components(graph: Dict[int, List[int]]) -> List[List[int]]:
@@ -26,6 +25,7 @@ def undirected_connected_components(graph: Dict[int, List[int]]) -> List[List[in
 
 # Function to find sets of duplicate rows
 def array_duplicate_rowsets(arrays: np.ndarray, metric="cityblock", names=None):
+    from scipy.spatial.distance import pdist
     # compute distances and threshold
     indices = np.arange(len(arrays))
     pair_dist = np.isclose(pdist(arrays, metric=metric), 0)
diff --git a/pylot/util/env.py b/pylot/util/env.py
index d7680e3..5b77ca5 100644
--- a/pylot/util/env.py
+++ b/pylot/util/env.py
@@ -6,8 +6,7 @@ from datetime import datetime
 
 import jc
 import psutil
-import torch
-
+from pylot.torch.torchlib import torch
 
 def platform_info():
     return {
diff --git a/pylot/util/img.py b/pylot/util/img.py
index 7f85df7..c3b1b0f 100644
--- a/pylot/util/img.py
+++ b/pylot/util/img.py
@@ -1,8 +1,6 @@
 import numpy as np
 from PIL import Image, ImageChops, ImageOps
-
-import torch
-
+from pylot.torch.torchlib import torch
 
 def RGB_project(x):
     C, H, W = x.shape
diff --git a/pylot/util/imports.py b/pylot/util/imports.py
new file mode 100644
index 0000000..f33101b
--- /dev/null
+++ b/pylot/util/imports.py
@@ -0,0 +1,28 @@
+__all__ = [
+    'optional_import'
+]
+
+import importlib.util
+import types
+from typing import Optional
+
+def optional_import(module_name: str) -> Optional[types.ModuleType]:
+    """
+    Attempt to import a module by name only if it is installed.
+
+    Parameters
+    ----------
+    module_name : str
+        The name of the module to import.
+
+    Returns
+    -------
+    module : Optional[module]
+        The imported module if available, else None.
+    """
+    spec = importlib.util.find_spec(module_name)
+    if spec is not None:
+        module = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(module)
+        return module
+    return None
\ No newline at end of file
diff --git a/pylot/util/ioutil.py b/pylot/util/ioutil.py
index 77a39b2..84d186c 100644
--- a/pylot/util/ioutil.py
+++ b/pylot/util/ioutil.py
@@ -18,7 +18,6 @@ import PIL.JpegImagePlugin
 import pyarrow as pa
 import pyarrow.feather as feather
 import pyarrow.parquet as pq
-import scipy.io
 import xxhash
 import yaml
 import zstd
@@ -26,7 +25,7 @@ from loguru import logger
 from loguru._logger import FileSink
 
 import pandas as pd
-import torch
+from pylot.torch.torchlib import torch
 
 m.patch()
 
@@ -471,11 +470,13 @@ class MatFormat(FileFormat):
 
     @classmethod
     def save(cls, obj, fp):
+        import scipy.io
         fp = cls.check_fp(fp)
         scipy.io.savemat(fp, obj)
 
     @classmethod
     def load(cls, fp) -> object:
+        import scipy.io
         fp = cls.check_fp(fp)
         return scipy.io.loadmat(fp)
 
diff --git a/pylot/util/libcheck.py b/pylot/util/libcheck.py
index 90429ff..a4c5b56 100644
--- a/pylot/util/libcheck.py
+++ b/pylot/util/libcheck.py
@@ -26,13 +26,13 @@ def check_scipy_mkl():
 
 
 def check_torch_cuda():
-    import torch
+    from pylot.torch.torchlib import torch
 
     return torch.cuda.is_available()
 
 
 def check_torch_cudnn_benchmark():
-    import torch
+    from pylot.torch.torchlib import torch
 
     return torch.backends.cudnn.benchmark
 
diff --git a/pylot/util/meter.py b/pylot/util/meter.py
index 9d79d0b..d96b0b3 100644
--- a/pylot/util/meter.py
+++ b/pylot/util/meter.py
@@ -4,7 +4,7 @@ from typing import Union, Iterable
 from heapq import heappop, heappush
 
 import numpy as np
-import torch
+from pylot.torch.torchlib import torch
 
 Numeric = Union[np.ndarray, torch.Tensor, int, float]
 Numerics = Union[Iterable[Numeric]]
diff --git a/pylot/util/profile.py b/pylot/util/profile.py
index d4dc660..de6d835 100644
--- a/pylot/util/profile.py
+++ b/pylot/util/profile.py
@@ -1,7 +1,7 @@
 import itertools
 from typing import Tuple, List
 
-import torch
+from pylot.torch.torchlib import torch
 from tqdm.auto import tqdm
 
 import pandas as pd
diff --git a/pylot/util/s3.py b/pylot/util/s3.py
deleted file mode 100644
index e5e7410..0000000
--- a/pylot/util/s3.py
+++ /dev/null
@@ -1,158 +0,0 @@
-import os
-import pathlib
-import shutil
-import uuid
-from contextlib import contextmanager
-from typing import Optional
-
-import s3fs
-
-_s3driver = None
-
-
-def make_path(path):
-    if path.startswith("s3://"):
-        S3Path.init_driver(
-            os.environ["S3_HOST"], os.environ["S3_KEY"], os.environ["S3_SECRET"]
-        )
-        return S3Path(path[len("s3://") :])
-    return pathlib.Path(path)
-
-
-class S3Path(pathlib.PosixPath):
-
-    _s3driver: Optional[s3fs.S3FileSystem] = None
-
-    @classmethod
-    def get_driver(cls):
-        assert (
-            cls._s3driver is not None
-        ), "Driver must be initialized with S3Path.init_driver"
-        return cls._s3driver
-
-    @classmethod
-    def init_driver(cls, host, key, secret):
-        if cls._s3driver is None:
-            cls._s3driver = s3fs.S3FileSystem(
-                anon=False,
-                client_kwargs=dict(endpoint_url=host),
-                key=key,
-                secret=secret,
-            )
-
-    # def __new__(cls, *args, **kwargs):
-    def _init(self, *args, **kwargs):
-        if self._s3driver is None and all(
-            k in os.environ for k in ("S3_HOST", "S3_KEY", "S3_SECRET")
-        ):
-            S3Path.init_driver(
-                os.environ["S3_HOST"], os.environ["S3_KEY"], os.environ["S3_SECRET"]
-            )
-        assert (
-            self._s3driver is not None
-        ), "Driver must be initialized with S3Path.init_driver"
-        super()._init(*args, **kwargs)
-
-    def chmod(self, mode):
-        return self._s3driver.chmod(str(self), mode)
-
-    def exists(self):
-        return self._s3driver.exists(str(self))
-
-    def glob(self, pattern):
-        for p in self._s3driver.glob(str(self) + "/" + pattern):
-            yield p
-
-    def mkdir(self, parents=False, exist_ok=True):
-        if self.exists() and not exist_ok:
-            raise ValueError("Folder already exists")
-        if not self.exists():
-            (self / "._s3").touch()
-        # self._s3driver.mkdir(str(self), create_parents=parents)
-
-    @contextmanager
-    def open(self, mode="r"):
-        with self._s3driver.open(str(self), mode) as f:
-            yield f
-
-    def rename(self, target):
-        if isinstance(target, str):
-            target = S3Path(target)
-        assert isinstance(target, S3Path)
-        self._s3driver.rename(str(self), str(target))
-        return target
-
-    def rmdir(self):
-        (self / "._s3").unlink(missing_ok=True)
-        # self._s3driver.rmdir(str(self))
-
-    def stat(self):
-        return self._s3driver.stat(str(self))
-
-    def touch(self):
-        self._s3driver.touch(str(self), truncate=False)
-
-    def iterdir(self):
-        for path in self._s3driver.ls(str(self)):
-            yield S3Path(path)
-
-    def unlink(self, missing_ok=False):
-        try:
-            self._s3driver.rm(str(self))
-        except FileNotFoundError:
-            if missing_ok:
-                return
-            raise
-
-    def is_file(self):
-        return self._s3driver.is_file(str(self))
-
-    def is_dir(self):
-        return self._s3driver.is_dir(str(self))
-
-    def rmtree(self):
-        return self._s3driver.rm(str(self), recursive=True)
-
-    @staticmethod
-    @contextmanager
-    def as_local(remote_path):
-        if isinstance(remote_path, S3Path):
-            local_path = pathlib.Path("/tmp/" + uuid.uuid4().hex)
-            local_path = local_path.with_suffix(remote_path.suffix)
-
-            if remote_path.exists():
-                remote_path._s3driver.get(
-                    str(remote_path), str(local_path), recursive=True
-                )
-
-            yield local_path
-            remote_path._s3driver.put(str(local_path), str(remote_path), recursive=True)
-
-            if local_path.is_file():
-                local_path.unlink()
-            else:
-                shutil.rmtree(local_path)
-        else:
-            yield remote_path
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}('{super().__str__()}')"
-
-    def __str__(self):
-        return f"s3://{super().__str__()}"
-
-
-# @contextmanager
-# def local_path(path):
-#     if isinstance(path, S3Path):
-#         local_path = pathlib.Path('/tmp/' + uuid.uuid4().hex)
-#         local_path = local_path.with_suffix(path.suffix)
-#         yield local_path
-#         driver = path.get_driver()
-#         driver.put(str(local_path), str(path), recursive=True)
-#         if local_path.is_file():
-#             local_path.unlink()
-#         else:
-#             shutil.rmtree(local_path)
-#     else:
-#         yield path
diff --git a/pylot/util/shapecheck.py b/pylot/util/shapecheck.py
index 820a4c2..d987ffc 100644
--- a/pylot/util/shapecheck.py
+++ b/pylot/util/shapecheck.py
@@ -1,6 +1,6 @@
 import string
 from typing import Union, Tuple
-import torch
+from pylot.torch.torchlib import torch
 import numpy as np
 from tabulate import tabulate
 
diff --git a/pylot/util/summary.py b/pylot/util/summary.py
index 13dd551..7e53c45 100644
--- a/pylot/util/summary.py
+++ b/pylot/util/summary.py
@@ -2,8 +2,7 @@ import pathlib
 from collections import OrderedDict
 
 import numpy as np
-import torch
-import torch.nn as nn
+from pylot.torch.torchlib import torch, nn
 
 
 def summary(model, input_size, batch_size=-1, device="cuda", echo=True, as_stats=False):
diff --git a/pylot/util/timer.py b/pylot/util/timer.py
index 9bcbcdd..0f6cf27 100644
--- a/pylot/util/timer.py
+++ b/pylot/util/timer.py
@@ -3,8 +3,7 @@ from contextlib import contextmanager
 import functools
 import time
 
-import torch
-import torch.cuda
+from pylot.torch.torchlib import torch
 import pandas as pd
 
 from .meter import StatsMeter
diff --git a/pylot/util/torchutils.py b/pylot/util/torchutils.py
index 0c54c71..9be5e6e 100644
--- a/pylot/util/torchutils.py
+++ b/pylot/util/torchutils.py
@@ -1,5 +1,4 @@
-import torch
-
+from pylot.torch.torchlib import torch
 
 def to_device(inputs, device, channels_last=False):
 
@@ -31,7 +30,6 @@ def to_device(inputs, device, channels_last=False):
 
 
 def torch_traceback():
-    import torch
     from rich.traceback import install
 
     install(show_locals=True)

commit b341002725c745602424599b65004c46439cbef5
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue May 6 12:23:22 2025 -0400

    [REFACTOR] Remove unused imports

diff --git a/pylot/analysis/collect.py b/pylot/analysis/collect.py
index c5dced9..4d24ae0 100644
--- a/pylot/analysis/collect.py
+++ b/pylot/analysis/collect.py
@@ -1,9 +1,8 @@
 import collections
 import getpass
 import itertools
-import json
 import pathlib
-from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
+from concurrent.futures import ThreadPoolExecutor
 from fnmatch import fnmatch
 from typing import Optional, List, Union, Dict, Tuple
 
@@ -16,7 +15,6 @@ import pandas as pd
 
 # unused import because we want the patching side-effects
 # on pd.DataFrames
-from ..pandas import api
 from ..pandas.api import augment_from_attrs
 from ..pandas.convenience import (
     ensure_hashable, to_categories, concat_with_attrs, remove_redundant_columns
diff --git a/pylot/analysis/data.py b/pylot/analysis/data.py
index 5e9af62..ac070a0 100644
--- a/pylot/analysis/data.py
+++ b/pylot/analysis/data.py
@@ -1,9 +1,3 @@
-import collections
-import functools
-import inspect
-import pathlib
-import shutil
-import numpy as np
 import pandas as pd
 
 
diff --git a/pylot/augmentation/geometry.py b/pylot/augmentation/geometry.py
index 326a543..5126a33 100644
--- a/pylot/augmentation/geometry.py
+++ b/pylot/augmentation/geometry.py
@@ -1,8 +1,4 @@
-from typing import Optional, Union
-
-import torch
 import kornia.augmentation as KA
-from kornia.constants import Resample, SamplePadding
 
 
 def RandomScale(scale, **kwargs):
diff --git a/pylot/augmentation/variable.py b/pylot/augmentation/variable.py
index 03a0433..e4f0ac5 100644
--- a/pylot/augmentation/variable.py
+++ b/pylot/augmentation/variable.py
@@ -1,10 +1,8 @@
-import random
 from typing import Optional, Union, Tuple, Dict
 
 import torch
 import numpy as np
 import kornia as K
-import kornia.augmentation as KA
 from kornia.constants import BorderType
 
 from pydantic import validate_call
diff --git a/pylot/callbacks/debug.py b/pylot/callbacks/debug.py
index 69c57ff..e56cfc5 100644
--- a/pylot/callbacks/debug.py
+++ b/pylot/callbacks/debug.py
@@ -1,4 +1,3 @@
-import time
 import signal
 import sys
 import subprocess
@@ -37,7 +36,7 @@ class InspectStack:
     """
 
     def __init__(self, experiment):
-        import stackprinter
+        pass
 
         signal.signal(signal.SIGUSR2, inspect_stack_handler)
 
@@ -96,7 +95,6 @@ class TorchTraceback:
 
 
 ######################################
-import torch.cuda
 import subprocess
 
 # from ..util.gpu import gpu_stats
diff --git a/pylot/callbacks/ema.py b/pylot/callbacks/ema.py
index 911fac8..51ddfad 100644
--- a/pylot/callbacks/ema.py
+++ b/pylot/callbacks/ema.py
@@ -1,4 +1,3 @@
-import torch
 
 from pylot.util import autosave, autoload
 
diff --git a/pylot/callbacks/epoch.py b/pylot/callbacks/epoch.py
index c0fd9bb..09cdfb2 100644
--- a/pylot/callbacks/epoch.py
+++ b/pylot/callbacks/epoch.py
@@ -1,10 +1,9 @@
 import copy
 import sys
 import time
-from collections import defaultdict
 from datetime import datetime, timedelta
 from fnmatch import fnmatch
-from typing import Literal, Union
+from typing import Literal
 
 import numpy as np
 from pydantic import validate_call
@@ -24,16 +23,21 @@ def PrintLogged(experiment):
 
         df = df[df.epoch == epoch].drop(columns=["epoch"])
 
-        dfp = pd.pivot(
+        logger.critical(df)
+
+        dfp = pd.pivot_table(
             pd.melt(
                 df,
                 id_vars="phase",
-                var_name="metric"
+                var_name="metric",
             ),
             index="metric",
             columns="phase",
+            aggfunc="mean",
         )
 
+        logger.critical(dfp)
+
         dfp.columns = [x[1] for x in dfp.columns]
 
         # Log the metrics table.
diff --git a/pylot/callbacks/img.py b/pylot/callbacks/img.py
index 42049c3..7be8852 100644
--- a/pylot/callbacks/img.py
+++ b/pylot/callbacks/img.py
@@ -4,7 +4,6 @@ from typing import Optional, List, Any
 import torch
 from torch import Tensor
 
-from ..util.img import toImg, torch_renorm
 from ..nn.hooks import HookedModule
 
 from .util import sample_batches, tensor2html
diff --git a/pylot/callbacks/profile.py b/pylot/callbacks/profile.py
index ef66fbd..332d4e2 100644
--- a/pylot/callbacks/profile.py
+++ b/pylot/callbacks/profile.py
@@ -4,7 +4,6 @@ import math
 from tabulate import tabulate
 from loguru import logger
 
-from ..util import printc
 from ..util.timer import StatsCUDATimer, StatsTimer
 from ..util.torchutils import to_device
 
diff --git a/pylot/callbacks/setup.py b/pylot/callbacks/setup.py
index 4843a40..bbb39cb 100644
--- a/pylot/callbacks/setup.py
+++ b/pylot/callbacks/setup.py
@@ -1,6 +1,5 @@
 import json
 
-import pandas as pd
 from tabulate import tabulate
 from loguru import logger
 
diff --git a/pylot/callbacks/template.py b/pylot/callbacks/template.py
index 407b4bf..6ecc75d 100644
--- a/pylot/callbacks/template.py
+++ b/pylot/callbacks/template.py
@@ -1,8 +1,4 @@
-import json
-import os
 import pathlib
-
-
 import jinja2
 
 
diff --git a/pylot/datasets/lmdb.py b/pylot/datasets/lmdb.py
index 2028c94..a75f6a4 100644
--- a/pylot/datasets/lmdb.py
+++ b/pylot/datasets/lmdb.py
@@ -8,7 +8,7 @@
 import io
 import math
 from concurrent.futures import ProcessPoolExecutor
-from typing import Any, Callable, Literal, Optional
+from typing import Callable, Literal, Optional
 
 import lmdb
 import numpy as np
@@ -18,7 +18,6 @@ from tqdm.auto import tqdm
 
 from pylot.datasets import IndexedImageFolder
 from pylot.util.ioutil import autodecode, autoencode
-from torch.utils.data import Dataset
 
 
 class ImageFolderLMDB(VisionDataset):
diff --git a/pylot/datasets/prepare.py b/pylot/datasets/prepare.py
index 8413ee9..1255458 100644
--- a/pylot/datasets/prepare.py
+++ b/pylot/datasets/prepare.py
@@ -2,7 +2,6 @@
 # https://albumentations.ai/docs/examples/pytorch_semantic_segmentation/
 from urllib.request import urlretrieve
 import os
-import pathlib
 import shutil
 from tqdm.auto import tqdm
 
diff --git a/pylot/datasets/selfsuper.py b/pylot/datasets/selfsuper.py
index 9b3734c..07b93f2 100644
--- a/pylot/datasets/selfsuper.py
+++ b/pylot/datasets/selfsuper.py
@@ -1,4 +1,3 @@
-from dataclasses import dataclass
 from functools import wraps
 from typing import Tuple
 
diff --git a/pylot/datasets/vision/imagenette.py b/pylot/datasets/vision/imagenette.py
index 9869aed..a1f6d62 100644
--- a/pylot/datasets/vision/imagenette.py
+++ b/pylot/datasets/vision/imagenette.py
@@ -3,7 +3,6 @@ from typing import Literal
 from ..lmdb import ImageFolderLMDB
 from ..path import DatapathMixin
 
-from pydantic import validate_call
 from ...util.meta import store_attr
 
 __all__ = ["Imagenette", "Imagewoof"]
diff --git a/pylot/datasets/vision/nist.py b/pylot/datasets/vision/nist.py
index 9902170..77b02ae 100644
--- a/pylot/datasets/vision/nist.py
+++ b/pylot/datasets/vision/nist.py
@@ -1,12 +1,7 @@
-import functools
-import pathlib
-import os
 from typing import Literal
 
-from torchvision import transforms, datasets
 import torchvision.transforms as TF
 import torchvision.datasets as TD
-import PIL
 from pydantic import validate_call
 from sklearn.model_selection import train_test_split
 import numpy as np
diff --git a/pylot/datasets/vision/notmnist.py b/pylot/datasets/vision/notmnist.py
index 1b5cab0..9642b82 100644
--- a/pylot/datasets/vision/notmnist.py
+++ b/pylot/datasets/vision/notmnist.py
@@ -6,11 +6,11 @@ import numpy as np
 from PIL import Image
 from pydantic import validate_call
 from sklearn.model_selection import train_test_split
-from torchvision.datasets import ImageFolder, MNIST
+from torchvision.datasets import MNIST
 
 from tqdm.auto import tqdm
 
-from pylot.datasets import DatapathMixin, IndexedImageFolder
+from pylot.datasets import IndexedImageFolder
 
 # Download and untar
 # http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz
diff --git a/pylot/datasets/vision/pascal.py b/pylot/datasets/vision/pascal.py
index 1877c51..e32de99 100644
--- a/pylot/datasets/vision/pascal.py
+++ b/pylot/datasets/vision/pascal.py
@@ -1,4 +1,3 @@
-from dataclasses import dataclass
 from functools import lru_cache
 from typing import Tuple
 
diff --git a/pylot/datasets/vision/usps28.py b/pylot/datasets/vision/usps28.py
index 3343342..cf8d675 100644
--- a/pylot/datasets/vision/usps28.py
+++ b/pylot/datasets/vision/usps28.py
@@ -1,5 +1,4 @@
 import os
-from typing import Optional, Callable
 
 import torch
 import numpy as np
diff --git a/pylot/experiment/config.py b/pylot/experiment/config.py
index 6bfb334..b9bed21 100644
--- a/pylot/experiment/config.py
+++ b/pylot/experiment/config.py
@@ -1,6 +1,6 @@
 import yaml
 import hashlib
-from collections.abc import Mapping, MutableMapping
+from collections.abc import MutableMapping
 
 # from .util import dict_recursive_update, expand_dots, expand_keys
 from ..util import allbut, expand_keys
diff --git a/pylot/experiment/distributed.py b/pylot/experiment/distributed.py
index be7bd44..2ba6510 100644
--- a/pylot/experiment/distributed.py
+++ b/pylot/experiment/distributed.py
@@ -2,7 +2,6 @@ from torch.nn.parallel import DistributedDataParallel as DDP
 import torch.distributed as dist
 import os
 from pylot.experiment import TrainExperiment
-from pylot.util.pipes import quiet_std
 from pylot.util import ThunderDict, MetricsDict
 from torch.utils.data.distributed import DistributedSampler
 from torch.utils.data import DataLoader
diff --git a/pylot/experiment/util.py b/pylot/experiment/util.py
index b9324f3..110133a 100644
--- a/pylot/experiment/util.py
+++ b/pylot/experiment/util.py
@@ -1,8 +1,5 @@
 import datetime
-import functools
-import string
 import random
-import time
 import importlib
 import pathlib
 import random
diff --git a/pylot/loss/segmentation.py b/pylot/loss/segmentation.py
index 19fcf3a..db7c04f 100644
--- a/pylot/loss/segmentation.py
+++ b/pylot/loss/segmentation.py
@@ -1,4 +1,4 @@
-from typing import Optional, Union, Literal
+from typing import Optional, Union
 
 import torch
 from torch import Tensor
diff --git a/pylot/metrics/accuracy.py b/pylot/metrics/accuracy.py
index fcab848..db1cb5d 100644
--- a/pylot/metrics/accuracy.py
+++ b/pylot/metrics/accuracy.py
@@ -1,7 +1,5 @@
 import numpy as np
-import torch
 
-from torch import Tensor
 
 
 def correct(output, target, topk=(1,)):
diff --git a/pylot/metrics/efficiency/flops.py b/pylot/metrics/efficiency/flops.py
index 88ee8a0..70f050d 100644
--- a/pylot/metrics/efficiency/flops.py
+++ b/pylot/metrics/efficiency/flops.py
@@ -1,4 +1,3 @@
-import numpy as np
 
 from torch import nn
 
diff --git a/pylot/metrics/segmentation.py b/pylot/metrics/segmentation.py
index 08afe4e..3177a27 100644
--- a/pylot/metrics/segmentation.py
+++ b/pylot/metrics/segmentation.py
@@ -2,7 +2,6 @@ from typing import Optional, Union, List, Literal
 
 import torch
 from torch import Tensor
-import torch.nn.functional as F
 from pydantic import validate_call
 
 
diff --git a/pylot/metrics/util.py b/pylot/metrics/util.py
index 4771101..5adc352 100644
--- a/pylot/metrics/util.py
+++ b/pylot/metrics/util.py
@@ -1,9 +1,7 @@
-from enum import Enum
 from typing import Literal, Optional, Tuple, Union
 
 import einops as E
 from pydantic import validate_call
-
 import torch
 import torch.nn.functional as F
 from torch import Tensor
diff --git a/pylot/models/conv.py b/pylot/models/conv.py
index 400dba7..08265e9 100644
--- a/pylot/models/conv.py
+++ b/pylot/models/conv.py
@@ -3,7 +3,6 @@ from dataclasses import dataclass
 from typing import List, Optional, Dict, Any
 
 from torch import nn, Tensor
-import torch.nn.functional as F
 
 from ..util import validate_arguments_init
 from ..nn import ConvBlock, resize, Flatten
diff --git a/pylot/models/hyper/container.py b/pylot/models/hyper/container.py
index b360e8b..6e592e1 100644
--- a/pylot/models/hyper/container.py
+++ b/pylot/models/hyper/container.py
@@ -1,5 +1,5 @@
 import copy
-from typing import Dict, Any, Union
+from typing import Dict, Any
 
 import torch
 from torch import nn
diff --git a/pylot/models/hyper/norm.py b/pylot/models/hyper/norm.py
index 731ea2c..d88a4ad 100644
--- a/pylot/models/hyper/norm.py
+++ b/pylot/models/hyper/norm.py
@@ -13,7 +13,6 @@ from .encoder import Encoder
 
 Shapes = Dict[str, Tuple[int, ...]]
 
-from .base import HyperNet
 
 
 class HyperWeightNorm(nn.Module):
diff --git a/pylot/models/vae.py b/pylot/models/vae.py
index 256b9e3..f488740 100644
--- a/pylot/models/vae.py
+++ b/pylot/models/vae.py
@@ -1,5 +1,4 @@
 import math
-from dataclasses import dataclass
 
 from typing import List, Dict, Optional, Any, Tuple
 
diff --git a/pylot/models/vision/cifar_resnet.py b/pylot/models/vision/cifar_resnet.py
index 8c4fe3b..c23131a 100644
--- a/pylot/models/vision/cifar_resnet.py
+++ b/pylot/models/vision/cifar_resnet.py
@@ -35,7 +35,6 @@ import torch.nn as nn
 import torch.nn.functional as F
 import torch.nn.init as init
 
-from . import weights_path
 
 __all__ = [
     "ResNet",
diff --git a/pylot/models/vision/cifar_vgg.py b/pylot/models/vision/cifar_vgg.py
index a52135a..39db7be 100644
--- a/pylot/models/vision/cifar_vgg.py
+++ b/pylot/models/vision/cifar_vgg.py
@@ -9,7 +9,6 @@ import torch.nn as nn
 # import torch.nn.functional as F
 import torch.nn.init as init
 
-from . import weights_path
 
 
 class ConvBNReLU(nn.Module):
diff --git a/pylot/nn/batch_renorm.py b/pylot/nn/batch_renorm.py
index e039d05..f4aed85 100644
--- a/pylot/nn/batch_renorm.py
+++ b/pylot/nn/batch_renorm.py
@@ -3,7 +3,6 @@
 # https://github.com/mf1024/Batch-Renormalization-PyTorch/blob/master/batch_renormalization.py
 import torch
 from pydantic import validate_call
-import einops as E
 
 
 __all__ = ["BatchRenorm1d", "BatchRenorm2d", "BatchRenorm3d"]
diff --git a/pylot/nn/hyper/conv.py b/pylot/nn/hyper/conv.py
index f554c2e..0fff70c 100644
--- a/pylot/nn/hyper/conv.py
+++ b/pylot/nn/hyper/conv.py
@@ -1,4 +1,4 @@
-from typing import Optional, Tuple, Union
+from typing import Tuple, Union
 
 from torch import nn, Tensor
 
diff --git a/pylot/nn/hyper/conv_block.py b/pylot/nn/hyper/conv_block.py
index 13af4a7..274f73c 100644
--- a/pylot/nn/hyper/conv_block.py
+++ b/pylot/nn/hyper/conv_block.py
@@ -1,13 +1,10 @@
+from typing import List
+
 from torch import nn
 
 from . import separable
 from ...nn.nonlinearity import get_nonlinearity
-from ...nn.init import initialize_layer
-
-from typing import List, Optional, Union
-
 from .. import hyper
-
 from .void import VoidModule
 
 
diff --git a/pylot/nn/init.py b/pylot/nn/init.py
index 50ad2b7..c529905 100644
--- a/pylot/nn/init.py
+++ b/pylot/nn/init.py
@@ -1,6 +1,5 @@
 import warnings
 
-import torch
 from torch import nn
 from torch.nn import init
 
diff --git a/pylot/nn/norm.py b/pylot/nn/norm.py
index f17f496..001bc04 100644
--- a/pylot/nn/norm.py
+++ b/pylot/nn/norm.py
@@ -2,7 +2,6 @@ from typing import Literal, Tuple, Union
 
 import einops as E
 import torch
-from pydantic import validate_call
 from pylot.nn import batch_renorm
 from torch import nn
 
diff --git a/pylot/nn/record.py b/pylot/nn/record.py
index 02874a4..83387f3 100644
--- a/pylot/nn/record.py
+++ b/pylot/nn/record.py
@@ -6,7 +6,6 @@ from torch import nn
 from contextlib import contextmanager
 
 from pylot.nn.hooks import HookedModule
-from pylot.util.meta import store_attr
 from pylot.nn.hyper import VoidModule
 
 from fnmatch import fnmatch
diff --git a/pylot/nn/weight_norm.py b/pylot/nn/weight_norm.py
index cced5e9..068b97d 100644
--- a/pylot/nn/weight_norm.py
+++ b/pylot/nn/weight_norm.py
@@ -1,7 +1,6 @@
 import torch
 import torch.nn as nn
 from torch.nn import Parameter
-from functools import wraps
 
 # Hookless implementation of weight norm
 
diff --git a/pylot/optim/larc.py b/pylot/optim/larc.py
index 75cbfa4..adff647 100644
--- a/pylot/optim/larc.py
+++ b/pylot/optim/larc.py
@@ -1,7 +1,5 @@
 import torch
-from torch import nn
 from torch.optim import SGD
-from torch.nn.parameter import Parameter
 
 from ..util import delegates, separate_kwargs
 
diff --git a/pylot/plot/cmap.py b/pylot/plot/cmap.py
index 4009f7a..8840edd 100644
--- a/pylot/plot/cmap.py
+++ b/pylot/plot/cmap.py
@@ -1,4 +1,3 @@
-import matplotlib.pyplot as plt
 from matplotlib.colors import ListedColormap
 
 Monokai = ListedColormap(
diff --git a/pylot/plot/scales.py b/pylot/plot/scales.py
index 6f5e71a..2a5754c 100644
--- a/pylot/plot/scales.py
+++ b/pylot/plot/scales.py
@@ -1,7 +1,4 @@
-import matplotlib.pyplot as plt
 import matplotlib.scale as mscale
-import matplotlib.transforms as mtransforms
-import numpy as np
 
 
 class Log2Scale(mscale.LogScale):
diff --git a/pylot/util/img.py b/pylot/util/img.py
index f5175ee..7f85df7 100644
--- a/pylot/util/img.py
+++ b/pylot/util/img.py
@@ -62,7 +62,7 @@ def hist_equalize(img: np.ndarray):
     hist, bins = np.histogram(img.flatten(), 256, [0, 256])
 
     cdf = hist.cumsum()
-    cdf_normalized = cdf * hist.max() / cdf.max()
+    cdf * hist.max() / cdf.max()
 
     cdf_m = np.ma.masked_equal(cdf, 0)
     cdf_m = (cdf_m - cdf_m.min()) * 255 / (cdf_m.max() - cdf_m.min())
diff --git a/pylot/util/ioutil.py b/pylot/util/ioutil.py
index 95a65e1..77a39b2 100644
--- a/pylot/util/ioutil.py
+++ b/pylot/util/ioutil.py
@@ -5,9 +5,9 @@ import json
 import pathlib
 import pickle
 import shutil
-from abc import ABC, abstractmethod
+from abc import ABC
 from contextlib import contextmanager
-from typing import Any, Literal, Optional, Union
+from typing import Any, Union
 
 import lz4.frame
 import msgpack
diff --git a/pylot/util/libcheck.py b/pylot/util/libcheck.py
index 76ac8a0..90429ff 100644
--- a/pylot/util/libcheck.py
+++ b/pylot/util/libcheck.py
@@ -41,9 +41,9 @@ def check_environment():
     try:
         from rich.console import Console
         error_console = Console(stderr=True, style='bold red', highlighter=None)
-        warn = error_console.print
+        error_console.print
     except ImportError:
-        from warnings import warn
+        pass
 
     #if not check_numpy_mkl():
     #    warn("Intel MKL extensions not available for NumPy")
diff --git a/pylot/util/meter.py b/pylot/util/meter.py
index 1776e9e..9d79d0b 100644
--- a/pylot/util/meter.py
+++ b/pylot/util/meter.py
@@ -2,7 +2,6 @@ import itertools
 from abc import abstractmethod
 from typing import Union, Iterable
 from heapq import heappop, heappush
-from collections import defaultdict
 
 import numpy as np
 import torch
diff --git a/pylot/util/s3.py b/pylot/util/s3.py
index bd36b40..e5e7410 100644
--- a/pylot/util/s3.py
+++ b/pylot/util/s3.py
@@ -1,6 +1,5 @@
 import os
 import pathlib
-import tempfile
 import shutil
 import uuid
 from contextlib import contextmanager
diff --git a/pylot/util/torchutils.py b/pylot/util/torchutils.py
index 5ca3abc..0c54c71 100644
--- a/pylot/util/torchutils.py
+++ b/pylot/util/torchutils.py
@@ -31,7 +31,7 @@ def to_device(inputs, device, channels_last=False):
 
 
 def torch_traceback():
-    import torch, numpy
+    import torch
     from rich.traceback import install
 
     install(show_locals=True)
@@ -63,7 +63,6 @@ def _make_graph(
         return find_name(of, self_input, suffix=cur)
 
     gr = mod.graph
-    toshow = []
     # list(traced_model.graph.nodes())[0]
     self_input = next(gr.inputs())
     self_type = self_input.type().str().split(".")[-1]
diff --git a/tests/test_experiments.py b/tests/test_experiments.py
index 2112c5c..39ffe0d 100644
--- a/tests/test_experiments.py
+++ b/tests/test_experiments.py
@@ -3,8 +3,6 @@ Tests for PyLot experiments.
 """
 
 import pytest
-from loguru import logger
-import yaml
 from pathlib import Path
 import pylot.experiment.base as base
 from pylot.util.ioutil import autosave

commit 707b623b7942976d2ed025237f79491fccf291c4
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue May 6 12:01:32 2025 -0400

    [TESTS] Add test for determining how much time it takes to import pylot

diff --git a/tests/benchmarks/importing_time_calculator.py b/tests/benchmarks/importing_time_calculator.py
new file mode 100644
index 0000000..cd9fd86
--- /dev/null
+++ b/tests/benchmarks/importing_time_calculator.py
@@ -0,0 +1,59 @@
+"""
+Calculates import times from a log file containing all imports
+
+Examples
+--------
+>>> # Gather the import times into a log file
+>>> python -X importtime -c "import pylot" > tests/benchmarks/import_time.log 2>&1
+
+Notes
+-----
+As of 2025-05-06:
+
+torch               : 8844.80 ms
+scipy               : 2945.41 ms
+sympy               : 2711.64 ms
+wandb               : 2626.32 ms
+pandas              : 1804.84 ms
+kornia              : 1588.28 ms
+torchvision         : 1251.38 ms
+matplotlib          : 929.33 ms
+IPython             : 796.19 ms
+prompt_toolkit      : 781.53 ms
+pylot               : 635.98 ms
+sklearn             : 556.98 ms
+wandb_graphql       : 555.94 ms
+jedi                : 498.02 ms
+botocore            : 471.99 ms
+mpmath              : 435.02 ms
+numpy               : 365.64 ms
+triton              : 305.96 ms
+git                 : 272.53 ms
+joblib              : 269.09 ms
+"""
+
+# Parse into a summary of total time spent per top-level package
+from collections import defaultdict
+
+summary = defaultdict(int)
+
+log_path = 'tests/benchmarks/import_time.log'
+
+with open(log_path, 'r') as f:
+    for line in f:
+        if not line.startswith("import time:"):
+            continue
+        try:
+            parts = line.split('|')
+            time_us = int(parts[0].replace("import time:", "").strip())
+            module = parts[2].strip().split('.')[0]  # top-level module
+            summary[module] += time_us
+        except Exception:
+            continue  # Skip malformed lines
+
+# Sort by cumulative import time
+sorted_summary = sorted(summary.items(), key=lambda x: x[1], reverse=True)
+
+# Print summary
+for module, time_us in sorted_summary[:20]:
+    print(f"{module:20s}: {time_us / (1000):.2f} ms")

commit 3c5cbd68439a15cd6ca6e367a5845738525b176a
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 22:06:11 2025 -0400

    [TESTS] Add first tests for experiments

diff --git a/tests/test_experiments.py b/tests/test_experiments.py
new file mode 100644
index 0000000..2112c5c
--- /dev/null
+++ b/tests/test_experiments.py
@@ -0,0 +1,85 @@
+"""
+Tests for PyLot experiments.
+"""
+
+import pytest
+from loguru import logger
+import yaml
+from pathlib import Path
+import pylot.experiment.base as base
+from pylot.util.ioutil import autosave
+from pylot.util.config import ImmutableConfig, FHDict
+from pylot.util.metrics import MetricsDict
+from pylot.util.thunder import ThunderDict
+
+@pytest.fixture
+def exp_dir(tmp_path: Path):
+    """
+    Make a minimal experiment directory with a super simple config.yml
+
+    Notes
+    -----
+    `tmp_path` is a built-in pytest fixture that points to the OS's temp dir.
+    """
+
+    # Make the directory in the filesystem
+    exp_dir_path = tmp_path / "exp"
+    exp_dir_path.mkdir()
+
+    config = {
+        'experiment': {'seed': 19},
+        'log': {'root': 'dalcalab_root'},
+    }
+
+    # Write the configuration to the yaml
+    config_path = exp_dir_path / 'config.yml'
+    autosave(config, config_path)
+
+    return exp_dir_path
+
+
+def test_base_exp_init(exp_dir: str):
+    """
+    Test that the initialization sets instance attributes correctly.
+    """
+
+    # Initialize the experiment
+    exp = base.BaseExperiment(
+        path=str(exp_dir),
+        logging_level='info'
+    )
+
+    assert exp.path == exp_dir
+    assert exp.name == exp_dir.stem
+    assert isinstance(exp.config, ImmutableConfig)
+    assert exp.config._data['experiment']['seed'] == 19
+    assert isinstance(exp.properties, FHDict)
+    assert isinstance(exp.metadata, FHDict)
+    assert isinstance(exp.metricsd, MetricsDict)
+    assert isinstance(exp.store, ThunderDict)
+
+    # Hash uses the exp dir path
+    assert hash(exp) == hash(exp_dir)
+
+
+def test_metrics_property(exp_dir):
+    """
+    Test that the metrics property properly records the metrics.
+    """
+
+    # Initialize experiment
+    exp = base.BaseExperiment(str(exp_dir))
+
+    # Log a metric
+    first_metric_real = {'dice': 0.75}
+    exp.metricsd['metrics'].log(first_metric_real)
+
+    # Log another metric
+    second_metric_real = {'mse': 1.0232}
+    exp.metricsd['metrics'].log(second_metric_real)
+    
+    first_metric = exp.metrics.data[0]
+    second_metric = exp.metrics.data[1]
+
+    assert first_metric_real == first_metric
+    assert second_metric_real == second_metric
\ No newline at end of file

commit a002fc8ac49f1dc0bcbdfc4bbf6975b317e5701c
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 21:30:33 2025 -0400

    [DOCS] Update docstring in TrainExperiment

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index 121720e..149c798 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -33,12 +33,25 @@ class TrainExperiment(BaseExperiment):
 
     Attributes
     ----------
-    config : dict
-        inherited from `BaseExperiment`.
-    properties : dict
-        inherited from `BaseExperiment`.
+    path : pathlib.Path
+        Absolute path to a particular experimental run.
+    name : str
+        Name of the particular experimental run in the format
+        `YYYYMMDD_HHMMSS-nonce-hash`
+    config : ImmutableConfig
+        Dictionary of the experiment's configuration that is not mutable.
+        Contains a configuration hash.
+    properties : HDict
+        Hierarchical dictionary (supporting dotted keys) containing the
+        properties of the experiment such as {epoch, num_params, ...}
     metadata : dict
         inherited from `BaseExperiment`.
+    metricsd : MetricsDict
+        Dictionary containig all the metrics.
+    store : ThunderDict
+        Not sure
+    callbacks : dict
+        Callbacks to run at ...
     train_dataset : torch.utils.data.Dataset
         Training dataset set by `build_data()` method.
     val_dataset : torch.utils.data.Dataset
@@ -52,6 +65,22 @@ class TrainExperiment(BaseExperiment):
     properties : dict
         Properties and metadata of `TrainExperiment`
     optim : torch.optim
+    state : dict
+        A dictionary containing `model` (the model state dict), `optim`
+        (the optimizer's state dict) and `_epoch` (the current epoch in
+        training, derived from the `properties` attribute)
+    checkpoints : list
+        A list of absolute paths to all checkpoints for an experimental run. 
+
+    Methods
+    -------
+    set_state(state, strict)
+        Restore the model and optimizer states from a checkpoint dictionary.
+    checkpoint(tag='last')
+        Save the current state of the model, optimizer, and epoch to a
+        checkpoint.
+    load(tag='last')
+        Load a checkpoint and restore the model, optimizer, and epoch states.
     """
 
     def __init__(self, path, *args, **kwargs):

commit bdcefabd7beeac8e4eab1054a36684b72bdb3b67
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 21:30:15 2025 -0400

    [DOCS] Update attributes in BaseExperiment

diff --git a/pylot/experiment/base.py b/pylot/experiment/base.py
index a728ef8..6b95927 100644
--- a/pylot/experiment/base.py
+++ b/pylot/experiment/base.py
@@ -71,11 +71,13 @@ class BaseExperiment:
     Attributes
     ----------
     path : pathlib.Path
-        Path to the directory containing the experiment run.
+        Absolute path to a particular experimental run.
     name : str
-        Name of the experiment, derived from the run directory stem.
+        Name of the particular experimental run in the format
+        `YYYYMMDD_HHMMSS-nonce-hash`
     config : ImmutableConfig
-        Configuration object loaded from `config.yml`.
+        Dictionary of the experiment's configurations that is not mutable.
+        Contains a configuration hash.
     properties : FHDict
         Dictionary-like store for experiment properties saved to JSON.
     metadata : FHDict
@@ -107,6 +109,7 @@ class BaseExperiment:
         if isinstance(path, str):
             path = pathlib.Path(path)
 
+        # Store the path as an attribute
         self.path = path
 
         # Make sure logging is set up and make first log confirming experiment

commit 6bc5bf353524d84df897bf65c56d982a1b025579
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 21:29:52 2025 -0400

    [DOCS] Document config module more

diff --git a/pylot/util/config.py b/pylot/util/config.py
index 39bfb10..82cdbaf 100644
--- a/pylot/util/config.py
+++ b/pylot/util/config.py
@@ -277,33 +277,154 @@ class HDict(MutableMapping):
             val = self.__class__(val, _sep=self._sep)
         return val
 
-    # Composite methods
-    def get(self, key, default=None):
-        if key not in self:
-            return default
-        return self[key]
+    def __setitem__(self, key: Key, value):
+        """
+        Set a value using a composite key.
+
+        Parameters
+        ----------
+        key : str
+            Composite key where value will be placed.
+        value : object
+            Value to assign.
+
+        Examples
+        --------
+        >>> h = HDict()
+        >>> h['foo.bar'] = 10
+        >>> h['foo.bar']
+        10
+        """
+
+        return set_nested(
+            nested_dict=self._data,
+            key=key,
+            value=value,
+            sep=self._sep
+        )
+
+    def __delitem__(self, key: Key):
+        """
+        Delete an item using a composite key.
+
+        Parameters
+        ----------
+        key : str
+            Composite key to delete.
+
+        Examples
+        --------
+        >>> h = HDict({'x.y': 1})
+        >>> del h['x.y']
+        >>> 'x.y' in h
+        False
+        """
+
+        return del_nested(
+            nested_dict=self._data,
+            key=key,
+            sep=self._sep
+        )
 
-    # Delegated methods
     def __iter__(self):
+        """
+        Return an iterator over top-level keys.
+
+        Returns
+        -------
+        iterator
+            Iterator over keys.
+
+        Examples
+        --------
+        >>> list(HDict({'a': 1, 'b': 2}))
+        ['a', 'b']
+        """
+
         return iter(self._data)
 
     def __len__(self):
+        """
+        Return the number of top-level items.
+
+        Returns
+        -------
+        int
+            Number of items.
+
+        Examples
+        --------
+        >>> len(HDict({'a.b': 1, 'a.c': 2}))
+        1
+        """
+
         return len(self._data)
 
-    # Mutable methods
-    def __setitem__(self, key: Key, value):
-        return set_nested(self._data, key, value, sep=self._sep)
+    def get(self, key: str, default=None):
+        """
+        Return value for key if key is in dictionary, else default.
 
-    def __delitem__(self, key: Key):
-        return del_nested(self._data, key, sep=self._sep)
+        Parameters
+        ----------
+        key : str
+            Composite key.
+        default : object, optional
+            Default value if key is not found.
+
+        Returns
+        -------
+        object
+            Value for the key or default.
+
+        Examples
+        --------
+        >>> h = HDict({'k.v': 9})
+        >>> h.get('k.v')
+        9
+        >>> h.get('missing', 0)
+        0
+        """
+
+        if key not in self:
+            return default
+
+        return self[key]
 
     def pop(self, key: Key, *default):
+        """
+        Remove specified key and return the corresponding value.
+
+        Parameters
+        ----------
+        key : str
+            Composite key to remove.
+        default : object, optional
+            Value to return if key is not found.
+
+        Returns
+        -------
+        object
+            Value of the removed key or default.
+
+        Examples
+        --------
+        >>> h = HDict({'a.b': 2})
+        >>> h.pop('a.b')
+        2
+        >>> h.pop('not.there', 5)
+        5
+        """
+
         assert len(default) <= 1, "Expected at most one default value"
+
         if key not in self and len(default) > 0:
             return default[0]
+
         val = pop_nested(self._data, key, sep=self._sep)
+
         if isinstance(val, dict):
             val = HDict(val, _sep=self._sep)
+
         return val
 
     def update(self, other: Union["HDict", dict]):

commit 1a784298c202c07731d1d535f3cdf2cf6aca0e82
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 20:48:49 2025 -0400

    [DOCS,WIP] Begin documenting config.py

diff --git a/pylot/util/config.py b/pylot/util/config.py
index 4142a2d..39bfb10 100644
--- a/pylot/util/config.py
+++ b/pylot/util/config.py
@@ -154,18 +154,71 @@ def contains_nested(nested_dict, key, sep=None) -> bool:
 
 
 class HDict(MutableMapping):
-    # Hierarchical Dictionary
+    """
+    Hierarchical dictionary supporting dotted key paths for nested structures.
+
+    This class allows intuitive access to nested dictionaries using string keys
+    separated by a custom delimiter (default is '.'). It supports both
+    immutable and mutable dictionary operations and can serialize/deserialize
+    its content.
+
+    Attributes
+    ----------
+    _data : dict
+        Internal dictionary holding the nested data.
+    _sep : str
+        Separator used for composite keys.
+
+    Examples
+    --------
+    >>> h = HDict({'a.b': 1, 'a.c': 2})
+    >>> h['a.b']
+    1
+    >>> h['a']['c']
+    2
+    """
 
     def __init__(self, initial=None, *, _sep="."):
+        """
+        Initialize the `HDict`.
+
+        Parameters
+        ----------
+        initial : dict or HDict, optional
+            Initial data to populate the dictionary.
+        _sep : str, optional
+            Separator used for nested keys (default is ".").
+        """
+
+        # Store separator for nested key access
         self._sep = _sep
+
+        # Initialize the data store
         self._data = dict()
+
+        # Initialize with a deep nested structure if provided
         if initial is not None:
+
+            # Convert into normal dictionary
             if isinstance(initial, HDict):
                 initial = initial.to_dict()
+
+            # Convert flattened dict to nested structure
             initial = unflatten(initial, sep=self._sep)
+
+            # Override _data with the initialized content
             self._data = dict(initial)
 
     def __repr__(self):
+        """
+        Return the string representation of the HDict.
+
+        Returns
+        -------
+        str
+            String representation of the object.
+        """
+
         s = f"{self.__class__.__name__}({repr(self._data)}"
         if self._sep != ".":
             s += f", _sep={repr(self._sep)}"
@@ -173,9 +226,52 @@ class HDict(MutableMapping):
         return s
 
     def __contains__(self, key: Key) -> bool:
-        return contains_nested(self._data, key, sep=self._sep)
+        """
+        Check whether a key exists in the dictionary.
+
+        Parameters
+        ----------
+        key : str
+            Composite key to query
+
+        Returns
+        -------
+        bool
+            True if key exists, False otherwise.
+
+        Examples
+        --------
+        >>> 'a.b' in HDict({'a.b': 1})
+        True
+        """
+
+        return contains_nested(
+            nested_dict=self._data, key=key, sep=self._sep
+        )
 
     def __getitem__(self, key: Key):
+        """
+        Retrieve a value using a composite key with custom seperator.
+
+        Parameters
+        ----------
+        key : str
+            Composite key for nested access.
+
+        Returns
+        -------
+        object or HDict
+            Retrieved value; wrapped in HDict if it is a dict.
+
+        Examples
+        --------
+        >>> h = HDict({'x.y': 3})
+        >>> h['x.y']
+        3
+        >>> type(h['x'])
+        <class '__main__.HDict'>
+        """
+
         val = get_nested(self._data, key, sep=self._sep)
         if isinstance(val, dict):
             val = self.__class__(val, _sep=self._sep)
@@ -194,14 +290,6 @@ class HDict(MutableMapping):
     def __len__(self):
         return len(self._data)
 
-    # Methods are inherited from Mapping mixin
-    # def keys(self):
-    #     return self._data.keys()
-    # def items(self):
-    #     return self._data.items()
-    # def values(self):
-    #     return self._data.values()
-
     # Mutable methods
     def __setitem__(self, key: Key, value):
         return set_nested(self._data, key, value, sep=self._sep)
@@ -303,6 +391,34 @@ class Config(HDict):
 
 
 class ImmutableConfig(HDict):
+    """
+    An immutable configuration dictionary that computes a digest and
+    supports hashing, but disallows any modification operations.
+
+    This class inherits from `HDict` and overrides mutation methods
+    to raise an `ImmutableConfigError`, ensuring that instances
+    remain unchanged after creation.
+
+    Methods
+    -------
+    __hash__():
+        Returns the hash of the configuration based on its digest.
+    
+    digest():
+        Computes a hexadecimal digest of the internal data.
+    
+    __setitem__(key, value):
+        Disabled. Raises an ImmutableConfigError.
+    
+    __delitem__(key):
+        Disabled. Raises an ImmutableConfigError.
+    
+    pop(key):
+        Disabled. Raises an ImmutableConfigError.
+    
+    update(other):
+        Disabled. Raises an ImmutableConfigError.
+    """
     def __hash__(self):
         return hash(int(self.digest(), 16))
 

commit fac0352a5c6d3131a5655f9a8e223f6cf82b862b
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 18:53:44 2025 -0400

    [DOCS] Add documentation for the load() method of TrainExperiment

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index 702f513..121720e 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -424,10 +424,40 @@ class TrainExperiment(BaseExperiment):
             return checkpoints
         return [c.stem for c in checkpoints]
 
-    def load(self, tag=None):
+    def load(self, tag: str = 'last'):
+        """
+        Load a checkpoint and restore the model, optimizer, and epoch state.
+
+        This method is used to resume training or begin evaluation from a saved
+        checkpoint. It has the following operations:
+           1. Build the path to the requested checkpoint.
+           2. Load the checkpoint dictionary from path with `torch.load()`.
+           3. Restore the model, optimizer, and epoch states from the
+           checkpoint dictionary using `self.set_state()`.
+
+        Parameters
+        ----------
+        tag : str, optional
+            A string identifier for the checkpoint file to load. Defaults
+            to 'last', which typically represents the most recent checkpoint.
+
+        Returns
+        -------
+        self : object
+            Returns the experiment instance.
+
+        Examples
+        --------
+        >>> # Load from the 10th epoch
+        >>> trainer.load("epoch_10")
+        >>> # OR resume from the last checkpoint
+        >>> trainer.load()
+        """
+
+        # Construct the path to the checkpoint directory
         checkpoint_dir = self.path / "checkpoints"
-        tag = tag if tag is not None else "last"
 
+        # Open and load the checkpoint file
         with (checkpoint_dir / f"{tag}.pt").open("rb") as f:
             state = torch.load(
                 f=f,
@@ -435,8 +465,10 @@ class TrainExperiment(BaseExperiment):
                 map_location=self.device
             )
 
+            # Restore model, optimizer, and epoch from state
             self.set_state(state)
 
+        # Log the successful load
         logger.info(
             f"Loaded checkpoint with tag:{tag}. "
             f"Last epoch:{self.properties['epoch']}"

commit ad5018976a0dd80123c3174dd574a0e1952530e8
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 18:45:14 2025 -0400

    [DOCS] Add documentation to state(), set_state(), and checkpoint() methods of TrainExperiment

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index f82d857..702f513 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -288,16 +288,79 @@ class TrainExperiment(BaseExperiment):
 
     @property
     def state(self):
+        """
+        The current state of the model, optimizer, and epoch number.
+
+        This property constructs and returns a dictionary representing the
+        current training state, including the model's state dict (its
+        parameters), the optimizer state dict, and the current epoch. It is
+        intended for use in checkpointing, saving, and restoring training
+        progress.
+
+        Returns
+        -------
+        dict
+            A dictionary describing the state of the experiment containing the
+            following keys:
+            - "model": the state dict of the model (`state_dict()`).
+            - "optim": the state dict of the optimizer (`state_dict()`).
+            - "_epoch": the current epoch value from `self.properties`.
+
+        Examples
+        --------
+        >>> state = trainer.state
+        >>> torch.save(state, "checkpoint.pt")
+        """
+
         return {
-            "model": self.model.state_dict(),
-            "optim": self.optim.state_dict(),
-            "_epoch": self.properties["epoch"],
+            "model": self.model.state_dict(),       # Serialized model weights
+            "optim": self.optim.state_dict(),       # Serialized optimizer
+            "_epoch": self.properties["epoch"],     # Last/current epoch
         }
 
-    def set_state(self, state, strict=True):
+    def set_state(
+        self,
+        state: dict,
+        strict: bool = True,
+    ):
+        """
+        Restore the model and optimizer state from a checkpoint dictionary.
+
+        This method updates the internal state of the model and optimizer
+        using the provided `state` dictionary (from a checkpoint). It supports
+        restoring `torch.nn.Module` and `torch.optim.Optimizer` objects by
+        calling their `load_state_dict` methods. The training epoch is
+        also updated from the checkpoint metadata.
+
+        Parameters
+        ----------
+        state : dict
+            A dictionary obtained from a checkpoint file. It must include the
+            following keys: {'model', 'optim', '_epoch'}.
+        strict : bool, optional
+            Whether to strictly enforce that the keys in the state dictionary
+            match the keys returned by the moduleâ€™s `state_dict` function.
+            Default is True.
+
+        Examples
+        --------
+        >>> # Initialize an experiment (must be same as one saved to ckpt)
+        >>> experiment = MyCustomExperiment(*args, **kwargs)
+        >>> # Load the checkpoint
+        >>> checkpoint = torch.load("checkpoint.pt")
+        >>> # Load the state from the checkpoint into the experiment
+        >>> experiment.set_state(checkpoint, strict=False)
+        """
+
         for attr, state_dict in state.items():
+
+            # Skip metadata keys that start with underscore
             if not attr.startswith("_"):
+
+                # Get correct instance attr (e.g., self.model or self.optim)
                 x = getattr(self, attr)
+
+                # Restore model or optimizer state
                 if isinstance(x, nn.Module):
                     x.load_state_dict(state_dict, strict=strict)
                 elif isinstance(x, torch.optim.Optimizer):
@@ -305,21 +368,54 @@ class TrainExperiment(BaseExperiment):
                 else:
                     raise TypeError(f"Unsupported type {type(x)}")
 
+        # Restore epoch-related metadata
         self._checkpoint_epoch = state["_epoch"]
         self._epoch = state["_epoch"]
 
-    def checkpoint(self, tag=None):
+    def checkpoint(self, tag: str = 'last'):
+        """
+        Save the current state of the model to a checkpoint `.pt` file.
+
+        This method serializes the model's state to 3 main keys: {'model', 
+        'optim' and '_epoch'} representing the model state dict, the optimizer
+        state dict, and the current epoch number, respectively. The checkpoint
+        is written to the `checkpoints/` subdirectory of the experiment dir.
+        The filename is determined by an optional tag, defaulting to "last" if
+        not provided.
+
+        Parameters
+        ----------
+        tag : str, optional
+            A string label to use for naming the checkpoint file. If None,
+            the default name "last.pt" is used.
+
+        Raises
+        ------
+        OSError
+            If the checkpoint directory cannot be created or the file cannot be written.
+
+        Examples
+        --------
+        >>> # Set state of model to 5th epoch and save
+        >>> model._epoch = 5
+        >>> model.checkpoint("epoch_5")
+        # Saves to: {self.path}/checkpoints/epoch_5.pt
+        """
+
+        # Store the current epoch in the properties dictionary
         self.properties["epoch"] = self._epoch
 
+        # Define (and create) the checkpoint directory if it doesn't exist
         checkpoint_dir = self.path / "checkpoints"
         checkpoint_dir.mkdir(exist_ok=True)
 
-        tag = tag if tag is not None else "last"
-        logger.info(f"Checkpointing with tag:{tag} at epoch:{self._epoch}")
-
+        # Save the `state` property (with model, optim, and _epoch keys)
         with (checkpoint_dir / f"{tag}.pt").open("wb") as f:
             torch.save(self.state, f)
 
+        # Log the confirmation that the checkpoint has been saved
+        logger.info(f"Checkpointing with tag:{tag} at epoch:{self._epoch}")
+
     @property
     def checkpoints(self, as_paths=False) -> List[str]:
         checkpoints = list((self.path / "checkpoints").iterdir())

commit 45760d2f4664f8a8912d52ad60f0aeb89f1ca1d4
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 18:30:16 2025 -0400

    [DOCS] Document build_initialization()

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index a2280a1..f82d857 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -27,10 +27,6 @@ from .base import BaseExperiment
 from .util import absolute_import, eval_config
 
 
-import torch
-from torch.utils.data import DataLoader
-
-
 class TrainExperiment(BaseExperiment):
     """
     A training experiment to be subclassed for concrete implementations.
@@ -228,15 +224,66 @@ class TrainExperiment(BaseExperiment):
             self.metric_fns = eval_config(metrics_config)
 
     def build_initialization(self):
+        """
+        Load and apply an initialization state to the model from a file.
+
+        This method checks for an 'initialization' section in the configuration.
+        If present, it loads a serialized state dictionary from the specified
+        path and applies it to the model. The configuration may optionally skip
+        optimizer state loading and enforce strict key matching.
+
+        Raises
+        ------
+        FileNotFoundError
+            If the initialization file does not exist.
+        RuntimeError
+            If `torch.load` fails or the state cannot be applied to the model.
+
+        Examples
+        --------
+        # Inside the class
+        >>> self.config = {
+        ...     "initialization": {
+        ...         "path": "checkpoint.pth",
+        ...         "optim": False,
+        ...         "strict": True
+        ...     }
+        ... }
+        >>> self.build_initialization()
+
+        Notes
+        -----
+        I am not sure how much this method is used??
+        """
+
+        # Check if 'initialization' is provided in the config dict
         if "initialization" in self.config:
+
+            # Convert the configuration to a dictionary for easy access
             init_cfg = self.config["initialization"].to_dict()
+
+            # Get the file path from the config and create a Path object
             path = pathlib.Path(init_cfg["path"])
+
+            # Open the checkpoint file and load the state using torch.load
             with path.open("rb") as f:
+
+                # Make sure it's on the right device!
                 state = torch.load(f, map_location=self.device)
+
+            # Optionally remove the optimizer state from the checkpoint?
             if not init_cfg.get("optim", True):
+                # (If we only want to initialize model weights)
                 state.pop("optim", None)
+
+            # Determine key-matching behavior If True, keys of state dict must
+            # match model perfectly
             strict = init_cfg.get("strict", True)
+
+            # Apply the loaded state to the model
             self.set_state(state, strict=strict)
+
+            # Log the successful initialization
             logger.info(f"Loaded initialization state from: {path}")
 
     @property

commit 5040f577ae03e3254c192c9ad455be0b65b4a184
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:14:47 2025 -0400

    [FIX] Refactor validate_call to validate_arguments for deprecation warning

diff --git a/pylot/augmentation/variable.py b/pylot/augmentation/variable.py
index 8f8bf83..03a0433 100644
--- a/pylot/augmentation/variable.py
+++ b/pylot/augmentation/variable.py
@@ -7,7 +7,7 @@ import kornia as K
 import kornia.augmentation as KA
 from kornia.constants import BorderType
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 from .common import AugmentationBase2D, _as_single_val, _as2tuple
 
@@ -56,7 +56,7 @@ class RandomBrightnessContrast(AugmentationBase2D):
 
 
 class FilterBase(AugmentationBase2D):
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         kernel_size: Union[int, Tuple[int, int]],
diff --git a/pylot/callbacks/epoch.py b/pylot/callbacks/epoch.py
index 3b5485f..c0fd9bb 100644
--- a/pylot/callbacks/epoch.py
+++ b/pylot/callbacks/epoch.py
@@ -7,7 +7,7 @@ from fnmatch import fnmatch
 from typing import Literal, Union
 
 import numpy as np
-from pydantic import validate_arguments
+from pydantic import validate_call
 from tabulate import tabulate
 from loguru import logger
 
@@ -154,7 +154,7 @@ class ETA:
 
 
 class ModelCheckpoint:
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         experiment,
diff --git a/pylot/callbacks/stop.py b/pylot/callbacks/stop.py
index 8938e6f..2ad4453 100644
--- a/pylot/callbacks/stop.py
+++ b/pylot/callbacks/stop.py
@@ -4,11 +4,11 @@ from fnmatch import fnmatch
 
 from loguru import logger
 import numpy as np
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 
 class EarlyStopping:
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         experiment,
diff --git a/pylot/datasets/split.py b/pylot/datasets/split.py
index 8550af2..bd28ab4 100644
--- a/pylot/datasets/split.py
+++ b/pylot/datasets/split.py
@@ -1,10 +1,10 @@
 from typing import List, Tuple
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 from sklearn.model_selection import train_test_split
 
 
-@validate_arguments
+@validate_call
 def data_splits(
     values: List[str], splits: Tuple[float, float, float], seed: int
 ) -> Tuple[List[str], List[str], List[str]]:
diff --git a/pylot/datasets/thunder.py b/pylot/datasets/thunder.py
index 9c526dd..9cffa23 100644
--- a/pylot/datasets/thunder.py
+++ b/pylot/datasets/thunder.py
@@ -1,7 +1,7 @@
 import pathlib
 from typing import List
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 from torch.utils.data import Dataset
 
@@ -9,7 +9,7 @@ from ..util import ThunderLoader, ThunderReader, UniqueThunderReader
 
 
 class ThunderDataset(Dataset):
-    @validate_arguments
+    @validate_call
     def __init__(
         self, path: pathlib.Path, preload: bool = False, reuse_fp: bool = True
     ):
diff --git a/pylot/datasets/vision/imagenette.py b/pylot/datasets/vision/imagenette.py
index e506ec5..9869aed 100644
--- a/pylot/datasets/vision/imagenette.py
+++ b/pylot/datasets/vision/imagenette.py
@@ -3,7 +3,7 @@ from typing import Literal
 from ..lmdb import ImageFolderLMDB
 from ..path import DatapathMixin
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 from ...util.meta import store_attr
 
 __all__ = ["Imagenette", "Imagewoof"]
diff --git a/pylot/datasets/vision/nist.py b/pylot/datasets/vision/nist.py
index 0e332ef..9902170 100644
--- a/pylot/datasets/vision/nist.py
+++ b/pylot/datasets/vision/nist.py
@@ -7,7 +7,7 @@ from torchvision import transforms, datasets
 import torchvision.transforms as TF
 import torchvision.datasets as TD
 import PIL
-from pydantic import validate_arguments
+from pydantic import validate_call
 from sklearn.model_selection import train_test_split
 import numpy as np
 
@@ -99,7 +99,7 @@ def _fromVisionDataset(dataset_name):
     class_ = DATASET_CONSTRUCTOR[dataset_name]
 
     class DatasetWrapper(class_, DatapathMixin):
-        @validate_arguments
+        @validate_call
         def __init__(
             self,
             split: Literal["train", "val", "test"],
diff --git a/pylot/datasets/vision/notmnist.py b/pylot/datasets/vision/notmnist.py
index 0ef2706..1b5cab0 100644
--- a/pylot/datasets/vision/notmnist.py
+++ b/pylot/datasets/vision/notmnist.py
@@ -4,7 +4,7 @@ from concurrent.futures import ProcessPoolExecutor
 import torch
 import numpy as np
 from PIL import Image
-from pydantic import validate_arguments
+from pydantic import validate_call
 from sklearn.model_selection import train_test_split
 from torchvision.datasets import ImageFolder, MNIST
 
@@ -35,7 +35,7 @@ class notMNIST(MNIST):
 
     # loading code is a tad hacky to make the dataset MNIST-inherited
 
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         root: Path,
diff --git a/pylot/loss/segmentation.py b/pylot/loss/segmentation.py
index 2e49583..19fcf3a 100644
--- a/pylot/loss/segmentation.py
+++ b/pylot/loss/segmentation.py
@@ -3,7 +3,7 @@ from typing import Optional, Union, Literal
 import torch
 from torch import Tensor
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 from .util import _loss_module_from_func
 from ..util.more_functools import partial
@@ -11,7 +11,7 @@ from ..metrics.segmentation import soft_dice_score, soft_jaccard_score, pixel_ms
 from ..metrics.util import InputMode, Reduction
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def soft_dice_loss(
     y_pred: Tensor,
     y_true: Tensor,
@@ -49,7 +49,7 @@ def soft_dice_loss(
     return loss
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def soft_jaccard_loss(
     y_pred: Tensor,
     y_true: Tensor,
diff --git a/pylot/metrics/segmentation.py b/pylot/metrics/segmentation.py
index 5e59870..08afe4e 100644
--- a/pylot/metrics/segmentation.py
+++ b/pylot/metrics/segmentation.py
@@ -3,7 +3,7 @@ from typing import Optional, Union, List, Literal
 import torch
 from torch import Tensor
 import torch.nn.functional as F
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 
 from .util import (
@@ -15,7 +15,7 @@ from .util import (
 )
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def pixel_accuracy(
     y_pred: Tensor, y_true: Tensor, mode: InputMode = "auto", from_logits: bool = False,
 ) -> Tensor:
@@ -57,7 +57,7 @@ def pixel_mse(
     #     return F.mse_loss(y_pred, y_true, reduction=reduction)
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def soft_dice_score(
     y_pred: Tensor,
     y_true: Tensor,
@@ -93,7 +93,7 @@ def soft_dice_score(
     )
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def soft_jaccard_score(
     y_pred: Tensor,
     y_true: Tensor,
@@ -131,7 +131,7 @@ def soft_jaccard_score(
     )
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def dice_score(
     y_pred: Tensor,
     y_true: Tensor,
@@ -249,7 +249,7 @@ def IoU(
 
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def mIoU(
     y_pred: Tensor,
     y_true: Tensor,
@@ -272,7 +272,7 @@ def mIoU(
     return iou
 
 
-# @validate_arguments(config=dict(arbitrary_types_allowed=True))
+# @validate_call(config=dict(arbitrary_types_allowed=True))
 # def mIoU(
 #     y_pred: Tensor,
 #     y_true: Tensor,
diff --git a/pylot/metrics/util.py b/pylot/metrics/util.py
index d84de3b..4771101 100644
--- a/pylot/metrics/util.py
+++ b/pylot/metrics/util.py
@@ -2,7 +2,7 @@ from enum import Enum
 from typing import Literal, Optional, Tuple, Union
 
 import einops as E
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 import torch
 import torch.nn.functional as F
@@ -21,7 +21,7 @@ def hard_max(x: Tensor):
     return F.one_hot(torch.argmax(x, dim=1), num_classes=x.shape[1]).permute(order)
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def _infer_mode(y_pred: Tensor, y_true: Tensor,) -> InputMode:
     batch_size, num_classes = y_pred.shape[:2]
 
@@ -31,7 +31,7 @@ def _infer_mode(y_pred: Tensor, y_true: Tensor,) -> InputMode:
         return "multiclass"
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def _inputs_as_onehot(
     y_pred: Tensor,
     y_true: Tensor,
@@ -86,7 +86,7 @@ def _inputs_as_onehot(
     return y_pred.float(), y_true.float()
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def _inputs_as_longlabels(
     y_pred: Tensor,
     y_true: Tensor,
@@ -128,7 +128,7 @@ def _inputs_as_longlabels(
     return y_pred, y_true.long()
 
 
-@validate_arguments(config=dict(arbitrary_types_allowed=True))
+@validate_call(config=dict(arbitrary_types_allowed=True))
 def _metric_reduction(
     loss: Tensor,
     reduction: Reduction = "mean",
diff --git a/pylot/nn/batch_renorm.py b/pylot/nn/batch_renorm.py
index f206c1e..e039d05 100644
--- a/pylot/nn/batch_renorm.py
+++ b/pylot/nn/batch_renorm.py
@@ -2,7 +2,7 @@
 # https://github.com/ludvb/batchrenorm/blob/master/batchrenorm/batchrenorm.py
 # https://github.com/mf1024/Batch-Renormalization-PyTorch/blob/master/batch_renormalization.py
 import torch
-from pydantic import validate_arguments
+from pydantic import validate_call
 import einops as E
 
 
@@ -10,7 +10,7 @@ __all__ = ["BatchRenorm1d", "BatchRenorm2d", "BatchRenorm3d"]
 
 
 class BatchRenorm(torch.jit.ScriptModule):
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         num_features: int,
diff --git a/pylot/nn/ema.py b/pylot/nn/ema.py
index 55642a0..4980c89 100644
--- a/pylot/nn/ema.py
+++ b/pylot/nn/ema.py
@@ -9,7 +9,7 @@ import copy
 import math
 from typing import Optional
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 import torch
 from torch import nn
@@ -34,7 +34,7 @@ class ModelEMA(nn.Module):
     GPU assignment and distributed training wrappers.
     """
 
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         model,
@@ -86,7 +86,7 @@ class ModelEMA(nn.Module):
 # Unlike ModelEMA which is intended for aside tracking, this is a full wrapper that uses EMA
 # in eval mode
 class EMAWrapper(nn.Module):
-    @validate_arguments
+    @validate_call
     def __init__(
         self,
         model,
diff --git a/pylot/nn/norm.py b/pylot/nn/norm.py
index 279a90c..f17f496 100644
--- a/pylot/nn/norm.py
+++ b/pylot/nn/norm.py
@@ -2,7 +2,7 @@ from typing import Literal, Tuple, Union
 
 import einops as E
 import torch
-from pydantic import validate_arguments
+from pydantic import validate_call
 from pylot.nn import batch_renorm
 from torch import nn
 
diff --git a/pylot/nn/residual.py b/pylot/nn/residual.py
index da63d18..bdf31bd 100644
--- a/pylot/nn/residual.py
+++ b/pylot/nn/residual.py
@@ -1,10 +1,10 @@
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 from torch import nn
 
 
 class ConvResidual(nn.Module):
-    @validate_arguments
+    @validate_call
     def __init__(
         self, module, in_channels: int, out_channels: int,
     ):
diff --git a/pylot/util/hash.py b/pylot/util/hash.py
index b75c0bd..a82b7f7 100644
--- a/pylot/util/hash.py
+++ b/pylot/util/hash.py
@@ -1,7 +1,7 @@
 import copy
 import hashlib
 import json
-from pydantic import validate_arguments
+from pydantic import validate_call
 import pathlib
 import zlib
 import xxhash
@@ -39,7 +39,7 @@ def json_digest(data) -> str:
     return h
 
 
-@validate_arguments
+@validate_call
 def file_crc(path: pathlib.Path, chunksize=DEFAULT_CHUNKSIZE) -> str:
     crc = 0
     with open(path, "rb") as f:
@@ -52,7 +52,7 @@ def file_crc(path: pathlib.Path, chunksize=DEFAULT_CHUNKSIZE) -> str:
     return f"{crc:x}"
 
 
-@validate_arguments
+@validate_call
 def file_digest(path: pathlib.Path, chunksize=DEFAULT_CHUNKSIZE) -> str:
     # Use xxhash as it's substantially faster than md5
     # See https://github.com/Cyan4973/xxHash
@@ -66,7 +66,7 @@ def file_digest(path: pathlib.Path, chunksize=DEFAULT_CHUNKSIZE) -> str:
     return h.hexdigest()
 
 
-@validate_arguments
+@validate_call
 def fast_file_digest(path: pathlib.Path) -> str:
     import imohash
 
diff --git a/pylot/util/thunder.py b/pylot/util/thunder.py
index acd875e..efff980 100644
--- a/pylot/util/thunder.py
+++ b/pylot/util/thunder.py
@@ -5,7 +5,7 @@ from typing import Dict, Iterable
 
 import lmdb
 from lmdbm import Lmdb
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 from pylot.util import autopackb, autounpackb
 
@@ -25,7 +25,7 @@ class ThunderDB(Lmdb):
 
 
 class ThunderDict(collections.abc.MutableMapping):
-    @validate_arguments
+    @validate_call
     def __init__(self, path: pathlib.Path):
         self.path = path
 
@@ -68,7 +68,7 @@ class ThunderDict(collections.abc.MutableMapping):
 
 
 class ThunderReader(collections.abc.Mapping):
-    @validate_arguments
+    @validate_call
     def __init__(self, path: pathlib.Path):
         self.path = path
         self._env = None
@@ -158,7 +158,7 @@ class ThunderLoader(collections.abc.Mapping):
 
     _loaded: Dict[pathlib.Path, "ThunderLoader"] = {}
 
-    @validate_arguments
+    @validate_call
     def __init__(self, path: pathlib.Path):
         self.path = path
         if path not in self._loaded:
diff --git a/pylot/util/validation.py b/pylot/util/validation.py
index 060f3a4..80b320f 100644
--- a/pylot/util/validation.py
+++ b/pylot/util/validation.py
@@ -1,7 +1,7 @@
 import json
 from typing import Dict, List, Type, Union
 
-from pydantic import validate_arguments
+from pydantic import validate_call
 
 JSONType = Type[
     Union[int, float, str, bool, None, Dict[str, "JSONType"], List["JSONType"]]
@@ -10,7 +10,7 @@ JSONType = Type[
 # This decorator is necessary because decorating directly
 # results in the class not being a valid type
 def validate_arguments_init(class_):
-    class_.__init__ = validate_arguments(class_.__init__)
+    class_.__init__ = validate_call(class_.__init__)
     return class_
 
 

commit ede21ef2666bbe340da2fbb293acdcbf9791cc42
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:13:32 2025 -0400

    [CHORE] Update gitignore

diff --git a/.gitignore b/.gitignore
index 26bd8a0..ad208b2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -103,3 +103,6 @@ venv.bak/
 # mypy
 .mypy_cache/
 _old
+experiments/*
+pylot_root/*
+sandbox/*
\ No newline at end of file

commit f2d76f6a2b2f065de64850bb60908165e279f814
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:12:46 2025 -0400

    [WIP] Stop throwing env errors

diff --git a/pylot/util/libcheck.py b/pylot/util/libcheck.py
index 28af01d..76ac8a0 100644
--- a/pylot/util/libcheck.py
+++ b/pylot/util/libcheck.py
@@ -45,18 +45,18 @@ def check_environment():
     except ImportError:
         from warnings import warn
 
-    if not check_numpy_mkl():
-        warn("Intel MKL extensions not available for NumPy")
-    if not check_scipy_mkl():
-        warn("Intel MKL extensions not available for SciPy")
-    if not check_libjpeg_turbo():
-        warn("libjpeg_turbo not enabled for Pillow")
-    if not check_pillow_simd():
-        warn("Using slow Pillow instead of Pillow-SIMD")
-    if not check_torch_cuda():
-        warn("PyTorch cannot find a valid GPU device, check CUDA_VISIBLE_DEVICES")
-    if not check_torch_cudnn_benchmark():
-        warn("cuDNN autotuner not enabled, set  torch.backends.cudnn.benchmark = True")
+    #if not check_numpy_mkl():
+    #    warn("Intel MKL extensions not available for NumPy")
+    #if not check_scipy_mkl():
+    #    warn("Intel MKL extensions not available for SciPy")
+    #if not check_libjpeg_turbo():
+    #    warn("libjpeg_turbo not enabled for Pillow")
+    #if not check_pillow_simd():
+    #    warn("Using slow Pillow instead of Pillow-SIMD")
+    #if not check_torch_cuda():
+    #    warn("PyTorch cannot find a valid GPU device, check CUDA_VISIBLE_DEVICES")
+    #if not check_torch_cudnn_benchmark():
+    #    warn("cuDNN autotuner not enabled, set  torch.backends.cudnn.benchmark = True")
 
 
 if __name__ == "__main__":

commit 61eaecf866cff433c5d72d45c29ddcc73d22f0e7
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:12:20 2025 -0400

    [FEAT] Introduce remove_redundant_columns

diff --git a/pylot/pandas/convenience.py b/pylot/pandas/convenience.py
index 00ac09d..3857cad 100644
--- a/pylot/pandas/convenience.py
+++ b/pylot/pandas/convenience.py
@@ -1,3 +1,15 @@
+
+__all__ = [
+    'groupby_mode_nonum',
+    'groupby_and_take_best',
+    'to_categories',
+    'ensure_hashable',
+    'broadcast_categories',
+    'set_value_to_column',
+    'concat_with_attrs',
+    'remove_redundant_columns'
+]
+
 import itertools
 import json
 import operator
@@ -123,3 +135,26 @@ def concat_with_attrs(dfs, **concat_kws):
     concat_df = pd.concat(dfs, **concat_kws)
     concat_df.attrs.update(unique_attrs)
     return concat_df
+
+
+def remove_redundant_columns(
+    dataframe: pd.DataFrame,
+):
+    """
+    Remove all columns that have the same values for all entires
+    """
+
+    # Get a series of the unique counts indexed by column name
+    unique_counts = dataframe.nunique()
+
+    # Get all the non-constant columns
+    varying = unique_counts > 1
+
+    # True for the 'path' col
+    always_keep = dataframe.columns == "path"     
+
+    # Keep mask is or
+    keep_mask = varying | always_keep
+    dataframe = dataframe.loc[:, keep_mask]
+
+    return dataframe

commit 449e8e368206c81d57ae21ad8cf5b8690dab41d9
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:12:03 2025 -0400

    [FIX] Add arg and kwarg passthrough in the init

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index 0a358a3..a2280a1 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -12,6 +12,7 @@ import copy
 import pathlib
 from typing import List
 
+from tqdm import tqdm
 import torch
 import wandb
 from torch import nn
@@ -26,6 +27,10 @@ from .base import BaseExperiment
 from .util import absolute_import, eval_config
 
 
+import torch
+from torch.utils.data import DataLoader
+
+
 class TrainExperiment(BaseExperiment):
     """
     A training experiment to be subclassed for concrete implementations.
@@ -53,7 +58,7 @@ class TrainExperiment(BaseExperiment):
     optim : torch.optim
     """
 
-    def __init__(self, path):
+    def __init__(self, path, *args, **kwargs):
         """
         Initialize experiment.
 
@@ -77,7 +82,7 @@ class TrainExperiment(BaseExperiment):
         torch.backends.cudnn.benchmark = True
 
         # Initialize parent `BaseExperiment`
-        super().__init__(path)
+        super().__init__(path, *args, **kwargs)
 
         # Set cuda if available
         self.device = torch.device(
@@ -147,8 +152,8 @@ class TrainExperiment(BaseExperiment):
         assert self.config["dataloader.batch_size"] <= len(
             self.train_dataset
         ), (
-        f'Batch size = {self.config["dataloader.batch_size"]} should not be '
-        f'larger than dataset (len = {len(self.train_dataset)})'
+            f'Batch size = {self.config["dataloader.batch_size"]} should not be '
+            f'larger than dataset (len = {len(self.train_dataset)})'
         )
 
         # Set the validation dataloader attribute
@@ -279,6 +284,7 @@ class TrainExperiment(BaseExperiment):
     def load(self, tag=None):
         checkpoint_dir = self.path / "checkpoints"
         tag = tag if tag is not None else "last"
+
         with (checkpoint_dir / f"{tag}.pt").open("rb") as f:
             state = torch.load(
                 f=f,
@@ -287,16 +293,20 @@ class TrainExperiment(BaseExperiment):
             )
 
             self.set_state(state)
-            logger.info(
-                f"Loaded checkpoint with tag:{tag}. "
-                f"Last epoch:{self.properties['epoch']}"
-            )
+
+        logger.info(
+            f"Loaded checkpoint with tag:{tag}. "
+            f"Last epoch:{self.properties['epoch']}"
+        )
 
         return self
 
     def to_device(self):
         self.model = to_device(
-            self.model, self.device, self.config.get("train.channels_last", False)
+            self.model, self.device, self.config.get(
+                "train.channels_last",
+                False
+            )
         )
 
     def run_callbacks(self, callback_group, **kwargs):
@@ -358,8 +368,14 @@ class TrainExperiment(BaseExperiment):
         meters = MeterDict()
 
         with torch.set_grad_enabled(grad_enabled):
+
+            if __debug__:
+                iterator = tqdm(enumerate(dl), total=len(dl), desc=phase)
+            else:
+                iterator = enumerate(dl)
+ 
             # with torch.inference_mode(not grad_enabled):
-            for batch_idx, batch in enumerate(dl):
+            for batch_idx, batch in iterator:
                 outputs = self.run_step(
                     batch_idx,
                     batch,
@@ -387,7 +403,6 @@ class TrainExperiment(BaseExperiment):
             }
 
         if self.config.get('wandb.track_it', False):
-            logger.info('LOGGING TO WANDB')
             logger.info(f"\n{wandb_metrics}")
             wandb.log(wandb_metrics)
     

commit d333cd02c6f88fc2e7734840a5b1deda1d1a6762
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:11:00 2025 -0400

    [CHORE] Add requirements.txt

diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..ebd6f31
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,45 @@
+albumentations==2.0.5
+box==0.1.5
+decoder==0.5
+device==0.1
+diskcache==5.6.3
+einops==0.8.1
+fastcore==1.8.1
+graphviz==0.20.3
+humanize==4.12.2
+imohash==1.1.0
+ipython==8.12.3
+ipywidgets==8.1.6
+jc==1.25.4
+Jinja2==3.1.6
+kornia==0.8.0
+lmdbm==0.0.6
+lz4==4.4.4
+matplotlib==3.10.1
+more_itertools==10.7.0
+msgpack_numpy==0.4.8
+msgpack_python==0.5.6
+numpy==2.2.5
+parse==1.20.2
+Pillow==11.2.1
+plotly==6.0.1
+psutil==6.1.1
+pyarrow==19.0.0
+pydantic==2.11.3
+pympler==1.1
+PyYAML==6.0.2
+rich==14.0.0
+s3fs==2025.3.2
+scikit_learn==1.6.1
+scipy==1.15.2
+seaborn==0.13.2
+stackprinter==0.2.12
+tabulate==0.9.0
+torchmetrics==1.7.1
+torchvision==0.22.0
+torchviz==0.0.3
+tqdm==4.67.1
+typer==0.15.2
+wandb==0.19.10
+xxhash==3.5.0
+zstd==1.5.6.7

commit 674ac339236eac142a9bc75ccb8d81bdfb376347
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:10:48 2025 -0400

    [FEAT] Add some more fun names to experiment nonce's

diff --git a/pylot/experiment/util.py b/pylot/experiment/util.py
index f8f178b..b9324f3 100644
--- a/pylot/experiment/util.py
+++ b/pylot/experiment/util.py
@@ -30,7 +30,8 @@ _ADJECTIVES: List[str] = [
     "best", "cool", "dark", "easy", "fine", "good", "high", "just",
     "kind", "lazy", "mean", "nice", "open", "pure", "rich", "soft",
     "true", "wild", "zero", "blue", "fast", "grey", "long", "pink",
-    "red", "slow"
+    "red", "slow", "silly", "brave", "fiery", "chill", "fun", "smart",
+    "witty", "happy", "bad", "old", "mad", 
 ]
 
 _NOUNS: List[str] = [
@@ -40,7 +41,9 @@ _NOUNS: List[str] = [
     "vapor", "whale", "xray", "yacht", "zebra", "bird", "cake",
     "door", "echo", "frog", "gate", "hill", "ink", "jar", "kite",
     "lamp", "moon", "nest", "owl", "pen", "quiz", "rose", "sun",
-    "top", "urn", "van", "web", "yak", "zip"
+    "top", "urn", "van", "web", "yak", "zip", "boat", "button",
+    "camel", "latte", "data", "year", "blob", "thingy", "donut",
+    "taco", "rock", "wolf", "nerd", "lemon", "sloth", "cow", "elk"
 ]
 
 
@@ -88,6 +91,7 @@ def eval_config(config):
         return eval_config(config.to_dict())
     if isinstance(config, list):
         return [eval_config(v) for v in config]
+
     for k, v in config.items():
         if isinstance(v, (dict, list)):
             config[k] = eval_config(v)

commit 1d28644a2fceb50eaddd61cfaf9ecdb85f1c90d7
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 17:10:31 2025 -0400

    [FEAT] Add more robust/flexible handling of different config objects and add an init subclass to wrap everything with loguru catches

diff --git a/pylot/experiment/base.py b/pylot/experiment/base.py
index 7b99f5f..a728ef8 100644
--- a/pylot/experiment/base.py
+++ b/pylot/experiment/base.py
@@ -28,6 +28,8 @@ __all__ = [
 
 # Standard library imports
 import pathlib
+import inspect
+import functools
 from abc import abstractmethod
 from typing import Union
 
@@ -39,7 +41,7 @@ from loguru import logger
 from .util import fix_seed, absolute_import, generate_tuid
 from ..util.metrics import MetricsDict
 from ..util.config import HDict, FHDict, ImmutableConfig, config_digest
-from ..util.ioutil import autosave
+from ..util.ioutil import autosave, ensure_logging
 from ..util.libcheck import check_environment
 from ..util.thunder import ThunderDict
 
@@ -86,7 +88,11 @@ class BaseExperiment:
         Mapping of callback groups to lists of initialized callbacks.
     """
 
-    def __init__(self, path: str):
+    def __init__(
+        self,
+        path: str,
+        logging_level: str = 'debug',
+    ):
         """
         Initialize an experiment from an existing experiment directory.
 
@@ -102,6 +108,13 @@ class BaseExperiment:
             path = pathlib.Path(path)
 
         self.path = path
+
+        # Make sure logging is set up and make first log confirming experiment
+        ensure_logging(
+            log_file_abspath=self.path / 'output.log',
+            level=logging_level,
+        )
+
         logger.info(f'Absolute path to experiment run: "{self.path}"')
 
         if not self.path.exists():
@@ -116,9 +129,6 @@ class BaseExperiment:
         self.config = ImmutableConfig.from_file(path / "config.yml")
 
         config_str = yaml.safe_dump(self.config._data, sort_keys=False)
-        logger.info(
-            f'Loaded config for experiment "{self.name}":\n{config_str}'
-        )
 
         # Initialize stores
         self.properties = FHDict(self.path / "properties.json")
@@ -147,7 +157,8 @@ class BaseExperiment:
     @classmethod
     def from_config(
         cls,
-        config: Union[dict, HDict]
+        config: Union[dict, HDict],
+        logging_level: str = 'debug',
     ) -> "BaseExperiment":
         """
         Create a new experiment directory from a configuration dictionary.
@@ -179,12 +190,25 @@ class BaseExperiment:
         `pylot_experiments`.
         """
 
+        if isinstance(config, dict):
+            pass
+
         # Convert HDict to dict if necessary
-        if isinstance(config, HDict):
+        elif isinstance(config, HDict):
             config = config.to_dict()
 
+        elif isinstance(config, (str, pathlib.Path)):
+            config = ImmutableConfig.from_file(config).to_dict()
+
+        else:
+            logger.error(
+                'Do not know how to handle config with passed config of type '
+                f'{type(config)}'
+            )
+
         # Make a default root for the experiment
         default_experiment_root = 'pylot_experiments'
+        print('config: ', config)
 
         # Make sure there's a valid root directory for the experiments
         if "log" not in config:
@@ -208,7 +232,7 @@ class BaseExperiment:
         digest = config_digest(config)
 
         # Generate the (unique) name for the experiment run.
-        experiment_unique_id = f"{created_timestamp}-{random_suffix}-{digest}"
+        experiment_unique_id = f"{created_timestamp}-{random_suffix}"#-{digest}"
         logger.info(
             f'Made unique identifier for experiment: "{experiment_unique_id}"'
         )
@@ -227,7 +251,12 @@ class BaseExperiment:
         autosave(metadata, experiment_dir / "metadata.json")
         autosave(config, experiment_dir / "config.yml")
 
-        return cls(str(experiment_dir.absolute()))
+        class_instance = cls(
+            path=str(experiment_dir.absolute()),
+            logging_level=logging_level,
+        )
+
+        return class_instance
 
     @property
     def metrics(self) -> MetricsDict:
@@ -251,7 +280,8 @@ class BaseExperiment:
         The `data` attribute of self.metrics returns the metrics dictionary
         """
 
-        logger.debug(f'Retrieving metrics for experiment: {self.name}')
+        if __debug__:
+            logger.debug(f'Retrieving metrics for experiment: {self.name}')
 
         return self.metricsd["metrics"]
 
@@ -330,3 +360,43 @@ class BaseExperiment:
             logger.info(
                 f'No callbacks configured for experiment run {self.name}'
             )
+
+    def __init_subclass__(cls, **kwargs):
+        """
+        Automatically wrap public, callable methods of subclasses with
+        logger.catch so that exceptions are logged (and optionally
+        reraised). Methods can opt out by setting __no_catch__ on them.
+        """
+        super().__init_subclass__(**kwargs)
+
+        for name, attr in list(cls.__dict__.items()):
+            # Skip private, magic, non-callables, or explicit opt-outs
+            if name.startswith("_") or not callable(attr):
+                continue
+            if getattr(attr, "__no_catch__", False):
+                continue
+
+            # Unwrap staticmethod/classmethod to the raw function
+            if isinstance(attr, classmethod):
+                func = attr.__func__
+                wrapped = classmethod(
+                    logger.catch(reraise=True)(
+                        functools.wraps(func)(func)
+                    )
+                )
+            elif isinstance(attr, staticmethod):
+                func = attr.__func__
+                wrapped = staticmethod(
+                    logger.catch(reraise=True)(
+                        functools.wraps(func)(func)
+                    )
+                )
+            elif inspect.isfunction(attr):
+                wrapped = logger.catch(reraise=True)(
+                    functools.wraps(attr)(attr)
+                )
+            else:
+                # leave other callables (e.g. descriptors) alone
+                continue
+
+            setattr(cls, name, wrapped)

commit 322fdb3148f37396bb5448d41efa6649b1b22c77
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun May 4 16:26:30 2025 -0400

    [DOCS] Add documentation to collect

diff --git a/pylot/analysis/collect.py b/pylot/analysis/collect.py
index bc3e1d9..c5dced9 100644
--- a/pylot/analysis/collect.py
+++ b/pylot/analysis/collect.py
@@ -5,7 +5,7 @@ import json
 import pathlib
 from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
 from fnmatch import fnmatch
-from typing import Optional, List, Union
+from typing import Optional, List, Union, Dict, Tuple
 
 import more_itertools
 import numpy as np
@@ -18,7 +18,9 @@ import pandas as pd
 # on pd.DataFrames
 from ..pandas import api
 from ..pandas.api import augment_from_attrs
-from ..pandas.convenience import ensure_hashable, to_categories, concat_with_attrs
+from ..pandas.convenience import (
+    ensure_hashable, to_categories, concat_with_attrs, remove_redundant_columns
+)
 from ..util import FileCache
 from ..util.config import HDict, keymap, valmap
 
@@ -392,36 +394,127 @@ class ResultsLoader:
         return full_df
 
     def load_aggregate(
-        self, config_df, metric_df, agg=None, metrics_groupby=("phase",)
-    ):
-        assert isinstance(config_df, pd.DataFrame)
-        assert isinstance(metric_df, pd.DataFrame)
+        self,
+        config_df: pd.DataFrame,
+        metric_df: pd.DataFrame,
+        agg: Optional[Dict[str, List[str]]] = None,
+        metrics_groupby: Tuple[str, ...] = ("phase",),
+        remove_redundant_cols: bool = False,
+    ) -> pd.DataFrame:
+        """
+        Aggregate metric records by experiment config and optional group keys.
 
+        Parameters
+        ----------
+        config_df : pd.DataFrame
+            DataFrame containing experiment configurations.
+        metric_df : pd.DataFrame
+            DataFrame with metric records to aggregate.
+        agg : dict of {str: list of str}, optional
+            Additional mapping from metric columns to aggregation functions.
+        metrics_groupby : tuple of str, optional
+            Extra column names in metric_df to group by (in addition to all
+            columns of config_df).
+        remove_redundant_cols : bool, optional
+            Remove all columns that have a constant value for all entries.
+
+        Returns
+        -------
+        pd.DataFrame
+            Aggregated DataFrame whose columns are the config columns,
+            any extra group keys, and the aggregated metrics named
+            '<aggfunc>_<original_column>'.
+        """
+
+        # Validate inputs
+        if not isinstance(config_df, pd.DataFrame):
+            msg = f"config_df must be a DataFrame, got {type(config_df)}"
+            logger.error(msg)
+            raise TypeError(msg)
+
+        if not isinstance(metric_df, pd.DataFrame):
+            msg = f"metric_df must be a DataFrame, got {type(metric_df)}"
+            logger.error(msg)
+            raise TypeError(msg)
+
+        # Identify columns
         config_cols = list(config_df.columns)
         metric_cols = list(set(metric_df.columns) - set(config_df.columns))
 
+        logger.debug(f"Config columns for grouping: {config_cols}")
+        logger.debug(f"Metric columns to aggregate: {metric_cols}")
+
         _agg_fns = collections.defaultdict(list)
 
+        # Default patterns: max for accuracies/scores, min for losses/errors
         DEFAULT_AGGS = {
             "max": ["*acc*", "*score*", "*epoch*"],
             "min": ["*loss*", "*err*"],
         }
 
+        # Calculate the aggregation for each default
         for agg_fn, patterns in DEFAULT_AGGS.items():
+
             for column in metric_cols:
+                # If col name matches one of the patterns, schedule it for agg
                 if any(fnmatch(column, p) for p in patterns):
                     _agg_fns[column].append(agg_fn)
+
+        # Incorporate any user-requested extra aggregations
         if agg is not None:
+
             for column, agg_fns in agg.items():
+                # Do not overwrite defaults; extend the list of functions
                 _agg_fns[column].extend(agg)
 
-        g = config_cols + list(metrics_groupby)
-        agg_df = metric_df.groupby(g, as_index=False, dropna=False, observed=True).agg(
+        grouping_cols = config_cols + list(metrics_groupby)
+
+        # TODO: Figure out why I was getting this error
+        if 'log_freq' in grouping_cols:
+            grouping_cols.remove('log_freq')
+
+        # Verify that all grouping columns exist in metric_df
+        missing = [c for c in grouping_cols if c not in metric_df.columns]
+
+        if missing:
+            msg = f"Missing grouping columns in metric_df: {missing}"
+            logger.error(msg)
+            raise KeyError(msg)
+
+        logger.debug(
+            f'Aggregating metrics_df dataframe by {grouping_cols}'
+        )
+
+        # Group the dataframe and apply aggregations
+        agg_df = metric_df.groupby(
+            by=grouping_cols,
+            as_index=False,
+            dropna=False,
+            observed=True
+        ).agg(
             _agg_fns
         )
-        agg_df.columns = [
-            col if agg == "" else f"{agg}_{col}" for col, agg in agg_df.columns.values
-        ]
+
+        new_columns = []
+        for i, (col, agg) in enumerate(agg_df.columns.values):
+            if agg == "":
+                new_columns.append(col)
+            else:
+                new_columns.append(f"{agg}_{col}")
+
+        agg_df.columns = new_columns
+
+        # Relocate phase to be the first column
+        phase_series = agg_df.pop("phase")
+        agg_df.insert(0, "phase", phase_series)
+
+        # Derive the name of the experiment from the path and make first column
+        name_series = agg_df["path"].astype(str).str.split('/').str[-1]
+        agg_df.insert(0, "name", name_series)
+
+        # Optionally remove redundant columns
+        if remove_redundant_cols:
+            agg_df = remove_redundant_columns(agg_df)
 
         return agg_df
 
@@ -429,12 +522,26 @@ class ResultsLoader:
         self,
         *paths,
         shorthand=True,
+        remove_redundant_cols: bool = False,
         **selector
     ):
+        """
+        Returns
+        -------
+        Tuple[pd.DataFrame]
+            A tuple of the following dataframes:
+            - configuration dataframe
+            - metrics dataframe
+            - aggregate dataframe
+        """
 
         dfc = self.load_configs(*paths, shorthand=shorthand,).select(**selector).copy()
         df = self.load_metrics(dfc)
-        dfa = self.load_aggregate(dfc, df)
+        dfa = self.load_aggregate(
+            config_df=dfc,
+            metric_df=df,
+            remove_redundant_cols=remove_redundant_cols
+        )
         return dfc, df, dfa
 
     def load_from_callable(self, config_df, load_fn, copy_cols=None, prefix="data"):

commit 265cfa47ba0387b41680c85496046e5e9b5ba722
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Fri May 2 12:00:03 2025 -0400

    [DOCS] Add documentation to autosave()

diff --git a/pylot/util/ioutil.py b/pylot/util/ioutil.py
index e900f53..95a65e1 100644
--- a/pylot/util/ioutil.py
+++ b/pylot/util/ioutil.py
@@ -1,5 +1,6 @@
 import gzip
 import io
+import sys
 import json
 import pathlib
 import pickle
@@ -552,15 +553,70 @@ def autoload(path: Union[str, pathlib.Path]) -> object:
 
 
 def autosave(obj, path: Union[str, pathlib.Path], parents=True) -> object:
+    """
+    Save an object to disk.
+
+    This function writes various Python objects (e.g., dicts, lists, np arrays,
+    pd DataFrames, torch.Tensors, etc.) to the specified `path` by
+    automatically selecting the appropriate serialization format based on the
+    file extension. It ensures parent directories exist (if `parents=True`),
+    supports compression for extensions like `.gz`, `.lz4`, and delegates the
+    actual write operation to format handlers registered in `_DEFAULTFORMAT`.
+
+    Parameters
+    ----------
+    obj : any
+        The Python object to save. Supported types include dicts, lists,
+        np arrays, pd DataFrames, torch.Tensors, etc.
+    path : str or pathlib.Path
+        Destination file path. The suffix (after the final dot) determines the
+        format (must be a key in `_DEFAULTFORMAT`, e.g., 'json', 'yml', 'pt',
+        'npy').
+    parents : bool, optional
+        If True, create parent directories before writing. Default is True
+
+    Examples
+    --------
+    >>> # Example 1: Save a Python dictionary as JSON:
+    >>> from pylot.util.ioutil import autosave
+    >>> config = {'lr': 0.001, 'batch_size': 16}
+    >>> autosave(config, 'experiments/run01/config.json')  # creates file
+
+    >>> # Example 2: Save a pandas DataFrame as CSV:
+    >>> import pandas as pd
+    >>> df = pd.DataFrame({'epoch': [1, 2], 'loss': [0.5, 0.4]})
+    >>> autosave(df, 'experiments/run01/metrics.csv')  # creates file
+
+    >>> # Example 3: Save a torch tensor with compression:
+    >>> import torch
+    >>> tensor = torch.randn(3, 3)
+    >>> autosave(tensor, 'experiments/run01/tensor.pt.lz4')  # LZ4-compressed
+    """
+
+    # Normalize path to pathlib.Path object
     if isinstance(path, str):
         path = pathlib.Path(path)
+
+    # Check if extension is supported
     ext = path.suffix.strip(".")
     if ext not in _DEFAULTFORMAT:
+        
+        msg = (
+            f'Extension {ext} is not supported for autosaving. Unable to save'
+            f'{path}'
+        )
+
+        logger.error(msg)
         raise InvalidExtensionError(ext)
+
+    # Make parent folders if requested
     if parents:
         path.parent.mkdir(exist_ok=True, parents=True)
+
+    # Encode compressed formats into bytes
     if ext.lower() in ("lz4", "zst", "gz"):
         obj = autoencode(obj, path.stem)
+
     return _DEFAULTFORMAT[ext].save(obj, path)
 
 
@@ -615,30 +671,53 @@ def is_jsonable(x):
 
 def ensure_logging(
     log_file_abspath: str = 'out.log',
-    level: str = 'INFO',
+    level: str = 'CRITICAL',
 ) -> None:
+    """
+    Parameters
+    ----------
+    log_file_abspath : str
+        Path to the logging file.
+    level : str
+        Level of debugging. Options are:
+        {TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL, `None`}. If set to None,
+        no logging takes place.
+    """
 
-    pylot_logger_exists = False
+    if level is not None:
+        pylot_logger_exists = False
+
+        for handler in logger._core.handlers.values():
+            sink = handler._sink
+            if isinstance(sink, FileSink):
+                if str(sink._path) == log_file_abspath:
+                    logger.info(
+                        f'Logger sink already exists at {log_file_abspath}'
+                    )
+
+                    pylot_logger_exists = True
+                    break
+        
+        if not pylot_logger_exists:
+            level = level.upper()
+
+            try:
+                logger.remove(0)
+            except:
+                pass
+
+            # Add back stderr at INFO or higher
+            logger.add(sys.stderr, level="INFO")
+
+            logger.add(
+                log_file_abspath,
+                level=level,
+                backtrace=True,
+                diagnose=True,
+            )
 
-    for handler in logger._core.handlers.values():
-        sink = handler._sink
-        if isinstance(sink, FileSink):
-            if str(sink._path) == log_file_abspath:
-                logger.info(
-                    f'Logger sink already exists at {log_file_abspath}'
-                )
+            logger.info(
+                f'Logger sink initialized to {log_file_abspath}',
+            )
 
-                pylot_logger_exists = True
-                break
-    
-    if not pylot_logger_exists:
-        level = level.upper()
-        logger.add(
-            log_file_abspath,
-            level=level,
-            backtrace=True,
-            diagnose=True,
-        )
-        logger.info(
-            f'Logger sink initialized to {log_file_abspath}',
-        )
+            logger.critical(f'Logging level: {level}')

commit 31be5effc6b327029c3805e57e85a4e052eb050e
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 18:47:49 2025 -0400

    [FEAT] Introduce ensure_logging() helper

diff --git a/pylot/util/ioutil.py b/pylot/util/ioutil.py
index 6351d1a..e900f53 100644
--- a/pylot/util/ioutil.py
+++ b/pylot/util/ioutil.py
@@ -62,6 +62,7 @@ __all__ = [
     "autohash",
     "inplace_edit",
     "is_jsonable",
+    "ensure_logging",
 ]
 
 
@@ -610,3 +611,34 @@ def is_jsonable(x):
         return True
     except (TypeError, OverflowError):
         return False
+
+
+def ensure_logging(
+    log_file_abspath: str = 'out.log',
+    level: str = 'INFO',
+) -> None:
+
+    pylot_logger_exists = False
+
+    for handler in logger._core.handlers.values():
+        sink = handler._sink
+        if isinstance(sink, FileSink):
+            if str(sink._path) == log_file_abspath:
+                logger.info(
+                    f'Logger sink already exists at {log_file_abspath}'
+                )
+
+                pylot_logger_exists = True
+                break
+    
+    if not pylot_logger_exists:
+        level = level.upper()
+        logger.add(
+            log_file_abspath,
+            level=level,
+            backtrace=True,
+            diagnose=True,
+        )
+        logger.info(
+            f'Logger sink initialized to {log_file_abspath}',
+        )

commit 47e0be28c65a14a0dd0399cae4aec538e50a5b25
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 18:47:08 2025 -0400

    [DOCS] Add public API documentation with __all__

diff --git a/pylot/util/ioutil.py b/pylot/util/ioutil.py
index bdc40f8..6351d1a 100644
--- a/pylot/util/ioutil.py
+++ b/pylot/util/ioutil.py
@@ -21,12 +21,49 @@ import scipy.io
 import xxhash
 import yaml
 import zstd
+from loguru import logger
+from loguru._logger import FileSink
 
 import pandas as pd
 import torch
 
 m.patch()
 
+__all__ = [
+    "FileExtensionError",
+    "FileFormat",
+    "NpyFormat",
+    "NpzFormat",
+    "PtFormat",
+    "YamlFormat",
+    "NumpyJSONEncoder",
+    "JsonFormat",
+    "JsonlFormat",
+    "CsvFormat",
+    "ParquetFormat",
+    "FeatherFormat",
+    "PickleFormat",
+    "BaseImageFormat",
+    "JPGFormat",
+    "PNGFormat",
+    "GzipFormat",
+    "LZ4Format",
+    "ZstdFormat",
+    "MsgpackFormat",
+    "PlaintextFormat",
+    "MatFormat",
+    "InvalidExtensionError",
+    "autoencode",
+    "autodecode",
+    "autoload",
+    "autosave",
+    "autopackb",
+    "autounpackb",
+    "autohash",
+    "inplace_edit",
+    "is_jsonable",
+]
+
 
 class FileExtensionError(Exception):
     pass
@@ -501,7 +538,9 @@ def autoload(path: Union[str, pathlib.Path]) -> object:
     if isinstance(path, str):
         path = pathlib.Path(path)
     if not path.exists():
-        raise FileNotFoundError(f"No such file {str(path)}")
+        error_message = f"Experiment not found in {path}"
+        logger.error(error_message)
+        raise FileNotFoundError(error_message)
 
     ext = path.suffix.strip(".")
     if ext not in _DEFAULTFORMAT:

commit b68f1bcd8a04dfdcaf89c7679b1261e14f68292b
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 18:46:19 2025 -0400

    [REFACTOR] Generate fun human-readable name in favor of random characters for unique identifier

diff --git a/pylot/experiment/util.py b/pylot/experiment/util.py
index 4c136c0..f8f178b 100644
--- a/pylot/experiment/util.py
+++ b/pylot/experiment/util.py
@@ -5,7 +5,8 @@ import random
 import time
 import importlib
 import pathlib
-from typing import Tuple, Dict
+import random
+from typing import Tuple, Dict, List
 
 from ..util.config import HDict, Config
 from ..util.ioutil import autoload
@@ -22,12 +23,52 @@ def fix_seed(seed):
     torch.manual_seed(seed)
 
 
+_ADJECTIVES: List[str] = [
+    "brisk", "calm", "damp", "eager", "faint", "glad", "huge", "icy",
+    "jolly", "keen", "loud", "mild", "neat", "odd", "pale", "quick",
+    "raw", "shy", "tidy", "vast", "warm", "young", "zany", "able",
+    "best", "cool", "dark", "easy", "fine", "good", "high", "just",
+    "kind", "lazy", "mean", "nice", "open", "pure", "rich", "soft",
+    "true", "wild", "zero", "blue", "fast", "grey", "long", "pink",
+    "red", "slow"
+]
+
+_NOUNS: List[str] = [
+    "apple", "beach", "cloud", "dream", "eagle", "flame", "glove",
+    "heart", "inlet", "jelly", "knife", "leaf", "mango", "night",
+    "ocean", "plant", "queen", "river", "stone", "tree", "unity",
+    "vapor", "whale", "xray", "yacht", "zebra", "bird", "cake",
+    "door", "echo", "frog", "gate", "hill", "ink", "jar", "kite",
+    "lamp", "moon", "nest", "owl", "pen", "quiz", "rose", "sun",
+    "top", "urn", "van", "web", "yak", "zip"
+]
+
+
+def generate_fun_name() -> str:
+    """
+    Generate a random run name composed of an adjective and a noun.
+
+    Returns
+    -------
+    str
+        A two-word name in the form "adjective-noun", randomly picked
+        from internal word lists. Each word is at most 5 characters.
+    """
+
+    adj = random.choice(_ADJECTIVES)
+    noun = random.choice(_NOUNS)
+    return f"{adj}-{noun}"
+
+
 def generate_tuid(nonce_length: int = 4) -> Tuple[str, int]:
-    rng = np.random.default_rng(time.time_ns())
+    """
+    Generate time-based unique ID for experiment run directories.
+    """
+
     now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
-    char_options = list(string.ascii_uppercase + string.digits)
-    nonce = "".join(rng.choice(char_options, size=nonce_length))
-    return now, nonce
+    nonce = generate_fun_name()
+
+    return now, nonce.upper()
 
 
 def absolute_import(reference):

commit bbc1a01105b52a484d8930d89b90c4387e000e49
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 16:02:43 2025 -0400

    [FIX] Dictionary flattening was not compatable with boolean entries

diff --git a/pylot/util/config.py b/pylot/util/config.py
index 1f077f4..4142a2d 100644
--- a/pylot/util/config.py
+++ b/pylot/util/config.py
@@ -43,8 +43,14 @@ def flatten(nested_dict, sep=None) -> Dict[Key, Any]:
                 flat_dict[(k,) + k2] = v2
         else:
             flat_dict[(k,)] = v
+
     if sep is not None:
-        flat_dict = keymap(lambda k: sep.join(k), flat_dict)
+        # Flatten and map bools to strings
+        flat_dict = {
+            sep.join(map(str, key_tuple)): val
+            for key_tuple, val in flat_dict.items()
+        }
+
     return flat_dict
 
 

commit 97b5143e89b2f3e98627c59ad5ab57417b57f15d
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 16:02:05 2025 -0400

    [DOCS] Add note to TrainExperiment

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index babe73d..0a358a3 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -98,6 +98,12 @@ class TrainExperiment(BaseExperiment):
 
         Reads dataset configuration from the `data` field of `self.config`
         and initializes datasets for training and validation splits.
+
+        Notes
+        -----
+        If using this method off-the-shelf, the dataset specified by the `data`
+        key must accept `train` and `val` arguments in its constructor to
+        specify training and validation splits.
         """
 
         # Make sure config file has 'data' field

commit 992cd2e6cdd88494945630ddb05d69891472a405
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 16:01:15 2025 -0400

    [DOCS] Begin documenting analysis/collect.py

diff --git a/pylot/analysis/collect.py b/pylot/analysis/collect.py
index a4b8fc9..bc3e1d9 100644
--- a/pylot/analysis/collect.py
+++ b/pylot/analysis/collect.py
@@ -5,11 +5,12 @@ import json
 import pathlib
 from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
 from fnmatch import fnmatch
-from typing import Optional, List
+from typing import Optional, List, Union
 
 import more_itertools
 import numpy as np
 from tqdm.auto import tqdm
+from loguru import logger
 
 import pandas as pd
 
@@ -53,67 +54,187 @@ def shorthand_columns(df):
 
 
 class ResultsLoader:
-    def __init__(self, cache_file: Optional[str] = None, num_workers: int = 8):
+    """
+    Loader utility for experimental results.
+
+    Attributes
+    ----------
+    _cache : FileCache
+        Cached file handler for fast repeated access.
+    _num_workers : int
+        Number of workers for parallel loading.
+    """
+
+    def __init__(
+        self,
+        cache_file: Optional[str] = None,
+        num_workers: int = 8
+    ):
+        """
+        Initialize `ResultsLoader`.
+
+        Parameters
+        ----------
+        cache_file : str or None, optional
+            Path to the disk cache file. If None, defaults to
+            "/tmp/{username}-results.diskcache".
+        num_workers : int, optional
+            Number of parallel workers for file loading.
+        """
+
+        # Determine the current user
         unixname = getpass.getuser()
-        cache_file = cache_file or f"/tmp/{unixname}-results.diskcache"
+        logger.info(
+            f'ResultsLoader is loading results for the unix user: {unixname}'
+        )
+
+        # Determine the 
+        if cache_file is None:
+            cache_file = f"/tmp/{unixname}-results.diskcache"
+            logger.debug(
+                'No results cache file specified to ResultsLoader constructor.'
+                f' Defaulting to cache file found at {cache_file}')
+
+        # Initialize disk cache and worker count
         self._cache = FileCache(cache_file)
         self._num_workers = num_workers
 
     def load_configs(
         self,
-        *paths,
-        shorthand=True,
-        properties=False,
-        metadata=False,
-        log=False,
-        callbacks=False,
-        categories=False,
-    ):
-        # ordered deduplication
+        *paths: Union[str, pathlib.Path],
+        shorthand: bool = True,
+        properties: bool = False,
+        metadata: bool = False,
+        log: bool = False,
+        callbacks: bool = False,
+        categories: bool = False,
+    ) -> pd.DataFrame:
+        """
+        Load and flatten a YAML configuration files from experiment folders.
+
+        This method loads the YAML configuration files associated with each
+        exoperimental run and flattens them so there is one entry per key.
+
+        Parameters
+        ----------
+        *paths : str or pathlib.Path
+            One or more base directories containing experiment run folders.
+        shorthand : bool, optional
+            Shorten verbose column names.
+        properties : bool, optional
+            Include `properties.json` contents as column.
+        metadata : bool, optional
+            Include `metadata.json` contents as column.
+        log : bool, optional
+            Keep or drop the `log` section in configs.
+        callbacks : bool, optional
+            Keep or drop the `callbacks` section.
+        categories : bool, optional
+            Convert string columns to pandas.Categorical.
+
+        Returns
+        -------
+        pandas.DataFrame
+            Flattened configuration records, one row per experiment.
+
+        Examples
+        --------
+        >>> # Initialize the loader with the default cache file
+        >>> loader = pylot.ResultsLoader()
+        >>> # Specify directory to folder containing all pylot experiments
+        >>> experiment_root = 'pylot_experiments'
+        >>> # Load the flattened config files
+        >>> df = loader.load_configs(path)
+        >>> df.keys()
+        Index(['epochs', 'model', 'in_channels', 'out_channels', 'ndim'])
+        """
+
+        # Deduplicate input paths
         paths = list(dict.fromkeys(paths))
-        assert all( isinstance(p, (str, pathlib.Path)) for p in paths)
 
+        for p in paths:
+            # Ensure all paths are valid
+            if not isinstance(p, (str, pathlib.Path)):
+
+                # Make error message
+                error_message = (
+                    f"Invalid experiment path: {p}: each entry must be a str "
+                    "or pathlib.Path"
+                )
+
+                # Throw errors
+                logger.error(error_message)
+                raise ValueError(error_message)
+
+        # Gather all immediate experiment subfolders
         folders = list(
             itertools.chain.from_iterable(
                 pathlib.Path(path).iterdir() for path in paths
             )
         )
 
+        # Load each config in parallel via cache
         configs = self._cache.gets(
-            (folder / "config.yml" for folder in folders), num_workers=self._num_workers
+            files=(folder / "config.yml" for folder in folders),
+            num_workers=self._num_workers
         )
 
         rows = []
-        for folder, cfg in tqdm(zip(folders, configs), leave=False, total=len(folders)):
+        for folder, cfg in tqdm(
+            zip(folders, configs),
+            leave=False,
+            total=len(folders)
+        ):
+
             if cfg is None:
-                continue
+                continue  # Skip missing configs
+
+            # Convert to nested dict with flatten suppport
             cfg = HDict(cfg)
+
+            # Optionally remove potential pollution
             if not log:
                 cfg.pop("log", None)
             if not callbacks:
                 cfg.pop("callbacks", None)
+
+            # Optionally attach metadata and properties json data
             if metadata:
-                cfg.update({"metadata": self._cache[folder / "metadata.json"]})
+                cfg.update(
+                    {"metadata": self._cache[folder / "metadata.json"]}
+                )
             if properties:
-                cfg.update({"properties": self._cache[folder / "properties.json"]})
+                cfg.update(
+                    {"properties": self._cache[folder / "properties.json"]}
+                )
 
+            # Flatten dict, then convert to tuple (for hashability)
             flat_cfg = valmap(list2tuple, cfg.flatten())
+
+            # Add entries for the experimental run folders
             flat_cfg["path"] = folder
 
+            # Clean up key names from config conventions
             flat_cfg = keymap(lambda x: x.replace("._class", ""), flat_cfg)
             flat_cfg = keymap(lambda x: x.replace("._fn", ""), flat_cfg)
 
+            # Add to the container
             rows.append(flat_cfg)
 
+        # Build DataFrame from list of dicts
         df = pd.DataFrame.from_records(rows)
 
+        # Ensure hashability
         ensure_hashable(df, inplace=True)
 
+        # Optionally shorten column names
         if shorthand:
             df = shorthand_columns(df)
 
+        # Optionally convert to categorical for memory/analysis
         if categories:
             df = to_categories(df, inplace=True, threshold=0.5)
+
         return df
 
     def load_sub_configs(
@@ -147,62 +268,111 @@ class ResultsLoader:
         df = df_sub.merge(config_df[copy_cols], on="path", how="left")
 
         return df
-    
+
     def load_metrics(
         self,
-        config_df,
-        file="metrics.jsonl",
-        prefix="log",
-        shorthand=True,
-        copy_cols=None,
-        path_key=None,
-        expand_attrs=False,
-        categories=False,
+        config_df: pd.DataFrame,
+        file: Union[str, List[str]] = "metrics.jsonl",
+        prefix: str = "log",
+        shorthand: bool = True,
+        copy_cols: Optional[List[str]] = None,
+        path_key: Optional[str] = None,
+        expand_attrs: bool = False,
+        categories: bool = False,
     ):
-        assert isinstance(config_df, pd.DataFrame)
 
-        log_dfs = []
-        if path_key is None:
-            path_key = "path"
-        if copy_cols is None:
-            copy_cols = config_df.columns.to_list()
+        # Ensure config_df is a pandas dataframe
+        if not isinstance(config_df, pd.DataFrame):
+
+            # Make the error message
+            error_message = (
+                f'config_df must be a pandas dataframe. Got {type(config_df)}'
+            )
+
+            # Throw errors
+            logger.error(error_message)
+            raise TypeError(error_message)
 
+        # Determine the name of the column that holds the experiment path
+        path_key = path_key or 'path'
+
+        # Decide which config columns to replicate on each metric row
+        copy_cols = copy_cols or config_df.columns.to_list()
+
+        # Extract all experiment folder paths
         folders = config_df[path_key].values
 
+        # Normalize file argument into a list of filenames
         files = [file] if isinstance(file, str) else file
         n_files = len(files)
+        logger.debug(
+            f"Expecting {n_files} metric files per experiment: {files}"
+        )
+
+        # Read metric files in parallel using the cache
         all_files = self._cache.gets(
             (folder / file for folder in folders for file in files),
             num_workers=self._num_workers,
         )
+
+        if not len(all_files) > 0:
+            error_message = f"No metric files found for patterns {files}"
+            logger.error(error_message)
+            raise FileNotFoundError(error_message)
+
+        log_dfs = []
+
+        # Build an iterator that repeats each config row for each file
         config_iter = more_itertools.repeat_each(config_df.iterrows(), n_files)
-        assert len(all_files)>0, f"No files found for {file}"
 
+        # Loop over each (config_row, log_df) pair
         for (_, row), log_df in tqdm(
-            zip(config_iter, all_files), total=len(config_df) * n_files, leave=False
+            zip(config_iter, all_files),
+            total=len(config_df) * n_files,
+            leave=False,
         ):
+
+            # Convert row -> dict and path -> pathlib.Path
             row = row.to_dict()
             path = pathlib.Path(row[path_key])
+
+            # Skip if this experiment has no metrics file
             if log_df is None:
+                logger.warning(f"Missing metrics file in {path}")
                 continue
+
+            # Prefix all metric column names (e.g. 'acc' -> 'log_acc')
             if prefix:
                 log_df.rename(
-                    columns={c: f"{prefix}_{c}" for c in log_df.columns}, inplace=True
+                    columns={c: f"{prefix}_{c}" for c in log_df.columns},
+                    inplace=True
                 )
+
+            # Copy metadata columns from config into each row of dataframe
             if len(copy_cols) > 0:
                 for col in copy_cols:
                     val = row[col]
+
+                    # If metadata is a tuple, repeat for each row as array
                     if isinstance(val, tuple):
-                        log_df[col] = np.array(itertools.repeat(val, len(log_df)))
+                        log_df[col] = np.array(
+                            itertools.repeat(val, len(log_df))
+                        )
                     else:
                         log_df[col] = val
+
+            # Optionally extract and attach additional attributes from metrics
             if expand_attrs:
                 log_df = augment_from_attrs(log_df, prefix=f"{prefix}_")
+
+            # Record the experiment path in the dataframe
             log_df["path"] = path
             log_dfs.append(log_df)
 
+        # Combine all experiment dataframe into one unified dataframe
         full_df = concat_with_attrs(log_dfs, ignore_index=True)
 
+        # Optionally simplify column names
         if shorthand:
             renames = {}
             for c in full_df.columns:
@@ -255,7 +425,12 @@ class ResultsLoader:
 
         return agg_df
 
-    def load_all(self, *paths, shorthand=True, **selector):
+    def load_all(
+        self,
+        *paths,
+        shorthand=True,
+        **selector
+    ):
 
         dfc = self.load_configs(*paths, shorthand=shorthand,).select(**selector).copy()
         df = self.load_metrics(dfc)

commit 1dd7f33adcbfe13022d12816bdc8bb0ccd65fb4c
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 14:23:10 2025 -0400

    [DOCS] Fix code block language in readme

diff --git a/README.md b/README.md
index 974ce2d..2b33aa5 100644
--- a/README.md
+++ b/README.md
@@ -59,7 +59,7 @@ log:
 
 ## 2 Define an Experiment
 
-```bash
+```python
 from pylot.experiment.base import TrainExperiment
 
 class MyExperiment(TrainExperiment):

commit be6440a3f43a0743d779c25ae9ead3fec6753976
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 14:22:13 2025 -0400

    [DOCS] Begin adding to readme.md

diff --git a/README.md b/README.md
index 40638a9..974ce2d 100644
--- a/README.md
+++ b/README.md
@@ -22,3 +22,65 @@ This library is designed as a supporting library to my other research efforts in
 - âœ… **Extensive Type Validation** - Type hints are included and _enforced_ thanks to pydantic.
 - ðŸ¼ **Pandas Extensions** - Pandas DataFrame API is extended to better handle result dataframes, with methods such as `.select`, `.drop_constant`, `.unique_per_col`  or the `UnixAccessor` functionality.
 - ðŸŽï¸ **Designed for Speed** - From careful I/O considerations to CUDA optimizations, pylot is painstakingly designed with efficiency and speed in mind.
+
+# Quick Start Guide
+
+## 1 Define a `config.yml`
+Common top-level keys:
+- `experiment`: General experiment details
+- `log`: Details about logging, the path for base experiments, ...
+- `data`: Information about the dataset
+
+A simple configuration file might look like:
+
+```yaml
+experiment:
+  epochs: 100
+
+model:
+    _class: neurite.pytorch.models.BasicUNet
+    ndim: 2
+    in_channels: 1
+    out_channels: 32
+
+data:
+    _class: my.torch.Dataset
+    path_to_data: 'dalcalab/data'
+
+dataloader:
+    batch_size: 8
+    num_workers: 2
+    pin_memory: False
+
+log:
+    root: "path/to/experiments/directory"
+    checkpoint_freq: 1
+```
+
+## 2 Define an Experiment
+
+```bash
+from pylot.experiment.base import TrainExperiment
+
+class MyExperiment(TrainExperiment):
+    """
+    An experiment!
+    """
+
+    def run(self):
+        """
+        Run training loop and log metrics.
+        """
+        print('starting training loop...')
+
+        # Iterate epochs for training
+        for epoch in range(self.config.get('experiment.epochs', 10)):
+            # Train, val, log metrics...
+            pass
+```
+
+# Core Modules
+- `pylot.experiment`: Classes for running experiments and training models.
+- `pylot.metrics`: Basic ML and CV metrics (accuracy, Dice, ...).
+- `pylot.analysis`: Statistical analysis and summary functions.
+- `pylot.pandas`: Convert results to pandas DataFrames.

commit 8614424e439e41a6d10471500b873fbc8021376a
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 13:45:12 2025 -0400

    [FIX] Wrong import and return type

diff --git a/pylot/experiment/base.py b/pylot/experiment/base.py
index bf2ce8f..7b99f5f 100644
--- a/pylot/experiment/base.py
+++ b/pylot/experiment/base.py
@@ -26,7 +26,6 @@ __all__ = [
     'BaseExperiment',
 ]
 
-
 # Standard library imports
 import pathlib
 from abc import abstractmethod
@@ -36,10 +35,6 @@ from typing import Union
 import yaml
 from loguru import logger
 
-from pylot.util.metrics import MetricsStore
-
-from pylot.util.metrics import MetricsStore
-
 # Local imports
 from .util import fix_seed, absolute_import, generate_tuid
 from ..util.metrics import MetricsDict
@@ -235,7 +230,7 @@ class BaseExperiment:
         return cls(str(experiment_dir.absolute()))
 
     @property
-    def metrics(self) -> MetricsStore:
+    def metrics(self) -> MetricsDict:
         """
         Retrieve the current run's metrics dictionary from the experiment
         run's `metrics.jsonl`.

commit a55e41d32651c36932fe9ecdb6acbce909ac30f6
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 13:42:26 2025 -0400

    [DOCS] Add documentation to pylot/experiment/base.py

diff --git a/pylot/experiment/base.py b/pylot/experiment/base.py
index 53e6196..bf2ce8f 100644
--- a/pylot/experiment/base.py
+++ b/pylot/experiment/base.py
@@ -1,11 +1,47 @@
-from abc import abstractmethod
+"""
+Base class and utilities for PyLot experiments.
+
+Classes
+-------
+BaseExperiment
+    Sets up experimental run, config loading, logging, metrics collection,
+    and callback management.
+
+Examples
+--------
+>>> # Define an experiment class with a concrete `run()` method.
+>>> class MyExp(BaseExperiment):
+...     def run(self):
+...         logger.info("Running MyExperiment")
+>>> # Make the configuration for it (can also be from an e.g. `.yml` file)
+>>> config = {'experiment': {'seed': 123}, 'log': {'root': './logs'}}
+>>> # Construct the experiment 
+>>> experiment = MyExperiment.from_config(config)
+>>> experiment.build_callbacks()
+>>> experiment.run()
+"""
+
+__all__ = [
+    'eval_callbacks',
+    'BaseExperiment',
+]
 
+
+# Standard library imports
 import pathlib
+from abc import abstractmethod
+from typing import Union
 
+# Third party imports 
 import yaml
+from loguru import logger
 
-from .util import fix_seed, absolute_import, generate_tuid
+from pylot.util.metrics import MetricsStore
+
+from pylot.util.metrics import MetricsStore
 
+# Local imports
+from .util import fix_seed, absolute_import, generate_tuid
 from ..util.metrics import MetricsDict
 from ..util.config import HDict, FHDict, ImmutableConfig, config_digest
 from ..util.ioutil import autosave
@@ -32,55 +68,196 @@ def eval_callbacks(all_callbacks, experiment):
 
 
 class BaseExperiment:
+    """
+    Base class for experiment setup and execution.
+
+    Attributes
+    ----------
+    path : pathlib.Path
+        Path to the directory containing the experiment run.
+    name : str
+        Name of the experiment, derived from the run directory stem.
+    config : ImmutableConfig
+        Configuration object loaded from `config.yml`.
+    properties : FHDict
+        Dictionary-like store for experiment properties saved to JSON.
+    metadata : FHDict
+        Dictionary-like store for metadata (creation time, digest).
+    metricsd : MetricsDict
+        Container for metric values, organized by name and group.
+    store : ThunderDict
+        Key-value store for intermediate or auxiliary data.
+    callbacks : dict
+        Mapping of callback groups to lists of initialized callbacks.
+    """
+
     def __init__(self, path: str):
         """
+        Initialize an experiment from an existing experiment directory.
+
+        Parameters
+        ----------
         path : str
-            Path to the configuration file (`.yml`) for the experiment.
+            Path to the experiment run directory containig a `config.yml`,
+            `metadata.json`, `properties.json`, etc... files.
         """
 
+        # Convert to pathlib if necessary
         if isinstance(path, str):
             path = pathlib.Path(path)
+
         self.path = path
-        assert path.exists()
+        logger.info(f'Absolute path to experiment run: "{self.path}"')
+
+        if not self.path.exists():
+            error_message = f'Path to experiment not found: "{self.path}"'
+            logger.error(error_message)
+            raise FileNotFoundError(error_message)
+
+        # Set derived instance attributes
         self.name = self.path.stem
 
-        
+        # Load the configuration file (defaults to 'config.yml')
         self.config = ImmutableConfig.from_file(path / "config.yml")
+
+        config_str = yaml.safe_dump(self.config._data, sort_keys=False)
+        logger.info(
+            f'Loaded config for experiment "{self.name}":\n{config_str}'
+        )
+
+        # Initialize stores
         self.properties = FHDict(self.path / "properties.json")
         self.metadata = FHDict(self.path / "metadata.json")
         self.metricsd = MetricsDict(self.path)
-
         self.store = ThunderDict(self.path / "store")
 
-        fix_seed(self.config.get("experiment.seed", 42))
+        # Set the global seed of the experiment for reproducibility
+        seed = self.config.get("experiment.seed", 42)
+        fix_seed(seed)
+        logger.info(f'Fixed global random seed to: {seed}')
+
+        # Check for optimized packages (e.g. numpy, scikit-learn, ...)
         check_environment()
 
+        # Record class & module for traceability. Useful for subclasses.
         self.properties["experiment.class"] = self.__class__.__name__
         self.properties["experiment.module"] = self.__class__.__module__
+        logger.debug("Recorded experiment class and module.")
 
+        # If additional log properties defined in config, update them
         if "log.properties" in self.config:
             self.properties.update(self.config["log.properties"])
+            logger.debug("Updated properties from config.log.properties.")
 
     @classmethod
-    def from_config(cls, config) -> "BaseExperiment":
+    def from_config(
+        cls,
+        config: Union[dict, HDict]
+    ) -> "BaseExperiment":
+        """
+        Create a new experiment directory from a configuration dictionary.
+
+        Parameters
+        ----------
+        config : dict or HDict
+            Collection of fields defining the configuration of an experiment.
+
+        Returns
+        -------
+        BaseExperiment
+            Experiment instance initialized within a new directory of the
+            current working directory.
+
+        Examples
+        --------
+        ### Defining a simple configuration file with a logging root
+        >>> config = {
+        ...     'experiment': {"seed": 99,},
+        ...     'log': {'root': 'my_experiment'}
+        ... }
+        >>> experiment = pylot.BaseExperiment.from_config(config)
+
+        Notes
+        -----
+        If the root directory for pylot experiments is not specified (by the
+        `log.root` key) in the config dict, experiment root defaults to
+        `pylot_experiments`.
+        """
+
+        # Convert HDict to dict if necessary
         if isinstance(config, HDict):
             config = config.to_dict()
 
-        root = pathlib.Path()
-        if "log" in config:
-            root = pathlib.Path(config["log"].get("root", "."))
-        create_time, nonce = generate_tuid()
+        # Make a default root for the experiment
+        default_experiment_root = 'pylot_experiments'
+
+        # Make sure there's a valid root directory for the experiments
+        if "log" not in config:
+            # Set the root
+            config["log"] = {"root": default_experiment_root}
+
+        if "root" not in config["log"]:
+            config['log']['root'] = default_experiment_root
+
+        # Determine base folder for experiments        
+        experiments_root = pathlib.Path(config['log']['root'])
+
+        # Log the root path of the experiments
+        logger.info(
+            'Root directory for experiments located at: '
+            f'"{experiments_root}"'
+        )
+
+        # Generate names for unique identifier of experimental run
+        created_timestamp, random_suffix = generate_tuid()
         digest = config_digest(config)
-        uuid = f"{create_time}-{nonce}-{digest}"
-        path = root / uuid
-        metadata = {"create_time": create_time, "nonce": nonce, "digest": digest}
-        autosave(metadata, path / "metadata.json")
 
-        autosave(config, path / "config.yml")
-        return cls(str(path.absolute()))
+        # Generate the (unique) name for the experiment run.
+        experiment_unique_id = f"{created_timestamp}-{random_suffix}-{digest}"
+        logger.info(
+            f'Made unique identifier for experiment: "{experiment_unique_id}"'
+        )
+
+        # Construct the path to the experiment run
+        experiment_dir = experiments_root / experiment_unique_id
+
+        # TODO: Determine wherelse `nonce` and `create_time` are used. Change.
+        metadata = {
+            "create_time": created_timestamp,
+            "nonce": random_suffix, 
+            "digest": digest
+        }
+
+        # TODO: document `autosave`
+        autosave(metadata, experiment_dir / "metadata.json")
+        autosave(config, experiment_dir / "config.yml")
+
+        return cls(str(experiment_dir.absolute()))
 
     @property
-    def metrics(self):
+    def metrics(self) -> MetricsStore:
+        """
+        Retrieve the current run's metrics dictionary from the experiment
+        run's `metrics.jsonl`.
+
+        Returns
+        -------
+        dict
+            Metrics dictionary containing logged metrics for the current
+            experiment run.
+
+        Examples
+        --------
+        >>> exp = BaseExperiment.from_config({'experiment': {'seed': 1}})
+        >>> metrics = exp.metrics  # initially empty dict
+
+        Notes
+        -----
+        The `data` attribute of self.metrics returns the metrics dictionary
+        """
+
+        logger.debug(f'Retrieving metrics for experiment: {self.name}')
+
         return self.metricsd["metrics"]
 
     def __hash__(self):
@@ -88,7 +265,29 @@ class BaseExperiment:
 
     @abstractmethod
     def run(self):
-        pass
+        """
+        Run the experiment.
+
+        This method should be overridden by subclasses (concrete classes) to
+        define training/evauluation routines.
+
+        Examples
+        --------
+        >>> class MyExp(BaseExperiment):
+        ...     def run(self):
+        ...         logger.info("Doing stuff, but also things!")
+        """
+
+        # Make the error message for logger and NotImplementedError
+        error_message = (
+            "BaseExperiment.run() called directly. This is an abstract method "
+            "that must be overridden in a subclass"
+        )
+
+        # Log the error
+        logger.error(error_message)
+
+        raise NotImplementedError(error_message)
 
     def __repr__(self):
         return f'{self.__class__.__name__}("{str(self.path)}")'
@@ -99,6 +298,40 @@ class BaseExperiment:
         return s
 
     def build_callbacks(self):
+        """
+        Instantiate and attach callbacks defined in the config.
+
+        Populates the `self.callbacks` dict with lists of callback
+        instances grouped by name.
+
+        Examples
+        --------
+        >>> exp = BaseExperiment.from_config({
+        ...     'experiment': {'seed': 5},
+        ...     'callbacks': {'train': ['module.CB']}
+        ... })
+        >>> exp.build_callbacks()
+        >>> 'train' in exp.callbacks
+        True
+        """
+        logger.info(
+            f"Building callbacks for experiment {self.name}"
+        )
+
+        # Initialize callbacks container
         self.callbacks = {}
+
         if "callbacks" in self.config:
+
+            # Get the callback group
             self.callbacks = eval_callbacks(self.config["callbacks"], self)
+
+            # Log the callback progress
+            logger.debug(
+                f'Attached callback groups: {list(self.callbacks.keys())}'
+            )
+
+        else:
+            logger.info(
+                f'No callbacks configured for experiment run {self.name}'
+            )

commit ea20bb8128188a382067b06988afb83fe7b54e79
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Apr 27 11:21:34 2025 -0400

    [REFACTOR] Retire prints in favor of logging

diff --git a/pylot/callbacks/epoch.py b/pylot/callbacks/epoch.py
index 6c6698a..3b5485f 100644
--- a/pylot/callbacks/epoch.py
+++ b/pylot/callbacks/epoch.py
@@ -9,22 +9,37 @@ from typing import Literal, Union
 import numpy as np
 from pydantic import validate_arguments
 from tabulate import tabulate
+from loguru import logger
 
 import torch
 import pandas as pd
 
 def PrintLogged(experiment):
+
     def PrintLoggedCallback(epoch):
-        print(f"Logged @ Epoch {epoch}", flush=True)
+
+        logger.info(f"Logged @ Epoch {epoch}", flush=True)
+
         df = experiment.metrics.df
+
         df = df[df.epoch == epoch].drop(columns=["epoch"])
+
         dfp = pd.pivot(
-            pd.melt(df, id_vars="phase", var_name="metric"),
+            pd.melt(
+                df,
+                id_vars="phase",
+                var_name="metric"
+            ),
             index="metric",
             columns="phase",
         )
+
         dfp.columns = [x[1] for x in dfp.columns]
-        print(tabulate(dfp, headers="keys"), flush=True)
+
+        # Log the metrics table.
+        logger.info(
+            f'\n{tabulate(dfp, headers="keys")}'
+        )
 
     return PrintLoggedCallback
 
@@ -37,13 +52,14 @@ def TerminateOnNaN(experiment, monitor="loss"):
         df = experiment.metrics.df
         for metric in monitor:
             if np.isnan(df[metric]).any():
-                print(
+                logger.error(
                     f"Encountered NaN value on {metric} at epoch {epoch}",
                     file=sys.stderr,
                 )
                 sys.exit(1)
+
             if np.isinf(df[metric]).any():
-                print(
+                logger.error(
                     f"Encountered Infinity value on {metric} at epoch {epoch}",
                     file=sys.stderr,
                 )
@@ -75,9 +91,12 @@ class ETA:
 
     def __call__(self, epoch):
         if epoch >= self.n_steps:
-            print("Done!")
+            logger.info("Done!")
+
             return
+
         self.timestamps[epoch] = time.time()
+
         if len(self.timestamps) % self.print_freq == 0:
             eta = self._least_squares_fit()
             time_per_epoch = self._time_per_epoch(epoch)
@@ -89,7 +108,11 @@ class ETA:
             remain = self.strfdelta(remain, "{hours:02d}:{minutes:02d}:{seconds:02d}")
             perepoch = self.strfdelta(time_per_epoch, "{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}")
             N = self.n_steps
-            print(f"ETA ({epoch}/{N}): {eta:%Y-%m-%d %H:%M:%S} - {remain} remaining - {perepoch} per epoch")
+
+            logger.info(
+                f"ETA ({epoch}/{N}): {eta:%Y-%m-%d %H:%M:%S} - {remain} "
+                f"remaining - {perepoch} per epoch"
+            )
 
     def _time_per_epoch(self, epoch) -> timedelta:
         if epoch==0:
@@ -138,14 +161,12 @@ class ModelCheckpoint:
         monitor: str = "loss",
         mode: Literal["auto", "min", "max"] = "auto",
         phase: str = "val",
-        # save_top_k: int = 1,
         save_freq: int = 1,
     ):
 
         self.phase = phase
         self.experiment = experiment
         self.monitor = monitor
-        # self.save_top_k = save_top_k
         self.save_freq = save_freq
 
         min_patterns = ["*loss*", "*err*"]
@@ -194,7 +215,7 @@ class ModelCheckpoint:
             if not (self.experiment.path / "checkpoints").exists(): 
                 (self.experiment.path / "checkpoints").mkdir(parents=True, exist_ok=True)
             with (self.experiment.path / f"checkpoints/{tag}.pt").open("wb") as f:
-                print(f"Saving model with {tag}")
+                logger.info(f"Saving model with {tag}")
                 torch.save(self._best_state, f)
                 self._have_to_save = False
 
diff --git a/pylot/callbacks/profile.py b/pylot/callbacks/profile.py
index 9b74b71..ef66fbd 100644
--- a/pylot/callbacks/profile.py
+++ b/pylot/callbacks/profile.py
@@ -2,6 +2,7 @@ import itertools
 import math
 
 from tabulate import tabulate
+from loguru import logger
 
 from ..util import printc
 from ..util.timer import StatsCUDATimer, StatsTimer
@@ -31,18 +32,25 @@ def Throughput(experiment, n_iter: int = 1_00, verbose: bool = True):
     timer_df = timer_df.set_index("label")
 
     if verbose:
-        print(tabulate(timer_df, headers="keys"), flush=True)
+        logger.info(
+            f'\n{tabulate(timer_df, headers="keys")}'
+        )
 
     t_dl = timer["train-dl"].mean
     t_gpu = timer["train-loop"].mean
     if t_dl > t_gpu and verbose:
-        printc(
-            f"Experiment is dataloader bound dl={t_dl:.2f}ms > gpu={t_gpu:.2f}ms",
-            color="RED",
+
+        logger.error(
+            f"Experiment is dataloader bound dl={t_dl:.2f}ms > "
+            f"gpu={t_gpu:.2f}ms",
         )
+
         recommended_num_workers = math.ceil(
             experiment.train_dl.num_workers * t_dl / t_gpu
         )
-        printc(f"Try setting num_workers={recommended_num_workers}", color="RED")
+
+        logger.error(
+            f"Try setting num_workers={recommended_num_workers}"
+        )
 
     return timer_df
diff --git a/pylot/callbacks/setup.py b/pylot/callbacks/setup.py
index 8678715..4843a40 100644
--- a/pylot/callbacks/setup.py
+++ b/pylot/callbacks/setup.py
@@ -2,6 +2,7 @@ import json
 
 import pandas as pd
 from tabulate import tabulate
+from loguru import logger
 
 from ..util import S3Path
 from ..util.summary import summary
@@ -12,8 +13,11 @@ from ..metrics import module_table, parameter_table
 def ParameterTable(experiment, save=True, verbose=True):
 
     df = parameter_table(experiment.model)
+
     if verbose:
-        print(tabulate(df, headers="keys"))
+        logger.info(
+            f'\n{tabulate(df, headers="keys")}'
+        )
 
     if save:
         with (experiment.path / "params.csv").open("w") as f:
@@ -24,7 +28,9 @@ def ModuleTable(experiment, save=True, verbose=True):
 
     df = module_table(experiment.model)
     if verbose:
-        print(tabulate(df, headers="keys"))
+        logger.info(
+            f'\n{tabulate(df, headers="keys")}'
+        )
 
     if save:
         with (experiment.path / "modules.csv").open("w") as f:
diff --git a/pylot/callbacks/stop.py b/pylot/callbacks/stop.py
index a52c5ca..8938e6f 100644
--- a/pylot/callbacks/stop.py
+++ b/pylot/callbacks/stop.py
@@ -2,6 +2,7 @@ import sys
 from typing import Literal, Optional
 from fnmatch import fnmatch
 
+from loguru import logger
 import numpy as np
 from pydantic import validate_arguments
 
@@ -53,9 +54,9 @@ class EarlyStopping:
         metrics = metrics[metrics.phase == self.phase]
 
         if self.check_finite and not np.isfinite(metrics[self.monitor]).all():
-            print(
-                f"Epoch {epoch}: Encountered non-finite value for {self.phase} {self.monitor}",
-                file=sys.stderr,
+            logger.error(
+                f"Epoch {epoch}: Encountered non-finite value for "
+                f"{self.phase} {self.monitor}",
             )
             sys.exit(0)
 
@@ -72,17 +73,19 @@ class EarlyStopping:
         cmp = {"max": np.greater, "min": np.less}[self.mode]
         # Non-recent is better than recent
         if cmp(fn(previous[quantity]), fn(recent[quantity])):
-            print(
-                f"Epoch {epoch}: {quantity} has not improved for {self.patience} epochs",
-                file=sys.stderr,
+            logger.error(
+                f"Epoch {epoch}: {quantity} has not improved for "
+                f"{self.patience} epochs",
             )
             sys.exit(0)
 
         if len(previous) > 0 and (
             abs(np.min(recent[quantity] - np.max(recent[quantity]))) <= self.min_delta
         ):
-            print(
-                f"Epoch {epoch}: {quantity} has improved less than {self.min_delta} in the last {self.patience} epochs",
+            logger.error(
+                f"Epoch {epoch}: {quantity} has improved less than "
+                f"{self.min_delta} in the last {self.patience} epochs",
                 file=sys.stderr,
             )
+
             sys.exit(0)
diff --git a/pylot/experiment/base.py b/pylot/experiment/base.py
index 80dc16e..53e6196 100644
--- a/pylot/experiment/base.py
+++ b/pylot/experiment/base.py
@@ -32,13 +32,19 @@ def eval_callbacks(all_callbacks, experiment):
 
 
 class BaseExperiment:
-    def __init__(self, path):
+    def __init__(self, path: str):
+        """
+        path : str
+            Path to the configuration file (`.yml`) for the experiment.
+        """
+
         if isinstance(path, str):
             path = pathlib.Path(path)
         self.path = path
         assert path.exists()
         self.name = self.path.stem
 
+        
         self.config = ImmutableConfig.from_file(path / "config.yml")
         self.properties = FHDict(self.path / "properties.json")
         self.metadata = FHDict(self.path / "metadata.json")
diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index a73a5b4..babe73d 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -16,6 +16,7 @@ import torch
 import wandb
 from torch import nn
 from torch.utils.data import DataLoader
+from loguru import logger
 
 from ..nn.util import num_params, split_param_groups_by_weight_decay
 from ..util.ioutil import autosave
@@ -178,7 +179,7 @@ class TrainExperiment(BaseExperiment):
             optim_cfg["params"] = split_param_groups_by_weight_decay(
                 self.model, optim_cfg["weight_decay"]
             )
-    
+
         else:
             optim_cfg["params"] = self.model.parameters()
 
@@ -225,7 +226,7 @@ class TrainExperiment(BaseExperiment):
                 state.pop("optim", None)
             strict = init_cfg.get("strict", True)
             self.set_state(state, strict=strict)
-            print(f"Loaded initialization state from: {path}")
+            logger.info(f"Loaded initialization state from: {path}")
 
     @property
     def state(self):
@@ -256,7 +257,7 @@ class TrainExperiment(BaseExperiment):
         checkpoint_dir.mkdir(exist_ok=True)
 
         tag = tag if tag is not None else "last"
-        print(f"Checkpointing with tag:{tag} at epoch:{self._epoch}")
+        logger.info(f"Checkpointing with tag:{tag} at epoch:{self._epoch}")
 
         with (checkpoint_dir / f"{tag}.pt").open("wb") as f:
             torch.save(self.state, f)
@@ -273,11 +274,18 @@ class TrainExperiment(BaseExperiment):
         checkpoint_dir = self.path / "checkpoints"
         tag = tag if tag is not None else "last"
         with (checkpoint_dir / f"{tag}.pt").open("rb") as f:
-            state = torch.load(f, map_location=self.device)
+            state = torch.load(
+                f=f,
+                weights_only=True,
+                map_location=self.device
+            )
+
             self.set_state(state)
-        print(
-            f"Loaded checkpoint with tag:{tag}. Last epoch:{self.properties['epoch']}"
-        )
+            logger.info(
+                f"Loaded checkpoint with tag:{tag}. "
+                f"Last epoch:{self.properties['epoch']}"
+            )
+
         return self
 
     def to_device(self):
@@ -290,7 +298,7 @@ class TrainExperiment(BaseExperiment):
             callback(**kwargs)
 
     def run(self):
-        print(f"Running {str(self)}")
+        logger.info(f"Running {str(self)}")
         epochs: int = self.config["train.epochs"]
         self.to_device()
         self.build_dataloader()
@@ -314,7 +322,7 @@ class TrainExperiment(BaseExperiment):
 
         for epoch in range(last_epoch + 1, epochs):
 
-            print(f"Start epoch {epoch}")
+            logger.info(f"Start epoch {epoch}")
 
             self._epoch = epoch
             self.run_phase("train", epoch)
@@ -373,7 +381,9 @@ class TrainExperiment(BaseExperiment):
             }
 
         if self.config.get('wandb.track_it', False):
-            wandb.log(wandb_metrics, step=epoch)
+            logger.info('LOGGING TO WANDB')
+            logger.info(f"\n{wandb_metrics}")
+            wandb.log(wandb_metrics)
     
         self.metrics.log(metrics)
 
@@ -406,4 +416,3 @@ class TrainExperiment(BaseExperiment):
 
     def build_augmentations(self):
         pass
-
diff --git a/pylot/util/metrics.py b/pylot/util/metrics.py
index 4d39cc7..5d9bf6f 100644
--- a/pylot/util/metrics.py
+++ b/pylot/util/metrics.py
@@ -16,9 +16,13 @@ class MetricsStore:
         self.path = path
 
     def log(self, *metrics: List[Dict[str, Any]]):
+
         with self.path.open("a") as f:
+
             for datapoint in metrics:
-                print(json.dumps(datapoint), file=f)
+                print(
+                    json.dumps(datapoint), file=f
+                )
 
     def log_df(self, df: pd.DataFrame):
         with self.path.open("a") as f:

commit 0a23f552d41c4f14f7a2157ac9a1a9afcea10eda
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Wed Apr 23 15:28:32 2025 -0400

    [BUILD] Add lmdbm back to pyproject

diff --git a/pyproject.toml b/pyproject.toml
index 4b20540..1f3ce19 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -31,7 +31,7 @@ dependencies = [
     "jc",
     "Jinja2",
     "kornia",
-#    "lmdbm",
+    "lmdbm",
      "lz4",
     "matplotlib",
 #    "more-itertools",

commit 11197330c1b4ad15ad00f829789ec0a49fffab64
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Wed Apr 23 10:27:00 2025 -0400

    [DOCS] Add documentation to train.py

diff --git a/pylot/experiment/train.py b/pylot/experiment/train.py
index 9dd703c..a73a5b4 100644
--- a/pylot/experiment/train.py
+++ b/pylot/experiment/train.py
@@ -1,9 +1,19 @@
+"""
+Class for training experiments.
+
+Example
+-------
+>>> from universeg.experiment import TrainExperiment
+>>> exp =  TrainExperiment.from_config(config)
+>>> exp.run()
+"""
+
 import copy
 import pathlib
-import sys
 from typing import List
 
 import torch
+import wandb
 from torch import nn
 from torch.utils.data import DataLoader
 
@@ -16,10 +26,64 @@ from .util import absolute_import, eval_config
 
 
 class TrainExperiment(BaseExperiment):
+    """
+    A training experiment to be subclassed for concrete implementations.
+
+    Attributes
+    ----------
+    config : dict
+        inherited from `BaseExperiment`.
+    properties : dict
+        inherited from `BaseExperiment`.
+    metadata : dict
+        inherited from `BaseExperiment`.
+    train_dataset : torch.utils.data.Dataset
+        Training dataset set by `build_data()` method.
+    val_dataset : torch.utils.data.Dataset
+        Training dataset set by `build_data()` method.
+    train_dl : torch.utils.data.DataLoader
+        Dataloader for training data, set by `build_dataloader()` method.
+    val_dl : torch.utils.data.DataLoader
+        Dataloader for training data, set by `build_dataloader()` method.
+    model : torch.nn.Module
+        Model with trainable parameters
+    properties : dict
+        Properties and metadata of `TrainExperiment`
+    optim : torch.optim
+    """
+
     def __init__(self, path):
+        """
+        Initialize experiment.
+
+        Parameters
+        ----------
+        path : str
+            Path to the directory containing a `config.yml`
+
+        Notes
+        -----
+        This method initializes the components of an experiment in the
+        following order:
+        1. Build the datasets (`train_dataset` and `val_dataset` attrs)
+        2. Build the dataloaders (`train_dl` and `val_dl` attrs)
+        3. Initialize the optimizer (`optim` attr)
+        4. Build the metrics (`metric_fns`)
+        5. Build the augmentations (`augmentations` only in concrete instances)
+        """
+
+        # Enable cuDNN auto-tuner for performance
         torch.backends.cudnn.benchmark = True
+
+        # Initialize parent `BaseExperiment`
         super().__init__(path)
-        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+
+        # Set cuda if available
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu"
+        )
+
+        # Build data, model, optimizer, loss, metrics, augmentations
         self.build_data()
         self.build_model()
         self.build_optim()
@@ -28,50 +92,128 @@ class TrainExperiment(BaseExperiment):
         self.build_augmentations()
 
     def build_data(self):
+        """
+        Set `train_dataset` and `val_dataset` instance attributes.
+
+        Reads dataset configuration from the `data` field of `self.config`
+        and initializes datasets for training and validation splits.
+        """
+
+        # Make sure config file has 'data' field
+        assert 'data' in self.config.keys(), (
+            f'"data" key not found in {self.path}/config.yml'
+        )
+
+        # Get the dataset configruations
         data_cfg = self.config["data"].to_dict()
-        dataset_constructor = data_cfg.pop("_class", None) or data_cfg.pop("_fn")
+
+        # Get the class or function constructor for the dataset
+        dataset_constructor = data_cfg.pop(
+            "_class", None
+        ) or data_cfg.pop("_fn")
+
+        # Dynamically import the constructor
         dataset_cls = absolute_import(dataset_constructor)
 
+        # Set the train and val dataset attrs by making an instance of the
+        # dataset classes and feeding in kwargs
         self.train_dataset = dataset_cls(split="train", **data_cfg)
         self.val_dataset = dataset_cls(split="val", **data_cfg)
 
     def build_dataloader(self):
+        """
+        Set `train_dl` and `val_dl` instance attributes. 
+
+        Reads dataloader configuration from the `dataloader` field of
+        `self.config`
+        """
+
+        # Make sure config file has 'dataloader' field
+        assert 'dataloader' in self.config.keys(), (
+            f'"dataloader" key not found in {self.path}/config.yml'
+        )
+
+        # Get the dataloader configuraion
+        dl_cfg = self.config["dataloader"]
+
+        # Make sure the batch size is less than the total training set
         assert self.config["dataloader.batch_size"] <= len(
             self.train_dataset
-        ), "Batch size larger than dataset"
-        dl_cfg = self.config["dataloader"]
+        ), (
+        f'Batch size = {self.config["dataloader.batch_size"]} should not be '
+        f'larger than dataset (len = {len(self.train_dataset)})'
+        )
+
+        # Set the validation dataloader attribute
         self.train_dl = DataLoader(
             self.train_dataset, shuffle=True, drop_last=False, **dl_cfg
         )
+
+        # Set the validation dataloader attribute
         self.val_dl = DataLoader(
             self.val_dataset, shuffle=False, drop_last=False, **dl_cfg
         )
 
     def build_model(self):
+        """
+        Initialize the model from config and record param count.
+        """
+
+        # Set the model
         self.model = eval_config(self.config["model"])
+
+        # Compute and store the number of trainable parameters into properties
         self.properties["num_params"] = num_params(self.model)
 
     def build_optim(self):
+        """
+        Initialize optimizer from config, handling weight decay.
+        """
+
+        # Get the optimizer dict from config
         optim_cfg = self.config["optim"].to_dict()
-        # TODO add lr_scheduler support
-        # self.lr_scheduler = eval_config(opt_cfg.pop('lr_scheduler', None)
 
+        # Optionaly handle weight decay for param groups
         if "weight_decay" in optim_cfg:
             optim_cfg["params"] = split_param_groups_by_weight_decay(
                 self.model, optim_cfg["weight_decay"]
             )
+    
         else:
             optim_cfg["params"] = self.model.parameters()
 
+        # Set the `optim` instance attr
         self.optim = eval_config(optim_cfg)
 
     def build_loss(self):
+        """
+        Initialize the loss function from configuration.
+
+        Reads the `loss_func` field of `self.config` and sets the
+        `loss_func` attribute.
+        """
+
+        # Instantiate loss function
         self.loss_func = eval_config(self.config["loss_func"])
 
     def build_metrics(self):
+        """
+        Initialize metric functions for logging.
+
+        Reads the `log.metrics` field of `self.config` (if present)
+        and sets `metric_fns` to a dict of callables.
+        """
+
+        # Initialize container for storing metrics
         self.metric_fns = {}
+
         if "log.metrics" in self.config:
-            self.metric_fns = eval_config(copy.deepcopy(self.config["log.metrics"]))
+
+            # Deep copy to avoid unwanted side effects
+            metrics_config = copy.deepcopy(self.config["log.metrics"])
+
+            # Initialize the metric functions
+            self.metric_fns = eval_config(metrics_config)
 
     def build_initialization(self):
         if "initialization" in self.config:
@@ -155,10 +297,12 @@ class TrainExperiment(BaseExperiment):
         self.build_callbacks()
 
         last_epoch: int = self.properties.get("epoch", -1)
+
         if last_epoch >= 0:
             self.load(tag="last")
             df = self.metrics.df
             autosave(df[df.epoch < last_epoch], self.path / "metrics.jsonl")
+
         else:
             self.build_initialization()
 
@@ -168,29 +312,25 @@ class TrainExperiment(BaseExperiment):
         checkpoint_freq: int = self.config.get("log.checkpoint_freq", 1)
         eval_freq: int = self.config.get("train.eval_freq", 1)
 
-        # self.run_callbacks("setup")
-
-        # try:
-
         for epoch in range(last_epoch + 1, epochs):
+
             print(f"Start epoch {epoch}")
+
             self._epoch = epoch
             self.run_phase("train", epoch)
+
             if eval_freq > 0 and (epoch % eval_freq == 0 or epoch == epochs - 1):
                 self.run_phase("val", epoch)
 
             if checkpoint_freq > 0 and epoch % checkpoint_freq == 0:
                 self.checkpoint()
+
             self.run_callbacks("epoch", epoch=epoch)
 
         self.checkpoint(tag="last")
 
         self.run_callbacks("wrapup")
 
-        # except KeyboardInterrupt:
-        #     print(f"Interrupted at epoch {epoch}. Tearing Down")
-        #     self.checkpoint(tag="interrupt")
-        #     sys.exit(1)
 
     def run_phase(self, phase, epoch):
 
@@ -213,14 +353,30 @@ class TrainExperiment(BaseExperiment):
                     augmentation=augmentation,
                     epoch=epoch,
                 )
+
                 metrics = self.compute_metrics(outputs)
+
                 meters.update(metrics)
+
                 self.run_callbacks(
                     "batch", epoch=epoch, batch_idx=batch_idx, phase=phase
                 )
 
-        metrics = {"phase": phase, "epoch": epoch, **meters.collect("mean")}
+        meters_collect = meters.collect("mean")
+
+        metrics = {"phase": phase, "epoch": epoch, **meters_collect}
+
+        wandb_metrics = {
+            'epoch': epoch,
+            f'{phase}_loss': meters_collect['loss'],
+            f'{phase}_dice_score': meters_collect['dice_score'],
+            }
+
+        if self.config.get('wandb.track_it', False):
+            wandb.log(wandb_metrics, step=epoch)
+    
         self.metrics.log(metrics)
+
         return metrics
 
     def run_step(self, batch_idx, batch, backward=True, augmentation=True, epoch=None):
@@ -251,8 +407,3 @@ class TrainExperiment(BaseExperiment):
     def build_augmentations(self):
         pass
 
-
-# Example use:
-# >>> from universeg.experiment import TrainExperiment
-# >>> exp =  TrainExperiment.from_config(config)
-# >>> exp.run()

commit aa47a662dddbbc5f9de581d332c556dcb8b63e53
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Wed Apr 23 09:57:05 2025 -0400

    [FIX] Add back some dependencies that are required

diff --git a/pyproject.toml b/pyproject.toml
index 252cef7..4b20540 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -20,7 +20,7 @@ dependencies = [
 #    "box",
 #    "decoder",
 #    "device",
-#    "diskcache",
+    "diskcache",
     "einops",
 #    "fastcore",
     "graphviz",
@@ -28,14 +28,14 @@ dependencies = [
 #    "imohash",
     "IPython",
     "ipywidgets",
-#    "jc",
+    "jc",
     "Jinja2",
-#    "kornia",
+    "kornia",
 #    "lmdbm",
-#    "lz4",
+     "lz4",
     "matplotlib",
 #    "more-itertools",
-#    "msgpack-numpy",
+    "msgpack-numpy",
 #    "msgpack-python",
     "numpy",
 #    "opencv-contrib-python",
@@ -43,19 +43,19 @@ dependencies = [
     "pandas",
 #    "parse",
 #    "Pillow",
-#    "plotly",
+    "plotly",
 #    "psutil",
-#    "pyarrow",
+    "pyarrow",
     "pydantic",
 #    "Pympler",
     "PyYAML",
     "rich",
-#    "s3fs",
+    "s3fs",
     "scikit-learn",
     "scipy",
-#    "seaborn",
+    "seaborn",
     "setuptools",
-#    "stackprinter",
+    "stackprinter",
     "tabulate",
     "torch",
     "torchmetrics",
@@ -63,8 +63,8 @@ dependencies = [
     "torchviz",
     "tqdm",
     "typer",
-#    "xxhash",
-#    "zstd",
+    "xxhash",
+    "zstd",
 ]
 
 requires-python = ">=3.9"

commit d16aaf88cdf82a6de6ba96c2f4fadeee36b5306d
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Wed Apr 23 08:12:14 2025 -0400

    [BUILD] Remove dependencies I don't use

diff --git a/pyproject.toml b/pyproject.toml
index c72a8fb..252cef7 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -16,46 +16,46 @@ classifiers = [
 ]
 keywords = ["deep learning"]
 dependencies = [
-    "albumentations",
-    "box",
-    "decoder",
-    "device",
-    "diskcache",
+#    "albumentations",
+#    "box",
+#    "decoder",
+#    "device",
+#    "diskcache",
     "einops",
-    "fastcore",
+#    "fastcore",
     "graphviz",
-    "humanize",
-    "imohash",
+#    "humanize",
+#    "imohash",
     "IPython",
     "ipywidgets",
-    "jc",
+#    "jc",
     "Jinja2",
-    "kornia",
-    "lmdbm",
-    "lz4",
+#    "kornia",
+#    "lmdbm",
+#    "lz4",
     "matplotlib",
-    "more-itertools",
-    "msgpack-numpy",
-    "msgpack-python",
+#    "more-itertools",
+#    "msgpack-numpy",
+#    "msgpack-python",
     "numpy",
-    "opencv-contrib-python",
-    "opencv-python-headless",
+#    "opencv-contrib-python",
+#    "opencv-python-headless",
     "pandas",
-    "parse",
-    "Pillow",
-    "plotly",
-    "psutil",
-    "pyarrow",
+#    "parse",
+#    "Pillow",
+#    "plotly",
+#    "psutil",
+#    "pyarrow",
     "pydantic",
-    "Pympler",
+#    "Pympler",
     "PyYAML",
     "rich",
-    "s3fs",
+#    "s3fs",
     "scikit-learn",
     "scipy",
-    "seaborn",
+#    "seaborn",
     "setuptools",
-    "stackprinter",
+#    "stackprinter",
     "tabulate",
     "torch",
     "torchmetrics",
@@ -63,8 +63,8 @@ dependencies = [
     "torchviz",
     "tqdm",
     "typer",
-    "xxhash",
-    "zstd",
+#    "xxhash",
+#    "zstd",
 ]
 
 requires-python = ">=3.9"

commit 222c404c9840df99b388b8f6104361b25506dfea
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Apr 15 09:13:15 2025 -0400

    [DOCS] Add module-level docstring to pylog

diff --git a/pylot/__init__.py b/pylot/__init__.py
index 5078911..db96b08 100644
--- a/pylot/__init__.py
+++ b/pylot/__init__.py
@@ -1,3 +1,50 @@
+"""
+Pylot is designed as a supporting library to my other research efforts in Deep
+Learning using PyTorch. It has many features and implementations but some of
+the most relevant ones:
+
+- **Experiment Management:** Human friendly classes to wrap the complexity of
+running experiments. Experiments can be loaded, providing convient access to
+model, data and logged results.
+
+- **Flexible Compilation:** Experiment options are specified via hierarchical
+YAML configs that are implicitly verified. No more argparse.
+
+- **Two-way Callbacks:** To modify experiments in a flexible and modular way,
+Pylot uses callbacks to extend the behavior of experiments.
+
+- **Simplified I/O:** Pylot removes the pains of loading/saving data through
+its many OOP interfaces in util.
+
+- **Results Analysis:** Result aggregation primitives such as Pareto frontiers,
+bootstrapping, and recursive selection are included.
+
+- **Metrics and Losses:** Implementation of common Computer Vision metrics and
+losses in pure PyTorch
+
+- **Model/Layer/Optim ZOO:** Pylot includes implementations of popular deep
+learning models, building blocks and optimization algorithms, such as:
+ - Sharpness Aware Minimization (SAM)
+ - Exponential Moving Average Weights (EMA)
+ - U-Net models
+ - Voxelmorph networks Hypernetworks
+ - `SqueezeExcitation` layers
+
+- **Extensive Type Validation:** Type hints are included and enforced thanks to
+pydantic.
+
+- **Pandas Extensions:** Pandas DataFrame API is extended to better handle
+result dataframes, with methods such as
+ - `.select()`
+ - `.drop_constant()`
+ - `.unique_per_col()`
+ - or the `UnixAccessor` functionality.
+
+- **Designed for Speed:** From careful I/O considerations to CUDA
+optimizations, pylot is painstakingly designed with efficiency and speed in
+mind.
+"""
+
 from . import analysis
 from . import augmentation
 from . import callbacks

commit 7bb8b9f09cdba0cc1df516d5c0ed9b52f3b2bf0f
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Apr 15 09:03:18 2025 -0400

    [BUILD] Add "include" to pyproject.toml to explicitly search for pylot when muliple top level directories (e.g. sandbox) are present.

diff --git a/pyproject.toml b/pyproject.toml
index eae7e13..c72a8fb 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,5 +1,5 @@
 [build-system]
-requires      = ["setuptools>=61.0.0", "wheel"]
+requires = ["setuptools>=61.0.0", "wheel"]
 build-backend = "setuptools.build_meta"
 
 [project]
@@ -74,4 +74,7 @@ Homepage = "https://github.com/jjgo/pylot"
 Repository = "https://github.com/jjgo/pylot"
 
 [project.optional-dependencies]
-build = ["build", "setuptols", "twine"]
\ No newline at end of file
+build = ["build", "setuptols", "twine"]
+
+[tool.setuptools.packages.find]
+include = ["pylot*"]
\ No newline at end of file

commit 36a6d3294aa872e1e31a1fca176276748a35a594
Merge: fb4057a 65c8bb6
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Tue Apr 15 08:24:50 2025 -0400

    [MERGE] Merge remote-tracking branch 'upstream/master'

commit fb4057aedfa6a3c63e3d6aa77d5735eee2d093da
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Feb 23 21:43:58 2025 -0500

    [STYLE] Import wildcards from all subpackages

diff --git a/pylot/__init__.py b/pylot/__init__.py
index a0c89a8..5078911 100644
--- a/pylot/__init__.py
+++ b/pylot/__init__.py
@@ -12,3 +12,18 @@ from . import pandas
 from . import plot
 from . import torch
 from . import util
+
+from .analysis import *
+from .augmentation import *
+from .callbacks import *
+from .datasets import *
+from .experiment import *
+from .loss import *
+from .metrics import *
+from .models import *
+from .nn import *
+from .optim import *
+from .pandas import *
+from .plot import *
+from .torch import *
+from .util import *

commit c585f349c2352d40988ab9aab0de407ff2d30f6e
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Feb 23 21:43:29 2025 -0500

    [FIX] Don't register colormaps to avoid import errors

diff --git a/pylot/plot/cmap.py b/pylot/plot/cmap.py
index 6fad363..4009f7a 100644
--- a/pylot/plot/cmap.py
+++ b/pylot/plot/cmap.py
@@ -16,8 +16,8 @@ Monokai = ListedColormap(
     ]
 )
 
-plt.register_cmap("Monokai", Monokai)
+# plt.register_cmap("Monokai", Monokai)
 Cyberpunk = ListedColormap(
     ["#08F7FE", "#FE53BB", "#F5D300", "#00ff41", "#ff0000", "#9467bd"]
 )
-plt.register_cmap("Cyberpunk", Cyberpunk)
+# plt.register_cmap("Cyberpunk", Cyberpunk)

commit 1a2440c146a0adcc6cbb72d8804f5b48c55c55b5
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Feb 23 21:38:58 2025 -0500

    [CHORE] Move __init__.py and add imports

diff --git a/__init__.py b/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/pylot/__init__.py b/pylot/__init__.py
new file mode 100644
index 0000000..a0c89a8
--- /dev/null
+++ b/pylot/__init__.py
@@ -0,0 +1,14 @@
+from . import analysis
+from . import augmentation
+from . import callbacks
+from . import datasets
+from . import experiment
+from . import loss
+from . import metrics
+from . import models
+from . import nn
+from . import optim
+from . import pandas
+from . import plot
+from . import torch
+from . import util

commit fda3ed8cbbdfb0bc48331fabfb7d6e5452cbca59
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Feb 23 21:38:26 2025 -0500

    [STYLE] Sort dependencies in pyproject.toml

diff --git a/pyproject.toml b/pyproject.toml
index 01e1207..86fc676 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -3,35 +3,35 @@ name = "pylot"
 version = "0.1.0"
 requires-python = ">=3.8"
 dependencies = [
-    "diskcache",
-    "einops",
-    "jc",
-    "more_itertools",
-    "numpy",
-    "s3fs",
-    "scikit-learn",
-    "stackprinter",
-    "tabulate",
-    "torch",
-    "torchvision",
-    "tqdm",
-    "pandas",
-    "ipywidgets",
-    "seaborn",
-    "plotly",
-    "kornia",
-    "pydantic",
-    "pyyaml",
-    "lmdbm",
-    "lz4",
-    "matplotlib",
-    "msgpack",
-    "msgpack_numpy",
-    "pillow",
-    "pyarrow",
-    "scipy",
-    "xxhash",
-    "zstd"
+    'diskcache',
+    'einops',
+    'ipywidgets',
+    'jc',
+    'kornia',
+    'lmdbm',
+    'lz4',
+    'matplotlib',
+    'more_itertools',
+    'msgpack',
+    'msgpack_numpy',
+    'numpy',
+    'pandas',
+    'pillow',
+    'plotly',
+    'pyarrow',
+    'pydantic',
+    'pyyaml',
+    's3fs',
+    'scikit-learn',
+    'scipy',
+    'seaborn',
+    'stackprinter',
+    'tabulate',
+    'torch',
+    'torchvision',
+    'tqdm',
+    'xxhash',
+    'zstd'
 ]
 
 [tool.setuptools.packages.find]

commit 08673f324efdec75354b57442ec3b9905317ead3
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Feb 23 21:36:14 2025 -0500

    [BUILD] Introduce pyproject.toml for easy pip installs.

diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000..01e1207
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,38 @@
+[project]
+name = "pylot"
+version = "0.1.0"
+requires-python = ">=3.8"
+dependencies = [
+    "diskcache",
+    "einops",
+    "jc",
+    "more_itertools",
+    "numpy",
+    "s3fs",
+    "scikit-learn",
+    "stackprinter",
+    "tabulate",
+    "torch",
+    "torchvision",
+    "tqdm",
+    "pandas",
+    "ipywidgets",
+    "seaborn",
+    "plotly",
+    "kornia",
+    "pydantic",
+    "pyyaml",
+    "lmdbm",
+    "lz4",
+    "matplotlib",
+    "msgpack",
+    "msgpack_numpy",
+    "pillow",
+    "pyarrow",
+    "scipy",
+    "xxhash",
+    "zstd"
+]
+
+[tool.setuptools.packages.find]
+include = ["pylot*"]

commit c17f9d9fe4d58cae882a0e0a318b2c0d59f93714
Author: EtienneChollet <etiennechollet2626@gmail.com>
Date:   Sun Feb 23 15:35:23 2025 -0500

    [ARCH] Encapsulate package code in pylot/

diff --git a/analysis/__init__.py b/pylot/analysis/__init__.py
similarity index 100%
rename from analysis/__init__.py
rename to pylot/analysis/__init__.py
diff --git a/analysis/aggregate.py b/pylot/analysis/aggregate.py
similarity index 100%
rename from analysis/aggregate.py
rename to pylot/analysis/aggregate.py
diff --git a/analysis/bootstrap.py b/pylot/analysis/bootstrap.py
similarity index 100%
rename from analysis/bootstrap.py
rename to pylot/analysis/bootstrap.py
diff --git a/analysis/collect.py b/pylot/analysis/collect.py
similarity index 100%
rename from analysis/collect.py
rename to pylot/analysis/collect.py
diff --git a/analysis/data.py b/pylot/analysis/data.py
similarity index 100%
rename from analysis/data.py
rename to pylot/analysis/data.py
diff --git a/analysis/pareto.py b/pylot/analysis/pareto.py
similarity index 100%
rename from analysis/pareto.py
rename to pylot/analysis/pareto.py
diff --git a/augmentation/__init__.py b/pylot/augmentation/__init__.py
similarity index 100%
rename from augmentation/__init__.py
rename to pylot/augmentation/__init__.py
diff --git a/augmentation/common.py b/pylot/augmentation/common.py
similarity index 100%
rename from augmentation/common.py
rename to pylot/augmentation/common.py
diff --git a/augmentation/geometry.py b/pylot/augmentation/geometry.py
similarity index 100%
rename from augmentation/geometry.py
rename to pylot/augmentation/geometry.py
diff --git a/augmentation/variable.py b/pylot/augmentation/variable.py
similarity index 100%
rename from augmentation/variable.py
rename to pylot/augmentation/variable.py
diff --git a/callbacks/__init__.py b/pylot/callbacks/__init__.py
similarity index 100%
rename from callbacks/__init__.py
rename to pylot/callbacks/__init__.py
diff --git a/callbacks/debug.py b/pylot/callbacks/debug.py
similarity index 100%
rename from callbacks/debug.py
rename to pylot/callbacks/debug.py
diff --git a/callbacks/ema.py b/pylot/callbacks/ema.py
similarity index 100%
rename from callbacks/ema.py
rename to pylot/callbacks/ema.py
diff --git a/callbacks/env.py b/pylot/callbacks/env.py
similarity index 100%
rename from callbacks/env.py
rename to pylot/callbacks/env.py
diff --git a/callbacks/epoch.py b/pylot/callbacks/epoch.py
similarity index 100%
rename from callbacks/epoch.py
rename to pylot/callbacks/epoch.py
diff --git a/callbacks/img.py b/pylot/callbacks/img.py
similarity index 100%
rename from callbacks/img.py
rename to pylot/callbacks/img.py
diff --git a/callbacks/misc.py b/pylot/callbacks/misc.py
similarity index 100%
rename from callbacks/misc.py
rename to pylot/callbacks/misc.py
diff --git a/callbacks/profile.py b/pylot/callbacks/profile.py
similarity index 100%
rename from callbacks/profile.py
rename to pylot/callbacks/profile.py
diff --git a/callbacks/setup.py b/pylot/callbacks/setup.py
similarity index 100%
rename from callbacks/setup.py
rename to pylot/callbacks/setup.py
diff --git a/callbacks/stop.py b/pylot/callbacks/stop.py
similarity index 100%
rename from callbacks/stop.py
rename to pylot/callbacks/stop.py
diff --git a/callbacks/template.py b/pylot/callbacks/template.py
similarity index 100%
rename from callbacks/template.py
rename to pylot/callbacks/template.py
diff --git a/callbacks/templates/base.html b/pylot/callbacks/templates/base.html
similarity index 100%
rename from callbacks/templates/base.html
rename to pylot/callbacks/templates/base.html
diff --git a/callbacks/templates/img_activations.j2 b/pylot/callbacks/templates/img_activations.j2
similarity index 100%
rename from callbacks/templates/img_activations.j2
rename to pylot/callbacks/templates/img_activations.j2
diff --git a/callbacks/templates/img_io.j2 b/pylot/callbacks/templates/img_io.j2
similarity index 100%
rename from callbacks/templates/img_io.j2
rename to pylot/callbacks/templates/img_io.j2
diff --git a/callbacks/util.py b/pylot/callbacks/util.py
similarity index 100%
rename from callbacks/util.py
rename to pylot/callbacks/util.py
diff --git a/callbacks/wrapup.py b/pylot/callbacks/wrapup.py
similarity index 100%
rename from callbacks/wrapup.py
rename to pylot/callbacks/wrapup.py
diff --git a/datasets/__init__.py b/pylot/datasets/__init__.py
similarity index 100%
rename from datasets/__init__.py
rename to pylot/datasets/__init__.py
diff --git a/datasets/cuda.py b/pylot/datasets/cuda.py
similarity index 100%
rename from datasets/cuda.py
rename to pylot/datasets/cuda.py
diff --git a/datasets/download/miniplaces.py b/pylot/datasets/download/miniplaces.py
similarity index 100%
rename from datasets/download/miniplaces.py
rename to pylot/datasets/download/miniplaces.py
diff --git a/datasets/fetch.py b/pylot/datasets/fetch.py
similarity index 100%
rename from datasets/fetch.py
rename to pylot/datasets/fetch.py
diff --git a/datasets/image_folder_tar.py b/pylot/datasets/image_folder_tar.py
similarity index 100%
rename from datasets/image_folder_tar.py
rename to pylot/datasets/image_folder_tar.py
diff --git a/datasets/indexed.py b/pylot/datasets/indexed.py
similarity index 100%
rename from datasets/indexed.py
rename to pylot/datasets/indexed.py
diff --git a/datasets/lmdb.py b/pylot/datasets/lmdb.py
similarity index 100%
rename from datasets/lmdb.py
rename to pylot/datasets/lmdb.py
diff --git a/datasets/organize/convert_to_imagefolder.py b/pylot/datasets/organize/convert_to_imagefolder.py
similarity index 100%
rename from datasets/organize/convert_to_imagefolder.py
rename to pylot/datasets/organize/convert_to_imagefolder.py
diff --git a/datasets/path.py b/pylot/datasets/path.py
similarity index 100%
rename from datasets/path.py
rename to pylot/datasets/path.py
diff --git a/datasets/prepare.py b/pylot/datasets/prepare.py
similarity index 100%
rename from datasets/prepare.py
rename to pylot/datasets/prepare.py
diff --git a/datasets/segmentation/__init__.py b/pylot/datasets/segmentation/__init__.py
similarity index 100%
rename from datasets/segmentation/__init__.py
rename to pylot/datasets/segmentation/__init__.py
diff --git a/datasets/segmentation/ellipse_segmentation.py b/pylot/datasets/segmentation/ellipse_segmentation.py
similarity index 100%
rename from datasets/segmentation/ellipse_segmentation.py
rename to pylot/datasets/segmentation/ellipse_segmentation.py
diff --git a/datasets/selfsuper.py b/pylot/datasets/selfsuper.py
similarity index 100%
rename from datasets/selfsuper.py
rename to pylot/datasets/selfsuper.py
diff --git a/datasets/split.py b/pylot/datasets/split.py
similarity index 100%
rename from datasets/split.py
rename to pylot/datasets/split.py
diff --git a/datasets/subset.py b/pylot/datasets/subset.py
similarity index 100%
rename from datasets/subset.py
rename to pylot/datasets/subset.py
diff --git a/datasets/thunder.py b/pylot/datasets/thunder.py
similarity index 100%
rename from datasets/thunder.py
rename to pylot/datasets/thunder.py
diff --git a/datasets/util.py b/pylot/datasets/util.py
similarity index 100%
rename from datasets/util.py
rename to pylot/datasets/util.py
diff --git a/datasets/vision/__init__.py b/pylot/datasets/vision/__init__.py
similarity index 100%
rename from datasets/vision/__init__.py
rename to pylot/datasets/vision/__init__.py
diff --git a/datasets/vision/cubbirds.py b/pylot/datasets/vision/cubbirds.py
similarity index 100%
rename from datasets/vision/cubbirds.py
rename to pylot/datasets/vision/cubbirds.py
diff --git a/datasets/vision/emnist.py b/pylot/datasets/vision/emnist.py
similarity index 100%
rename from datasets/vision/emnist.py
rename to pylot/datasets/vision/emnist.py
diff --git a/datasets/vision/imagenet.py b/pylot/datasets/vision/imagenet.py
similarity index 100%
rename from datasets/vision/imagenet.py
rename to pylot/datasets/vision/imagenet.py
diff --git a/datasets/vision/imagenette.py b/pylot/datasets/vision/imagenette.py
similarity index 100%
rename from datasets/vision/imagenette.py
rename to pylot/datasets/vision/imagenette.py
diff --git a/datasets/vision/main.py b/pylot/datasets/vision/main.py
similarity index 100%
rename from datasets/vision/main.py
rename to pylot/datasets/vision/main.py
diff --git a/datasets/vision/nist.py b/pylot/datasets/vision/nist.py
similarity index 100%
rename from datasets/vision/nist.py
rename to pylot/datasets/vision/nist.py
diff --git a/datasets/vision/notmnist.py b/pylot/datasets/vision/notmnist.py
similarity index 100%
rename from datasets/vision/notmnist.py
rename to pylot/datasets/vision/notmnist.py
diff --git a/datasets/vision/oxfordpets.py b/pylot/datasets/vision/oxfordpets.py
similarity index 100%
rename from datasets/vision/oxfordpets.py
rename to pylot/datasets/vision/oxfordpets.py
diff --git a/datasets/vision/pascal.py b/pylot/datasets/vision/pascal.py
similarity index 100%
rename from datasets/vision/pascal.py
rename to pylot/datasets/vision/pascal.py
diff --git a/datasets/vision/usps28.py b/pylot/datasets/vision/usps28.py
similarity index 100%
rename from datasets/vision/usps28.py
rename to pylot/datasets/vision/usps28.py
diff --git a/datasets/vision/utils.py b/pylot/datasets/vision/utils.py
similarity index 100%
rename from datasets/vision/utils.py
rename to pylot/datasets/vision/utils.py
diff --git a/experiment/__init__.py b/pylot/experiment/__init__.py
similarity index 100%
rename from experiment/__init__.py
rename to pylot/experiment/__init__.py
diff --git a/experiment/base.py b/pylot/experiment/base.py
similarity index 100%
rename from experiment/base.py
rename to pylot/experiment/base.py
diff --git a/experiment/config.py b/pylot/experiment/config.py
similarity index 100%
rename from experiment/config.py
rename to pylot/experiment/config.py
diff --git a/experiment/distributed.py b/pylot/experiment/distributed.py
similarity index 100%
rename from experiment/distributed.py
rename to pylot/experiment/distributed.py
diff --git a/experiment/finetune.py b/pylot/experiment/finetune.py
similarity index 100%
rename from experiment/finetune.py
rename to pylot/experiment/finetune.py
diff --git a/experiment/half.py b/pylot/experiment/half.py
similarity index 100%
rename from experiment/half.py
rename to pylot/experiment/half.py
diff --git a/experiment/train.py b/pylot/experiment/train.py
similarity index 100%
rename from experiment/train.py
rename to pylot/experiment/train.py
diff --git a/experiment/util.py b/pylot/experiment/util.py
similarity index 100%
rename from experiment/util.py
rename to pylot/experiment/util.py
diff --git a/loss/__init__.py b/pylot/loss/__init__.py
similarity index 100%
rename from loss/__init__.py
rename to pylot/loss/__init__.py
diff --git a/loss/deepsupervision.py b/pylot/loss/deepsupervision.py
similarity index 100%
rename from loss/deepsupervision.py
rename to pylot/loss/deepsupervision.py
diff --git a/loss/flat.py b/pylot/loss/flat.py
similarity index 100%
rename from loss/flat.py
rename to pylot/loss/flat.py
diff --git a/loss/multi.py b/pylot/loss/multi.py
similarity index 100%
rename from loss/multi.py
rename to pylot/loss/multi.py
diff --git a/loss/ncc.py b/pylot/loss/ncc.py
similarity index 100%
rename from loss/ncc.py
rename to pylot/loss/ncc.py
diff --git a/loss/segmentation.py b/pylot/loss/segmentation.py
similarity index 100%
rename from loss/segmentation.py
rename to pylot/loss/segmentation.py
diff --git a/loss/total_variation.py b/pylot/loss/total_variation.py
similarity index 100%
rename from loss/total_variation.py
rename to pylot/loss/total_variation.py
diff --git a/loss/util.py b/pylot/loss/util.py
similarity index 100%
rename from loss/util.py
rename to pylot/loss/util.py
diff --git a/loss/vae.py b/pylot/loss/vae.py
similarity index 100%
rename from loss/vae.py
rename to pylot/loss/vae.py
diff --git a/loss/wgan.py b/pylot/loss/wgan.py
similarity index 100%
rename from loss/wgan.py
rename to pylot/loss/wgan.py
diff --git a/metrics/__init__.py b/pylot/metrics/__init__.py
similarity index 100%
rename from metrics/__init__.py
rename to pylot/metrics/__init__.py
diff --git a/metrics/accuracy.py b/pylot/metrics/accuracy.py
similarity index 100%
rename from metrics/accuracy.py
rename to pylot/metrics/accuracy.py
diff --git a/metrics/activation.py b/pylot/metrics/activation.py
similarity index 100%
rename from metrics/activation.py
rename to pylot/metrics/activation.py
diff --git a/metrics/efficiency/__init__.py b/pylot/metrics/efficiency/__init__.py
similarity index 100%
rename from metrics/efficiency/__init__.py
rename to pylot/metrics/efficiency/__init__.py
diff --git a/metrics/efficiency/abstract_flops.py b/pylot/metrics/efficiency/abstract_flops.py
similarity index 100%
rename from metrics/efficiency/abstract_flops.py
rename to pylot/metrics/efficiency/abstract_flops.py
diff --git a/metrics/efficiency/flops.py b/pylot/metrics/efficiency/flops.py
similarity index 100%
rename from metrics/efficiency/flops.py
rename to pylot/metrics/efficiency/flops.py
diff --git a/metrics/model.py b/pylot/metrics/model.py
similarity index 100%
rename from metrics/model.py
rename to pylot/metrics/model.py
diff --git a/metrics/segmentation.py b/pylot/metrics/segmentation.py
similarity index 100%
rename from metrics/segmentation.py
rename to pylot/metrics/segmentation.py
diff --git a/metrics/size.py b/pylot/metrics/size.py
similarity index 100%
rename from metrics/size.py
rename to pylot/metrics/size.py
diff --git a/metrics/util.py b/pylot/metrics/util.py
similarity index 100%
rename from metrics/util.py
rename to pylot/metrics/util.py
diff --git a/models/__init__.py b/pylot/models/__init__.py
similarity index 100%
rename from models/__init__.py
rename to pylot/models/__init__.py
diff --git a/models/conv.py b/pylot/models/conv.py
similarity index 100%
rename from models/conv.py
rename to pylot/models/conv.py
diff --git a/models/hyper/__init__.py b/pylot/models/hyper/__init__.py
similarity index 100%
rename from models/hyper/__init__.py
rename to pylot/models/hyper/__init__.py
diff --git a/models/hyper/base.py b/pylot/models/hyper/base.py
similarity index 100%
rename from models/hyper/base.py
rename to pylot/models/hyper/base.py
diff --git a/models/hyper/container.py b/pylot/models/hyper/container.py
similarity index 100%
rename from models/hyper/container.py
rename to pylot/models/hyper/container.py
diff --git a/models/hyper/delta.py b/pylot/models/hyper/delta.py
similarity index 100%
rename from models/hyper/delta.py
rename to pylot/models/hyper/delta.py
diff --git a/models/hyper/encoder.py b/pylot/models/hyper/encoder.py
similarity index 100%
rename from models/hyper/encoder.py
rename to pylot/models/hyper/encoder.py
diff --git a/models/hyper/norm.py b/pylot/models/hyper/norm.py
similarity index 100%
rename from models/hyper/norm.py
rename to pylot/models/hyper/norm.py
diff --git a/models/mlp.py b/pylot/models/mlp.py
similarity index 100%
rename from models/mlp.py
rename to pylot/models/mlp.py
diff --git a/models/unet.py b/pylot/models/unet.py
similarity index 100%
rename from models/unet.py
rename to pylot/models/unet.py
diff --git a/models/vae.py b/pylot/models/vae.py
similarity index 100%
rename from models/vae.py
rename to pylot/models/vae.py
diff --git a/models/vision/__init__.py b/pylot/models/vision/__init__.py
similarity index 100%
rename from models/vision/__init__.py
rename to pylot/models/vision/__init__.py
diff --git a/models/vision/cifar_resnet.py b/pylot/models/vision/cifar_resnet.py
similarity index 100%
rename from models/vision/cifar_resnet.py
rename to pylot/models/vision/cifar_resnet.py
diff --git a/models/vision/cifar_vgg.py b/pylot/models/vision/cifar_vgg.py
similarity index 100%
rename from models/vision/cifar_vgg.py
rename to pylot/models/vision/cifar_vgg.py
diff --git a/models/vision/fresnet.py b/pylot/models/vision/fresnet.py
similarity index 100%
rename from models/vision/fresnet.py
rename to pylot/models/vision/fresnet.py
diff --git a/models/vision/head.py b/pylot/models/vision/head.py
similarity index 100%
rename from models/vision/head.py
rename to pylot/models/vision/head.py
diff --git a/models/vision/mnistnet.py b/pylot/models/vision/mnistnet.py
similarity index 100%
rename from models/vision/mnistnet.py
rename to pylot/models/vision/mnistnet.py
diff --git a/models/void/__init__.py b/pylot/models/void/__init__.py
similarity index 100%
rename from models/void/__init__.py
rename to pylot/models/void/__init__.py
diff --git a/models/void/unet.py b/pylot/models/void/unet.py
similarity index 100%
rename from models/void/unet.py
rename to pylot/models/void/unet.py
diff --git a/models/voxelmorph.py b/pylot/models/voxelmorph.py
similarity index 100%
rename from models/voxelmorph.py
rename to pylot/models/voxelmorph.py
diff --git a/models/wgan.py b/pylot/models/wgan.py
similarity index 100%
rename from models/wgan.py
rename to pylot/models/wgan.py
diff --git a/nn/__init__.py b/pylot/nn/__init__.py
similarity index 100%
rename from nn/__init__.py
rename to pylot/nn/__init__.py
diff --git a/nn/batch_renorm.py b/pylot/nn/batch_renorm.py
similarity index 100%
rename from nn/batch_renorm.py
rename to pylot/nn/batch_renorm.py
diff --git a/nn/bmap.py b/pylot/nn/bmap.py
similarity index 100%
rename from nn/bmap.py
rename to pylot/nn/bmap.py
diff --git a/nn/bufferdict.py b/pylot/nn/bufferdict.py
similarity index 100%
rename from nn/bufferdict.py
rename to pylot/nn/bufferdict.py
diff --git a/nn/conv_block.py b/pylot/nn/conv_block.py
similarity index 100%
rename from nn/conv_block.py
rename to pylot/nn/conv_block.py
diff --git a/nn/conv_op.py b/pylot/nn/conv_op.py
similarity index 100%
rename from nn/conv_op.py
rename to pylot/nn/conv_op.py
diff --git a/nn/drop.py b/pylot/nn/drop.py
similarity index 100%
rename from nn/drop.py
rename to pylot/nn/drop.py
diff --git a/nn/ema.py b/pylot/nn/ema.py
similarity index 100%
rename from nn/ema.py
rename to pylot/nn/ema.py
diff --git a/nn/flatten.py b/pylot/nn/flatten.py
similarity index 100%
rename from nn/flatten.py
rename to pylot/nn/flatten.py
diff --git a/nn/frozen_batchnorm.py b/pylot/nn/frozen_batchnorm.py
similarity index 100%
rename from nn/frozen_batchnorm.py
rename to pylot/nn/frozen_batchnorm.py
diff --git a/nn/hooks.py b/pylot/nn/hooks.py
similarity index 100%
rename from nn/hooks.py
rename to pylot/nn/hooks.py
diff --git a/nn/hyper/__init__.py b/pylot/nn/hyper/__init__.py
similarity index 100%
rename from nn/hyper/__init__.py
rename to pylot/nn/hyper/__init__.py
diff --git a/nn/hyper/conv.py b/pylot/nn/hyper/conv.py
similarity index 100%
rename from nn/hyper/conv.py
rename to pylot/nn/hyper/conv.py
diff --git a/nn/hyper/conv_block.py b/pylot/nn/hyper/conv_block.py
similarity index 100%
rename from nn/hyper/conv_block.py
rename to pylot/nn/hyper/conv_block.py
diff --git a/nn/hyper/linear.py b/pylot/nn/hyper/linear.py
similarity index 100%
rename from nn/hyper/linear.py
rename to pylot/nn/hyper/linear.py
diff --git a/nn/hyper/separable.py b/pylot/nn/hyper/separable.py
similarity index 100%
rename from nn/hyper/separable.py
rename to pylot/nn/hyper/separable.py
diff --git a/nn/hyper/void.py b/pylot/nn/hyper/void.py
similarity index 100%
rename from nn/hyper/void.py
rename to pylot/nn/hyper/void.py
diff --git a/nn/hyper/voidify.py b/pylot/nn/hyper/voidify.py
similarity index 100%
rename from nn/hyper/voidify.py
rename to pylot/nn/hyper/voidify.py
diff --git a/nn/hyper/xparam.py b/pylot/nn/hyper/xparam.py
similarity index 100%
rename from nn/hyper/xparam.py
rename to pylot/nn/hyper/xparam.py
diff --git a/nn/init.py b/pylot/nn/init.py
similarity index 100%
rename from nn/init.py
rename to pylot/nn/init.py
diff --git a/nn/interpolation.py b/pylot/nn/interpolation.py
similarity index 100%
rename from nn/interpolation.py
rename to pylot/nn/interpolation.py
diff --git a/nn/lambd.py b/pylot/nn/lambd.py
similarity index 100%
rename from nn/lambd.py
rename to pylot/nn/lambd.py
diff --git a/nn/nonlinearity.py b/pylot/nn/nonlinearity.py
similarity index 100%
rename from nn/nonlinearity.py
rename to pylot/nn/nonlinearity.py
diff --git a/nn/norm.py b/pylot/nn/norm.py
similarity index 100%
rename from nn/norm.py
rename to pylot/nn/norm.py
diff --git a/nn/record.py b/pylot/nn/record.py
similarity index 100%
rename from nn/record.py
rename to pylot/nn/record.py
diff --git a/nn/regularization.py b/pylot/nn/regularization.py
similarity index 100%
rename from nn/regularization.py
rename to pylot/nn/regularization.py
diff --git a/nn/residual.py b/pylot/nn/residual.py
similarity index 100%
rename from nn/residual.py
rename to pylot/nn/residual.py
diff --git a/nn/samplewise.py b/pylot/nn/samplewise.py
similarity index 100%
rename from nn/samplewise.py
rename to pylot/nn/samplewise.py
diff --git a/nn/separable.py b/pylot/nn/separable.py
similarity index 100%
rename from nn/separable.py
rename to pylot/nn/separable.py
diff --git a/nn/spatial_transformer.py b/pylot/nn/spatial_transformer.py
similarity index 100%
rename from nn/spatial_transformer.py
rename to pylot/nn/spatial_transformer.py
diff --git a/nn/squeeze_excite.py b/pylot/nn/squeeze_excite.py
similarity index 100%
rename from nn/squeeze_excite.py
rename to pylot/nn/squeeze_excite.py
diff --git a/nn/types_.py b/pylot/nn/types_.py
similarity index 100%
rename from nn/types_.py
rename to pylot/nn/types_.py
diff --git a/nn/util.py b/pylot/nn/util.py
similarity index 100%
rename from nn/util.py
rename to pylot/nn/util.py
diff --git a/nn/view.py b/pylot/nn/view.py
similarity index 100%
rename from nn/view.py
rename to pylot/nn/view.py
diff --git a/nn/vmap.py b/pylot/nn/vmap.py
similarity index 100%
rename from nn/vmap.py
rename to pylot/nn/vmap.py
diff --git a/nn/weight_norm.py b/pylot/nn/weight_norm.py
similarity index 100%
rename from nn/weight_norm.py
rename to pylot/nn/weight_norm.py
diff --git a/optim/__init__.py b/pylot/optim/__init__.py
similarity index 100%
rename from optim/__init__.py
rename to pylot/optim/__init__.py
diff --git a/optim/larc.py b/pylot/optim/larc.py
similarity index 100%
rename from optim/larc.py
rename to pylot/optim/larc.py
diff --git a/optim/lars.py b/pylot/optim/lars.py
similarity index 100%
rename from optim/lars.py
rename to pylot/optim/lars.py
diff --git a/optim/sam.py b/pylot/optim/sam.py
similarity index 100%
rename from optim/sam.py
rename to pylot/optim/sam.py
diff --git a/optim/scheduler/__init__.py b/pylot/optim/scheduler/__init__.py
similarity index 100%
rename from optim/scheduler/__init__.py
rename to pylot/optim/scheduler/__init__.py
diff --git a/optim/scheduler/poly.py b/pylot/optim/scheduler/poly.py
similarity index 100%
rename from optim/scheduler/poly.py
rename to pylot/optim/scheduler/poly.py
diff --git a/optim/scheduler/warmup.py b/pylot/optim/scheduler/warmup.py
similarity index 100%
rename from optim/scheduler/warmup.py
rename to pylot/optim/scheduler/warmup.py
diff --git a/optim/weight_decay.py b/pylot/optim/weight_decay.py
similarity index 100%
rename from optim/weight_decay.py
rename to pylot/optim/weight_decay.py
diff --git a/optim/wrapper.py b/pylot/optim/wrapper.py
similarity index 100%
rename from optim/wrapper.py
rename to pylot/optim/wrapper.py
diff --git a/pandas/__init__.py b/pylot/pandas/__init__.py
similarity index 100%
rename from pandas/__init__.py
rename to pylot/pandas/__init__.py
diff --git a/pandas/api.py b/pylot/pandas/api.py
similarity index 100%
rename from pandas/api.py
rename to pylot/pandas/api.py
diff --git a/pandas/convenience.py b/pylot/pandas/convenience.py
similarity index 100%
rename from pandas/convenience.py
rename to pylot/pandas/convenience.py
diff --git a/pandas/register.py b/pylot/pandas/register.py
similarity index 100%
rename from pandas/register.py
rename to pylot/pandas/register.py
diff --git a/pandas/unix.py b/pylot/pandas/unix.py
similarity index 100%
rename from pandas/unix.py
rename to pylot/pandas/unix.py
diff --git a/plot/__init__.py b/pylot/plot/__init__.py
similarity index 100%
rename from plot/__init__.py
rename to pylot/plot/__init__.py
diff --git a/plot/cmap.py b/pylot/plot/cmap.py
similarity index 100%
rename from plot/cmap.py
rename to pylot/plot/cmap.py
diff --git a/plot/fibonacci.py b/pylot/plot/fibonacci.py
similarity index 100%
rename from plot/fibonacci.py
rename to pylot/plot/fibonacci.py
diff --git a/plot/plot.py b/pylot/plot/plot.py
similarity index 100%
rename from plot/plot.py
rename to pylot/plot/plot.py
diff --git a/plot/save.py b/pylot/plot/save.py
similarity index 100%
rename from plot/save.py
rename to pylot/plot/save.py
diff --git a/plot/scales.py b/pylot/plot/scales.py
similarity index 100%
rename from plot/scales.py
rename to pylot/plot/scales.py
diff --git a/plot/turbo.py b/pylot/plot/turbo.py
similarity index 100%
rename from plot/turbo.py
rename to pylot/plot/turbo.py
diff --git a/plot/ui.py b/pylot/plot/ui.py
similarity index 100%
rename from plot/ui.py
rename to pylot/plot/ui.py
diff --git a/torch/api.py b/pylot/torch/api.py
similarity index 100%
rename from torch/api.py
rename to pylot/torch/api.py
diff --git a/util/__init__.py b/pylot/util/__init__.py
similarity index 100%
rename from util/__init__.py
rename to pylot/util/__init__.py
diff --git a/util/automap.py b/pylot/util/automap.py
similarity index 100%
rename from util/automap.py
rename to pylot/util/automap.py
diff --git a/util/choice.py b/pylot/util/choice.py
similarity index 100%
rename from util/choice.py
rename to pylot/util/choice.py
diff --git a/util/color.py b/pylot/util/color.py
similarity index 100%
rename from util/color.py
rename to pylot/util/color.py
diff --git a/util/config.py b/pylot/util/config.py
similarity index 100%
rename from util/config.py
rename to pylot/util/config.py
diff --git a/util/debug.py b/pylot/util/debug.py
similarity index 100%
rename from util/debug.py
rename to pylot/util/debug.py
diff --git a/util/dupes.py b/pylot/util/dupes.py
similarity index 100%
rename from util/dupes.py
rename to pylot/util/dupes.py
diff --git a/util/env.py b/pylot/util/env.py
similarity index 100%
rename from util/env.py
rename to pylot/util/env.py
diff --git a/util/filecache.py b/pylot/util/filecache.py
similarity index 100%
rename from util/filecache.py
rename to pylot/util/filecache.py
diff --git a/util/filesystem.py b/pylot/util/filesystem.py
similarity index 100%
rename from util/filesystem.py
rename to pylot/util/filesystem.py
diff --git a/util/future.py b/pylot/util/future.py
similarity index 100%
rename from util/future.py
rename to pylot/util/future.py
diff --git a/util/fzf.py b/pylot/util/fzf.py
similarity index 100%
rename from util/fzf.py
rename to pylot/util/fzf.py
diff --git a/util/hash.py b/pylot/util/hash.py
similarity index 100%
rename from util/hash.py
rename to pylot/util/hash.py
diff --git a/util/img.py b/pylot/util/img.py
similarity index 100%
rename from util/img.py
rename to pylot/util/img.py
diff --git a/util/ioutil.py b/pylot/util/ioutil.py
similarity index 100%
rename from util/ioutil.py
rename to pylot/util/ioutil.py
diff --git a/util/jupyter.py b/pylot/util/jupyter.py
similarity index 100%
rename from util/jupyter.py
rename to pylot/util/jupyter.py
diff --git a/util/libcheck.py b/pylot/util/libcheck.py
similarity index 100%
rename from util/libcheck.py
rename to pylot/util/libcheck.py
diff --git a/util/lock_file.py b/pylot/util/lock_file.py
similarity index 100%
rename from util/lock_file.py
rename to pylot/util/lock_file.py
diff --git a/util/mapping.py b/pylot/util/mapping.py
similarity index 100%
rename from util/mapping.py
rename to pylot/util/mapping.py
diff --git a/util/meta.py b/pylot/util/meta.py
similarity index 100%
rename from util/meta.py
rename to pylot/util/meta.py
diff --git a/util/meter.py b/pylot/util/meter.py
similarity index 100%
rename from util/meter.py
rename to pylot/util/meter.py
diff --git a/util/metrics.py b/pylot/util/metrics.py
similarity index 100%
rename from util/metrics.py
rename to pylot/util/metrics.py
diff --git a/util/more_functools.py b/pylot/util/more_functools.py
similarity index 100%
rename from util/more_functools.py
rename to pylot/util/more_functools.py
diff --git a/util/notify.py b/pylot/util/notify.py
similarity index 100%
rename from util/notify.py
rename to pylot/util/notify.py
diff --git a/util/parallel.py b/pylot/util/parallel.py
similarity index 100%
rename from util/parallel.py
rename to pylot/util/parallel.py
diff --git a/util/pipes.py b/pylot/util/pipes.py
similarity index 100%
rename from util/pipes.py
rename to pylot/util/pipes.py
diff --git a/util/print.py b/pylot/util/print.py
similarity index 100%
rename from util/print.py
rename to pylot/util/print.py
diff --git a/util/profile.py b/pylot/util/profile.py
similarity index 100%
rename from util/profile.py
rename to pylot/util/profile.py
diff --git a/util/randomnames.py b/pylot/util/randomnames.py
similarity index 100%
rename from util/randomnames.py
rename to pylot/util/randomnames.py
diff --git a/util/s3.py b/pylot/util/s3.py
similarity index 100%
rename from util/s3.py
rename to pylot/util/s3.py
diff --git a/util/shapecheck.py b/pylot/util/shapecheck.py
similarity index 100%
rename from util/shapecheck.py
rename to pylot/util/shapecheck.py
diff --git a/util/store.py b/pylot/util/store.py
similarity index 100%
rename from util/store.py
rename to pylot/util/store.py
diff --git a/util/summary.py b/pylot/util/summary.py
similarity index 100%
rename from util/summary.py
rename to pylot/util/summary.py
diff --git a/util/thunder.py b/pylot/util/thunder.py
similarity index 100%
rename from util/thunder.py
rename to pylot/util/thunder.py
diff --git a/util/timer.py b/pylot/util/timer.py
similarity index 100%
rename from util/timer.py
rename to pylot/util/timer.py
diff --git a/util/torchutils.py b/pylot/util/torchutils.py
similarity index 100%
rename from util/torchutils.py
rename to pylot/util/torchutils.py
diff --git a/util/unix.py b/pylot/util/unix.py
similarity index 100%
rename from util/unix.py
rename to pylot/util/unix.py
diff --git a/util/validation.py b/pylot/util/validation.py
similarity index 100%
rename from util/validation.py
rename to pylot/util/validation.py
